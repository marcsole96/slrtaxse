{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {r}\n",
    "install.packages(\"workflowr\")\n",
    "```\n",
    "\n",
    "# Representing a tree with dictionaries\n",
    "\n",
    "https://blog.finxter.com/5-best-ways-to-construct-and-manage-a-tree-in-python/\n",
    "https://builtin.com/articles/tree-python #This one is more complex\n",
    "https://bigtree.readthedocs.io/en/0.14.8/ #There is this package to\n",
    "create trees, but maybe it is too complex for us Pouly, Marc.\n",
    "“Estimating Text Similarity based on Semantic Concept Embeddings.” arXiv\n",
    "preprint arXiv:2401.04422 (2024).\n",
    "\n",
    "``` {python}\n",
    "import torch\n",
    "import einops\n",
    "import math\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "# Load the Jina AI embeddings model\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "taxonomy_tree = {\n",
    "    '1': {\n",
    "        '2': {\n",
    "            'A': 'Lake',\n",
    "            'B': 'River'\n",
    "        },\n",
    "        'C': 'House',\n",
    "        '3': {\n",
    "            '4': {\n",
    "                'D': 'Mountain',\n",
    "                'E': 'Everest',\n",
    "                'F': 'Volcano'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Function to extract leaf nodes\n",
    "def get_leaf_nodes(taxonomy):\n",
    "    leaves = {}\n",
    "    def traverse(node, path):\n",
    "        if isinstance(node, dict):\n",
    "            for k, v in node.items():\n",
    "                traverse(v, path + [k])\n",
    "        else:\n",
    "            leaves[path[-1]] = node  # Leaf node with its path\n",
    "    traverse(taxonomy, [])\n",
    "    return leaves\n",
    "\n",
    "# Function to calculate similarity using the Jina AI embeddings model\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Encode texts to get embeddings\n",
    "    embeddings = model.encode([text1, text2])\n",
    "    # Calculate cosine similarity\n",
    "    sim = torch.nn.functional.cosine_similarity(torch.tensor(embeddings[0]), torch.tensor(embeddings[1]), dim=0)\n",
    "    return sim.item()\n",
    "\n",
    "# Function to calculate R(T)\n",
    "def calculate_r_t(taxonomy):\n",
    "    leaves = get_leaf_nodes(taxonomy)\n",
    "    leaf_names = list(leaves.values())\n",
    "    groups = [leaf_names[i:i + 2] for i in range(0, len(leaf_names), 2)]  # Grouping pairs\n",
    "\n",
    "    total_groups = len(groups)\n",
    "    r_t_values = []\n",
    "\n",
    "    for group in groups:\n",
    "        # Calculate pairwise similarities within the group\n",
    "        similarities = []\n",
    "        for i in range(len(group)):\n",
    "            for j in range(i + 1, len(group)):\n",
    "                sim = calculate_similarity(group[i], group[j])\n",
    "                similarities.append(sim)\n",
    "\n",
    "        if similarities:\n",
    "            min_similarity = min(similarities)\n",
    "        else:\n",
    "            min_similarity = 0  # No pairs means no intruders possible\n",
    "\n",
    "        # Count intruders\n",
    "        intruder_count = 0\n",
    "        for leaf in leaf_names:\n",
    "            if leaf not in group:\n",
    "                sim_with_group = calculate_similarity(leaf, group[0])\n",
    "                if sim_with_group > min_similarity:\n",
    "                    intruder_count += 1\n",
    "\n",
    "        # Calculate R(T) for this group\n",
    "        n_ic = intruder_count\n",
    "        n_gc = len(group)\n",
    "        n_ac = len(leaf_names)\n",
    "\n",
    "        r_t = (1 - (n_ic / (n_gc * (n_ac - n_gc)))) if n_gc * (n_ac - n_gc) > 0 else 0\n",
    "        r_t_values.append(r_t)\n",
    "\n",
    "    return sum(r_t_values) / total_groups if total_groups > 0 else 0\n",
    "  \n",
    "\n",
    "def extract_ncat(taxonomy):\n",
    "    ncat = 0\n",
    "    first_category_found = False  # Flag to track if the first category has been encountered\n",
    "\n",
    "    def count_categories(node, is_root=True):\n",
    "        nonlocal ncat, first_category_found\n",
    "        if isinstance(node, dict):\n",
    "            # Only count nodes that are not the root and not leaves\n",
    "            if not is_root:\n",
    "                if not first_category_found:\n",
    "                    first_category_found = True  # Set the flag after the first category is found\n",
    "                else:\n",
    "                    ncat += 1  # Count the intermediate category\n",
    "                    print(f\"Found category: {list(node.keys())}\")  # Print the keys of the current category\n",
    "            # Recursively process children, marking them as non-root\n",
    "            for child in node.values():\n",
    "                count_categories(child, is_root=False)\n",
    "\n",
    "    count_categories(taxonomy)\n",
    "    return ncat\n",
    "\n",
    "\n",
    "\n",
    "def extract_nchar(taxonomy):\n",
    "    nchar = 0\n",
    "\n",
    "    def count_characteristics(node):\n",
    "        nonlocal nchar\n",
    "        if isinstance(node, dict):\n",
    "            for child in node.values():\n",
    "                count_characteristics(child)\n",
    "        else:\n",
    "            nchar += 1  # Count the current characteristic\n",
    "\n",
    "    count_characteristics(taxonomy)\n",
    "    return nchar\n",
    "\n",
    "def extract_depths_cat(taxonomy):\n",
    "    depths_cat = []\n",
    "\n",
    "    def find_depths(node, depth):\n",
    "        if isinstance(node, dict):\n",
    "            depths_cat.append(depth)  # Record the depth of this category\n",
    "            for child in node.values():\n",
    "                find_depths(child, depth + 1)\n",
    "\n",
    "    find_depths(taxonomy, 0)  # Start from depth 0\n",
    "    return depths_cat\n",
    "  \n",
    "  \n",
    "def extract_depths_char(taxonomy):\n",
    "    depths_char = []\n",
    "\n",
    "    def find_characteristic_depths(node, depth):\n",
    "        if isinstance(node, dict):\n",
    "            for child in node.values():\n",
    "                find_characteristic_depths(child, depth + 1)\n",
    "        else:\n",
    "            depths_char.append(depth)  # Record the depth of this characteristic\n",
    "\n",
    "    find_characteristic_depths(taxonomy, 0)  # Start from depth 0\n",
    "    return depths_char\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_conciseness(ncat, nchar, depths_cat, depths_char):\n",
    "    \"\"\"\n",
    "    Calculate the conciseness of the taxonomy using the proposed formula.\n",
    "\n",
    "    Parameters:\n",
    "    ncat (int): The number of categories.\n",
    "    nchar (int): The number of characteristics.\n",
    "    depths_cat (list): A list of depths for categories.\n",
    "    depths_char (list): A list of depths for characteristics.\n",
    "\n",
    "    Returns:\n",
    "    float: The conciseness value of the taxonomy.\n",
    "    \"\"\"\n",
    "    # Calculate the sum of the inverses of the depths for categories and characteristics\n",
    "    # Only include depths greater than 0 to avoid division by zero\n",
    "    sum_cat = sum(1 / d for d in depths_cat if d > 0) if ncat > 0 else 0  # Sum for categories\n",
    "    sum_char = sum(1 / d for d in depths_char if d > 0) if nchar > 0 else 0  # Sum for characteristics\n",
    "\n",
    "    # Calculate the total sum of inverses of depths\n",
    "    total_sum = sum_cat + sum_char\n",
    "\n",
    "    # Calculate conciseness using the provided formula\n",
    "    if total_sum > 0:\n",
    "        C_T = 1 / (1 + math.log(total_sum - 1))\n",
    "    else:\n",
    "        C_T = 0  # Return 0 if total_sum is not positive\n",
    "\n",
    "    return C_T\n",
    "\n",
    "  \n",
    "ncat = extract_ncat(taxonomy_tree)\n",
    "nchar = extract_nchar(taxonomy_tree)\n",
    "depths_cat = extract_depths_cat(taxonomy_tree)\n",
    "depths_char = extract_depths_char(taxonomy_tree)\n",
    "\n",
    "print(\"Number of categories (ncat):\", ncat)\n",
    "print(\"Number of characteristics (nchar):\", nchar)\n",
    "print(\"Depths of categories:\", depths_cat)\n",
    "print(\"Depths of characteristics:\", depths_char)\n",
    "\n",
    "# Calculate R(T) for the given taxonomy\n",
    "leaves=get_leaf_nodes(taxonomy_tree)\n",
    "print(leaves)\n",
    "robustness_value = calculate_r_t(taxonomy_tree)\n",
    "print(f\"Robustness R(T): {robustness_value:.4f}\")\n",
    "conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)\n",
    "print(f'The conciseness of the taxonomy is: {conciseness}')\n",
    "\n",
    "```\n",
    "\n",
    "## 1st paper a software cost estimation taxonomy for global software development projects\n",
    "\n",
    "``` {python}\n",
    "new_taxonomy = {\n",
    "    'Cost estimation for GSD': {\n",
    "        'Cost estimation context': {\n",
    "            'Planning': {\n",
    "                \"Conceptualization\": \"Conceptualization\",\n",
    "                \"Feasibility study\": \"Feasibility study\",\n",
    "                \"Preliminary planning\": \"Preliminary planning\",\n",
    "                \"Detail Planning\": \"Detail planning\",\n",
    "                \"Execution\": \"Execution\",\n",
    "                \"Commissioning\": \"Commissioning\"\n",
    "            },\n",
    "            'Project activities': {\n",
    "                \"System investigation\": \"System investigation\",\n",
    "                \"Analysis\": \"Analysis\",\n",
    "                \"Design\": \"Design\",\n",
    "                \"Implementation\": \"Implementation\",\n",
    "                \"Testing\": \"Testing\",\n",
    "                \"Maintenance\": \"Maintenance\",\n",
    "                \"Other Project Activities\": \"Project Activities.Other\"\n",
    "            },\n",
    "            'Project domain': {\n",
    "                \"SE\": \"Systems Engineering\",\n",
    "                \"Research & Dev\": {\n",
    "                    \"Telecommunication\": \"Telecommunication\"\n",
    "                },\n",
    "                \"Finance\": \"Finance\",\n",
    "                \"Healthcare\": \"Healthcare\",\n",
    "                \"Other Project Domain\": \"Project Domain.Other\"\n",
    "            },\n",
    "            'Project setting': {\n",
    "                \"Close onshore\": \"Close onshore\",\n",
    "                \"Distant onshore\": \"Distant onshore\",\n",
    "                \"Near offshore\": \"Near offshore\",\n",
    "                \"Far offshore\": \"Far offshore\"\n",
    "            },\n",
    "            'Planning approaches': {\n",
    "                \"Constructive Cost Model\": \"Constructive Cost Model\",\n",
    "                \"Capability Maturity Model Integration\": \"Capability Maturity Model Integration\",\n",
    "                \"Agile\": \"Agile\",\n",
    "                \"Delphi\": \"Delphi\",\n",
    "                \"GA\": \"Genetic Algorithms\",\n",
    "                \"CBR\": \"Case-Based Reasoning\",\n",
    "                \"Fuzzy similar\": \"Fuzzy similar\",\n",
    "                \"Other planning approaches\": \"Planning Approaches.other\"\n",
    "            },\n",
    "            'Number of sites': {\n",
    "                \"Value of number of sites\": \"Number of sites.Value\"\n",
    "            },\n",
    "            'Team size': {\n",
    "                \"No of team members\": \"Number of team members\"\n",
    "            }\n",
    "        },\n",
    "        'Estimation technique': {\n",
    "            'Estimation technique': {\n",
    "                \"Expert judgment\": \"Expert judgment\",\n",
    "                \"Machine learning\": \"Machine learning\",\n",
    "                \"Non-machine learning\": \"Non-machine learning\"\n",
    "            },\n",
    "            'Use technique': {\n",
    "                \"Individual\": \"Individual\",\n",
    "                \"Group-based estimation\": \"Group-based estimation\"\n",
    "            }\n",
    "        },\n",
    "        'Cost estimate': {\n",
    "            'Estimated cost': {\n",
    "                \"Estimate value\": \"Estimated value\"\n",
    "            },\n",
    "            'Actual cost': {\n",
    "                \"Value\": \"Actual cost.Value\"\n",
    "            },\n",
    "            'Estimation dimension': {\n",
    "                \"Effort hours\": \"Effort hours\",\n",
    "                \"Staff/cost\": \"Staff/cost\",\n",
    "                \"Hardware\": \"Hardware\",\n",
    "                \"Risk\": \"Risk\",\n",
    "                \"Portfolio\": \"Portfolio\"\n",
    "            },\n",
    "            'Accuracy measure': {\n",
    "                \"Baseline comparison\": \"Baseline comparison\",\n",
    "                \"Variation reduction\": \"Variation reduction\",\n",
    "                \"Sensitivity analysis\": \"Sensitivity analysis\"\n",
    "            }\n",
    "        },\n",
    "        'Cost estimators': {\n",
    "            'Product size': {\n",
    "                \"Size report\": \"Size report\",\n",
    "                \"Statistics analysis\": \"Statistics analysis\"\n",
    "            },\n",
    "            'Team experience': {\n",
    "                \"Considered\": \"Team experience.Considered\",\n",
    "                \"Not considered\": \"Team experience.Not considered\"\n",
    "            },\n",
    "            'Team structure': {\n",
    "                \"Considered\": \"Team structure.Considered\",\n",
    "                \"Not Considered\": \"Team structure.Not considered\"\n",
    "            },\n",
    "            'Product requirement': {\n",
    "                \"Performance\": \"Performance\",\n",
    "                \"Security\": \"Security\",\n",
    "                \"Availability\": \"Availability\",\n",
    "                \"Reliability\": \"Reliability\",\n",
    "                \"Maintainability\": \"Maintainability\",\n",
    "                \"Other requirement\": \"Producte requirement.Other\"\n",
    "            },\n",
    "            'Distributed teams distances': {\n",
    "                \"Geographical distance\": \"Geographical distance\",\n",
    "                \"Temporal distance\": \"Temporal distance\",\n",
    "                \"Socio-cultural distance\": \"Socio-cultural distance\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bajta_tax = new_taxonomy\n",
    "```\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE} leaves = get_leaf_nodes(new_taxonomy)\n",
    "print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "    ## 2nd paper, A taxonomy of web effort predictors\n",
    "    ```{python}\n",
    "    new_taxonomy = {\n",
    "        'Web Predictor': {\n",
    "            'Size Metric': {\n",
    "                'Length': {\n",
    "                            'Web page count': 'Web page count',\n",
    "                            'Media count': 'Media count',\n",
    "                            'New media count': 'New media count',\n",
    "                            'New Web page count': 'New Web page count',\n",
    "                            'Link count': 'Link count',\n",
    "                            'Program count': 'Program count',\n",
    "                            'Reused component count': 'Reused component count',\n",
    "                            'Lines of code': 'Lines of code',\n",
    "                            'Reused program count': 'Reused program count',\n",
    "                            'Reused media count': 'Reused media count',\n",
    "                            'Web page allocation': 'Web page allocation',\n",
    "                            'Reused lines of code': 'Reused lines of code',\n",
    "                            'Media allocation': 'Media allocation',\n",
    "                            'Reused media allocation': 'Reused media allocation',\n",
    "                            'Entity count': 'Entity count',\n",
    "                            'Attribute count': 'Attribute count',\n",
    "                            'Component count': 'Component count',\n",
    "                            'Statement count': 'Statement count',\n",
    "                            'Node count': 'Node count',\n",
    "                            'Collection slot size': 'Collection slot size',\n",
    "                            'Component granularity level': 'Component granularity level',\n",
    "                            'Slot granularity level': 'Slot granularity level',\n",
    "                            'Model node size': 'Model node size',\n",
    "                            'Cluster node size': 'Cluster node size',\n",
    "                            'Node slot size': 'Node slot size',\n",
    "                            'Publishing model unit count': 'Publishing model unit count',\n",
    "                            'Model slot size': 'Model slot size',\n",
    "                            'Association slot size': 'Association slot size',\n",
    "                            'Client script count': 'Client script count',\n",
    "                            'Server script count': 'Server script count',\n",
    "                            'Information slot count': 'Information slot count',\n",
    "                            'Association center slot count': 'Association center slot count',\n",
    "                            'Collection center slot count': 'Collection center slot count',\n",
    "                            'Component slot count': 'Component slot count',\n",
    "                            'Semantic association count': 'Semantic association count',\n",
    "                            'Segment count': 'Segment count',\n",
    "                            'Slot count': 'Slot count',\n",
    "                            'Cluster slot count': 'Cluster slot count',\n",
    "                            'Cluster count': 'Cluster count',\n",
    "                            'Publishing unit count': 'Publishing unit count',\n",
    "                            'Section count': 'Section count',\n",
    "                            'Inner/sub concern count': 'Inner/sub concern count',\n",
    "                            'Indifferent concern count': 'Indifferent concern count',\n",
    "                            'Module point cut count': 'Module point cut count',\n",
    "                            'Module count': 'Module count',\n",
    "                            'Module attribute count': 'Module attribute count',\n",
    "                            'Operation count': 'Operation count',\n",
    "                            'Comment count': 'Comment count',\n",
    "                            'Reused comment count': 'Reused comment count',\n",
    "                            'Media duration': 'Media duration',\n",
    "                            'Diffusion cut count': 'Diffusion cut count',\n",
    "                            'Concern module count': 'Concern module count',\n",
    "                            'Concern operation count': 'Concern operation count',\n",
    "                            'Anchor count': 'Anchor count'},\n",
    "                'Functionality': {\n",
    "                            'High feature count': 'High feature count',\n",
    "                            'Low feature count': 'Low feature count',\n",
    "                            'Reused high feature count': 'Reused high feature count',\n",
    "                            'Reused low feature count': 'Reused low feature count',\n",
    "                            'Web objects': 'Web objects',\n",
    "                            'Common Software Measurement International Consortium': 'Common Software Measurement International Consortium',\n",
    "                            'International Function Point Users Group': 'International Function Point Users Group',\n",
    "                            'Object-Oriented Heuristic Function Points': 'Object-Oriented Heuristic Function Points',\n",
    "                            'Object-Oriented Function Points': 'Object-Oriented Function Points',\n",
    "                            'Use case count': 'Use case count',\n",
    "                            'Feature count': 'Feature count',\n",
    "                            'Data Web points': 'Data Web points'},\n",
    "                \n",
    "                'Object-oriented': {\n",
    "                            'Cohesion': 'Cohesion',\n",
    "                            'Class coupling': 'Class coupling',\n",
    "                            'Concern coupling': 'Concern coupling'}, \n",
    "\n",
    "                'Complexity': {\n",
    "                            'Connectivity density': 'Connectivity density',\n",
    "                            'Cyclomatic complexity': 'Cyclomatic complexity',\n",
    "                            'Model collection complexity': 'Model collection complexity',\n",
    "                            'Model association complexity': 'Model association complexity',\n",
    "                            'Model link complexity': 'Model link complexity',\n",
    "                            'Page complexity': 'Page complexity',\n",
    "                            'Component complexity': 'Component complexity',\n",
    "                            'Total complexity': 'Total complexity',\n",
    "                            'Adaptation complexity': 'Adaptation complexity',\n",
    "                            'New complexity': 'New complexity',\n",
    "                            'Data usage complexity': 'Data usage complexity',\n",
    "                            'Data flow complexity': 'Data flow complexity',\n",
    "                            'Cohesion complexity': 'Cohesion complexity',\n",
    "                            'Interface complexity': 'Interface complexity',\n",
    "                            'Control flow complexity': 'Control flow complexity',\n",
    "                            'Class complexity': 'Class complexity',\n",
    "                            'Layout complexity': 'Layout complexity',\n",
    "                            'Input complexity': 'Input complexity',\n",
    "                            'Output complexity': 'Output complexity'} \n",
    "                            },\n",
    "            'Cost Driver': {\n",
    "              'Product':{\n",
    "                'Type of product': 'Product.Type',\n",
    "                'Stratum': 'Stratum',\n",
    "                'Compactness': 'Compactness',\n",
    "                'Structure': 'Structure',\n",
    "                'Architecture': 'Architecture',\n",
    "                'Integration with legacy systems': 'Integration with legacy systems',\n",
    "                'Concurrency level': 'Concurrency level',\n",
    "                'Processing requirements': 'Processing requirements',\n",
    "                'Database size': 'Database size',\n",
    "                'Requirements volatility level': 'Requirements volatility level',\n",
    "                'Requirements novelty level': 'Requirements novelty level',\n",
    "                'Reliability level': 'Reliability level',\n",
    "                'Maintainability level': 'Maintainability level',\n",
    "                'Time efficiency level': 'Time efficiency level',\n",
    "                'Memory efficiency level': 'Memory efficiency level',\n",
    "                'Portability level': 'Portability level',\n",
    "                'Scalability level': 'Scalability level',\n",
    "                'Quality level': 'Quality level',\n",
    "                'Usability level': 'Usability level',\n",
    "                'Readability level': 'Readability level',\n",
    "                'Security level': 'Security level',\n",
    "                'Installability level': 'Installability level',\n",
    "                'Modularity level': 'Modularity level',\n",
    "                'Flexibility level': 'Flexibility level',\n",
    "                'Testability level': 'Testability level',\n",
    "                'Accessibility level': 'Accessibility level',\n",
    "                'Trainability level': 'Trainability level',\n",
    "                'Innovation level': 'Innovation level',\n",
    "                'Technical factors': 'Technical factors',\n",
    "                'Storage constraint': 'Storage constraint',\n",
    "                'Reusability level': 'Reusability level',\n",
    "                'Robustness level': 'Robustness level',\n",
    "                'Design volatility': 'Design volatility',\n",
    "                'Experience level': 'Experience level',\n",
    "                'Requirements clarity level': 'Requirements clarity level'},\n",
    "            'Client': {\n",
    "                'Availability level': 'Availability level',\n",
    "                'IT literacy': 'IT literacy',\n",
    "                'Mapped workflows': 'Mapped workflows',\n",
    "                'Personality of client': 'Client.Personality'},\n",
    "                \n",
    "            'Development Company': {\n",
    "                'SPI program': 'SPI program',\n",
    "                'Metrics’ program': 'Metrics’ program',\n",
    "                'Number of projects in parallel': 'Number of projects in parallel',\n",
    "                'Software reuse': 'Software reuse'},\n",
    "            'Project': {\n",
    "                'Documentation level': 'Documentation level',\n",
    "                'Number of programming languages': 'Number of programming languages',\n",
    "                'Type of project': 'Project.Type',\n",
    "                'Process efficiency level': 'Process efficiency level',\n",
    "                'Project management level': 'Project management level',\n",
    "                'Infrastructure': 'Infrastructure',\n",
    "                'Development restriction': 'Development restriction',\n",
    "                'Time restriction': 'Time restriction',\n",
    "                'Risk level': 'Risk level',\n",
    "                'Rapid app development': 'Rapid app development',\n",
    "                'Operational mode': 'Operational mode',\n",
    "                'Resource level': 'Resource level',\n",
    "                'Lessons learned repository': 'Lessons learned repository'},            \n",
    "            'Team': {\n",
    "                'Domain experience level': 'Domain experience level',\n",
    "                'Team size': 'Team size',\n",
    "                'Deployment platform experience level': 'Deployment platform experience level',\n",
    "                'Team capability': 'Team capability',\n",
    "                'Programming language experience level': 'Programming language experience level',\n",
    "                'Tool experience level': 'Tool experience level',\n",
    "                'Communication level': 'Communication level',\n",
    "                'Software development experience': 'Software development experience',\n",
    "                'Work Team level': 'Work Team level',\n",
    "                'Stability level': 'Stability level',\n",
    "                'Motivation level': 'Motivation level',\n",
    "                'Focus factor': 'Focus factor',\n",
    "                'Tool experience level': 'Tool experience level',\n",
    "                'OO experience level': 'OO experience level',\n",
    "                'In-house experience': 'In-house experience'},\n",
    "            'Technology': {\n",
    "                'Authoring tool type': 'Authoring tool type',\n",
    "                'Productivity level': 'Productivity level',\n",
    "                'Novelty level': 'Novelty level',\n",
    "                'Platform volatility level': 'Platform volatility level',\n",
    "                'Difficulty level': 'Difficulty level',\n",
    "                'Platform support level': 'Platform support level'}}\n",
    "              \n",
    "    }\n",
    "    }\n",
    "\n",
    "    britto1_tax=new_taxonomy\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE}\n",
    "\n",
    "leaves = get_leaf_nodes(new_taxonomy) print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "\n",
    "    ## 3rd Paper A specialized global software engineering taxonomy for effort estimation\n",
    "    ```{python}\n",
    "\n",
    "    new_taxonomy = {\n",
    "        'GSE': {\n",
    "            'Project': {\n",
    "                'Site': {\n",
    "                    \"Location\": \"Location\",\n",
    "                    \"Legal Entity\": \"Legal Entity\",\n",
    "                    \"Geographic Distance\": \"Geographic Distance\",\n",
    "                    \"Temporal Distance\": \"Temporal Distance\",\n",
    "                    \"Estimation stage\": {\n",
    "                        \"Early Estimation stage\": \"Estimation stage.Early\",\n",
    "                        \"Early & Late Estimation stage\": \"Estimation stage.Early & Late\",\n",
    "                        \"Late Estimation stage\": \"Estimation stage.Late\"\n",
    "                    },\n",
    "                    \"Estimation process role\": {\n",
    "                        \"Estimator\": \"Estimator\",\n",
    "                        \"Estimator & Provider\": \"Estimator & Provider\",\n",
    "                        \"Provider\": \"Provider\"\n",
    "                    }\n",
    "                },\n",
    "                'Relationship': {\n",
    "                    \"Location\": \"Location\",\n",
    "                    \"Legal Entity\": \"Legal Entity\",\n",
    "                    \"Geographic Distance\": \"Geographic Distance\",\n",
    "                    \"Temporal Distance\": \"Temporal Distance\",\n",
    "                    \"Estimation process architectural model\": {\n",
    "                        \"Centralized\": \"Centralized\",\n",
    "                        \"Distributed\": \"Distributed\",\n",
    "                        \"Semi-distributed\": \"Semi-distributed\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    britto2_tax=new_taxonomy\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE}\n",
    "\n",
    "leaves = get_leaf_nodes(new_taxonomy) print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "    ## 4rth Paper: A taxonomy of Approaches and Methods for Software Effort Estimation\n",
    "    ```{python}\n",
    "    new_taxonomy = {\n",
    "        'Software estimation': {\n",
    "            'Basic Estimating Methods': {\n",
    "                \"Algorithmic\": {\n",
    "                    \"Constructive Cost Model\": \"Constructive Cost Model\",\n",
    "                    \"Software Life Cycle Management\": \"Software Life Cycle Management\",\n",
    "                    \"Software Evaluation and Estimation for Risk\": \"Software Evaluation and Estimation for Risk\"\n",
    "                },\n",
    "                \"Non-Algorithmic\": {\n",
    "                    \"Expert Judgment\": \"Expert Judgment\",  # Corrected spelling\n",
    "                    \"Analogy-Based\": \"Analogy-Based\"\n",
    "                }\n",
    "            },\n",
    "            'Combined Estimating Methods': {\n",
    "                \"Basic-Combination\": \"Basic-Combination\",\n",
    "                \"Legal Entity\": \"Legal Entity\",\n",
    "                \"Estimation process architectural model\": {\n",
    "                    \"Fuzzy Logic\": \"Fuzzy Logic\",\n",
    "                    \"Artificial Neural Networks\": \"Artificial Neural Networks\",\n",
    "                    \"Computational Intelligence\": {  # Corrected spelling\n",
    "                        \"swarm\": \"swarm\",\n",
    "                        \"evolutionary\": \"evolutionary\"\n",
    "                    }\n",
    "                },\n",
    "                \"AI-Combined hybrid\": \"AI-Combined hybrid\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    dashti_tax=new_taxonomy\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE}\n",
    "\n",
    "leaves = get_leaf_nodes(new_taxonomy) print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "    ## 5th Paper, Towards a Taxonomy of Hypermedia and Web Application Size Metrics. \n",
    "    ```{python}\n",
    "\n",
    "    new_taxonomy = {\n",
    "      \"Hypermedia and Web Application Size Metrics\":{\n",
    "        \"Motivation\":{\"Motivation\":\"Motivation\"},\n",
    "        \"Harvesting time\":{\n",
    "          \"Early\":\"Early size metric\",\n",
    "          \"Late\":\"Late size metric\"},\n",
    "        \"Metric foundation\":{\n",
    "          \"Problem-oriented metric\":\"Problem-oriented metric\",\n",
    "          \"Solution-oriented metric\":\"Solution-oriented metric\"},\n",
    "        \"Class\":{\n",
    "          \"Length\":\"Length\",\n",
    "          \"Functionality\":\"Functionality\",\n",
    "          \"Complexity\":\"Complexity\"},\n",
    "        \"Entity\":{\n",
    "          \"Web hypermedia application\":\"Web hypermedia application\",\n",
    "          \"Web software application\":\"Web software application\",\n",
    "          \"Web application\":\"Web application\",\n",
    "          \"Media\":\"Media\",\n",
    "          \"Program/Script\":\"Program/Sript\"},\n",
    "        \"Measurement Scale\":{\n",
    "          \"Nominal\":\"Nominal\",\n",
    "          \"Ordinal\":\"Ordinal\",\n",
    "          \"Interval\":\"Interval\",\n",
    "          \"Ratio\":\"Ratio\",\n",
    "          \"Absolute\":\"Absolute\"},\n",
    "        \"Computation\":{\n",
    "          \"Direct\":\"Direct\",\n",
    "          \"Indirect\":\"Indirect\"},\n",
    "        \"Validation\":{\n",
    "          \"Validated Empirically\":\"Validated Empirically\",\n",
    "          \"Validated Theoretically\":\"Validated Theoretically\",\n",
    "          \"Both Empirically and Theoretically\":\"Validation.Both\",\n",
    "          \"No Validation\":\"Validation.None\"},\n",
    "        \"Model dependency\":{\n",
    "          \"Specific\":\"Specific\",\n",
    "          \"Nonspecific\":\"Nonspecific\"}\n",
    "    }\n",
    "    }\n",
    "\n",
    "    mendes_tax=new_taxonomy\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE}\n",
    "\n",
    "leaves = get_leaf_nodes(new_taxonomy) print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "    ##4 6th Paper, An Effort Estimation Taxonomy for Agile Software Development\n",
    "    ```{python}\n",
    "    new_taxonomy = {\n",
    "        'Effort Estimation in ASD': {\n",
    "            'Estimation context': {\n",
    "                \"Planning level\": {\n",
    "                    \"Release Planning level\": \"Planning level.Release\",\n",
    "                    \"Sprint Planning level\": \"Planning level.Sprint\",\n",
    "                    \"Daily Planning level\": \"Planning level.Daily\",\n",
    "                    \"Bidding Planning level\": \"Planning level.Bidding\"\n",
    "                },\n",
    "                \"Estimated activities\": {\n",
    "                    \"Analysis\": \"Analysis\",\n",
    "                    \"Design\": \"Design\",\n",
    "                    \"Implementation\": \"Implementation\",\n",
    "                    \"Testing\": \"Testing\",\n",
    "                    \"Maintenance\": \"Maintenance\",\n",
    "                    \"All estimateed activities\": \"Estimated activities.All\"\n",
    "                },\n",
    "                \"Agile methods\": {\n",
    "                    \"Extreme Programming\": \"Extreme Programming\",\n",
    "                    \"Scrum\": \"Scrum\",\n",
    "                    \"Customized Extreme Programming\": \"Customized Extreme Programming\",\n",
    "                    \"Customized Scrum\": \"Customized Scrum\",\n",
    "                    \"Dynamic Systems Development Method\": \"Dynamic Systems Development Method\",\n",
    "                    \"Crystal\": \"Crystal\",\n",
    "                    \"Feature-Driven Development\": \"Feature-Driven Development\",\n",
    "                    \"Kanban\": \"Kanban\"\n",
    "                },\n",
    "                \"Project domain\": {\n",
    "                    \"Communications industry\": \"Communications industry\",\n",
    "                    \"Transportation\": \"Transportation\",\n",
    "                    \"Financial\": \"Financial\",\n",
    "                    \"Education\": \"Education\",\n",
    "                    \"Health\": \"Health\",\n",
    "                    \"Retail/Wholesale\": \"Retail/Wholesale\",\n",
    "                    \"Manufacturing\": \"Manufacturing\",\n",
    "                    \"Government/Military\": \"Government/Military\",\n",
    "                    \"Other project domain\": \"Project somain.Other\"\n",
    "                },\n",
    "                \"Project setting\": {\n",
    "                    \"Co-located Project setting\": \"Project setting.Co-located\",\n",
    "                    \"Distributed: Close Onshore\": \"Distributed: Close Onshore\",\n",
    "                    \"Distributed: Distant Onshore\": \"Distributed: Distant Onshore\",\n",
    "                    \"Distributed: Near Offshore\": \"Distributed: Near Offshore\",\n",
    "                    \"Distributed: Far Offshore\": \"Distributed: Far Offshore\"\n",
    "                },\n",
    "                \"Estimation entity\": {\n",
    "                    \"User story Estimation entity\": \"User story\",\n",
    "                    \"Task Estimation entity\": \"Task\",\n",
    "                    \"Use case Estimation entity\": \"Use case\",\n",
    "                    \"Other Estimation entity\": \"Estimation entity.Other\"\n",
    "                },\n",
    "                \"Number of entities estimated\": {\n",
    "                    \"Number of entities estimated\": \"Number of entities estimated\"\n",
    "                },\n",
    "                \"Team size\": {\n",
    "                    \"No. of team members\": \"Team size.Value\"\n",
    "                }\n",
    "            },\n",
    "            'Estimation technique': {\n",
    "                \"Estimation Techniques\": {\n",
    "                    \"Planning Poker\": \"Planning Poker\",\n",
    "                    \"Expert Judgement\": \"Expert Judgement\",\n",
    "                    \"Analogy\": \"Analogy\",\n",
    "                    \"Use case points method\": \"Use case points method\",\n",
    "                    \"Other estimation technique\": \"Estimation technique.Other\"\n",
    "                },\n",
    "                \"Type\": {\n",
    "                    \"Single type\": \"Type.Single\",\n",
    "                    \"Group type\": \"Type.Group\"\n",
    "                }\n",
    "            },\n",
    "            'Effort predictors': {\n",
    "                \"Size\": {\n",
    "                    \"Story points\": \"Story points\",\n",
    "                    \"User case points\": \"User case points\",\n",
    "                    \"Function points\": \"Function points\",\n",
    "                    \"Other Effort predictors\": \"Other Effort predictors\",\n",
    "                    \"Not used Effort predictors\": \"Not used Effort predictors\",\n",
    "                    \"Considered without any metric\": \"Considered without any metric\"\n",
    "                },\n",
    "                \"Team's prior experience\": {\n",
    "                    \"Considered Team's prior experience\": \"Team's prior experience.Considered\",\n",
    "                    \"Not Considered Team's prior experience\": \"Team's prior experience.Not Considered\"\n",
    "                },\n",
    "                \"Team's skill level\": {\n",
    "                    \"Considered Team's skill level\": \"Team's skill level.Considered\",\n",
    "                    \"Not Considered Team's skill level\": \"Team's skill level.Not Considered\"\n",
    "                },\n",
    "                \"Non functional requirements\": {\n",
    "                    \"Performance\": \"Performance\",\n",
    "                    \"Security\": \"Security\",\n",
    "                    \"Availability\": \"Availability\",\n",
    "                    \"Reliability\": \"Reliability\",\n",
    "                    \"Maintainability\": \"Maintainability\",\n",
    "                    \"Other Non functional requirements\": \"Non functional requirements.Other\",  # Changed period to comma\n",
    "                    \"Not considered Non functional requirements\": \"Non functional requirements.Not considered\"\n",
    "                },\n",
    "                \"Distributed teams' issues\": {\n",
    "                    \"Considered Distributed teams\": \"Distributed teams.Considered\",\n",
    "                    \"Not Considered Distributed teams\": \"Distributed teams.Not Considered\",\n",
    "                    \"Not applicable Distributed teams\": \"Distributed teams.Not applicable\"\n",
    "                },\n",
    "                \"Customer Communication\": {\n",
    "                    \"Considered Customer Communication\": \"Customer Communication.Considered\",\n",
    "                    \"Not Considered Customer Communication\": \"Customer Communication.Not Considered\"\n",
    "                }\n",
    "            },\n",
    "            'Effort estimate': {\n",
    "                \"Estimated effort\": {\n",
    "                    \"Estimate value(s)\": \"Estimate value(s)\"\n",
    "                },\n",
    "                \"Actual effort\": {\n",
    "                    \"Actual effort Value\": \"Actual effort.Value\"\n",
    "                },\n",
    "                \"Type\": {\n",
    "                    \"Point Type\": \"Point Type\",\n",
    "                    \"Three point Type\": \"Three point Type\",\n",
    "                    \"Distribution Type\": \"Distribution Type\",\n",
    "                    \"Other Type\": \"Other Type\"\n",
    "                },\n",
    "                \"Unit\": {\n",
    "                    \"House/days\": \"House/days\",\n",
    "                    \"Pair days\": \"Pair/days\",\n",
    "                    \"Ideal hours\": \"Ideal hours\",\n",
    "                    \"Other Unit\": \"Unit.Other\"\n",
    "                },\n",
    "                \"Accuracy Level\": {\n",
    "                    \"Accuracy Level Value\": \"Accuracy Level.Value\"\n",
    "                },\n",
    "                \"Accuracy measure\": {\n",
    "                    \"Mean Magnitude of Relative Error\": \"Mean Magnitude of Relative Error\",\n",
    "                    \"Median Magnitude of Relative Error\": \"Median Magnitude of Relative Error\",\n",
    "                    \"Bias of Relative Error\": \"Bias of Relative Error\",\n",
    "                    \"Other Accuracy measure\": \"Accuracy measure.Other\",\n",
    "                    \"Not used Accuracy measure\": \"Accuracy measure.Not used\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    usman_tax=new_taxonomy\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE}\n",
    "\n",
    "leaves = get_leaf_nodes(new_taxonomy) print(leaves)\n",
    "\n",
    "ncat = extract_ncat(new_taxonomy) nchar = extract_nchar(new_taxonomy)\n",
    "depths_cat = extract_depths_cat(new_taxonomy) depths_char =\n",
    "extract_depths_char(new_taxonomy)\n",
    "\n",
    "print(“Number of categories (ncat):”, ncat) print(“Number of\n",
    "characteristics (nchar):”, nchar) print(“Depths of categories:”,\n",
    "depths_cat) print(“Depths of characteristics:”, depths_char)\n",
    "\n",
    "robustness_value = calculate_r\\_t(new_taxonomy) print(f”Robustness R(T):\n",
    "{robustness_value:.4f}“) conciseness= calculate_conciseness(ncat, nchar,\n",
    "depths_cat, depths_char) print(f’The conciseness of the taxonomy is:\n",
    "{conciseness}’)\n",
    "\n",
    "\n",
    "    ```{python}\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    import matplotlib\n",
    "    plt.clf()\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')  # You can change this to any available style\n",
    "\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    # Extracting categories sets\n",
    "    def extract_intermediate_elements(taxonomy, result=None):\n",
    "        if result is None:\n",
    "            result = set()\n",
    "\n",
    "        for key, value in taxonomy.items():\n",
    "            if isinstance(value, dict):\n",
    "                result.add(key)\n",
    "                extract_intermediate_elements(value, result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    bajta_tax_categories = extract_intermediate_elements(bajta_tax)\n",
    "    britto1_tax_categories = extract_intermediate_elements(britto1_tax)\n",
    "    britto2_tax_categories = extract_intermediate_elements(britto2_tax)\n",
    "    dashti_tax_categories = extract_intermediate_elements(dashti_tax)\n",
    "    mendes_tax_categories = extract_intermediate_elements(mendes_tax)\n",
    "    usman_tax_categories = extract_intermediate_elements(usman_tax)\n",
    "\n",
    "    print({'Bajta': bajta_tax_categories})\n",
    "    print({'Britto_2017':britto1_tax_categories})\n",
    "    print({'Britto_2016':britto2_tax_categories})\n",
    "    print({'Dashti':dashti_tax_categories})\n",
    "    print({'Mendes':mendes_tax_categories})\n",
    "    print({'Usman':usman_tax_categories})\n",
    "\n",
    "    sets = {\n",
    "        'Bajta': bajta_tax_categories,\n",
    "        'Britto_2017': britto1_tax_categories,\n",
    "        'Britto_2016': britto2_tax_categories,\n",
    "        'Dashti': dashti_tax_categories,\n",
    "        'Mendes': mendes_tax_categories,\n",
    "        'Usman': usman_tax_categories\n",
    "    }\n",
    "\n",
    "\n",
    "    # Extract characteristics (Execute only one of the two, or characteristics set or categories set)\n",
    "\n",
    "    def extract_leaf_elements(nested_dict):\n",
    "        \"\"\"Recursively extract leaf nodes from a nested dictionary.\"\"\"\n",
    "        leaves = set()\n",
    "        for key, value in nested_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively process sub-dictionaries\n",
    "                leaves.update(extract_leaf_elements(value))\n",
    "            else:\n",
    "                # Add the current key as it's a leaf node\n",
    "                leaves.add(value)\n",
    "        return leaves\n",
    "\n",
    "    # Example usage with your taxonomies\n",
    "    bajta_tax_leaves = extract_leaf_elements(bajta_tax)\n",
    "    britto1_tax_leaves = extract_leaf_elements(britto1_tax)\n",
    "    britto2_tax_leaves = extract_leaf_elements(britto2_tax)\n",
    "    dashti_tax_leaves = extract_leaf_elements(dashti_tax)\n",
    "    mendes_tax_leaves = extract_leaf_elements(mendes_tax)\n",
    "    usman_tax_leaves = extract_leaf_elements(usman_tax)\n",
    "\n",
    "    sets = {\n",
    "        'Bajta': bajta_tax_leaves,\n",
    "        'Britto_2017': britto1_tax_leaves,\n",
    "        'Britto_2016': britto2_tax_leaves,\n",
    "        'Dashti': dashti_tax_leaves,\n",
    "        'Mendes': mendes_tax_leaves,\n",
    "        'Usman': usman_tax_leaves\n",
    "    }\n",
    "\n",
    "    # Output the final dictionary\n",
    "\n",
    "    # Step 2: Flatten the sets into a dataframe (assuming 'sets' is already defined)\n",
    "    words = []\n",
    "    labels = []\n",
    "    for label, words_set in sets.items():\n",
    "        for word in words_set:\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Word': words, 'Set': labels})\n",
    "\n",
    "    # Step 3: Load the pre-trained model and tokenizer\n",
    "    model_name = \"jinaai/jina-embeddings-v3\"\n",
    "    if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        print(\"Model and tokenizer are already loaded.\")\n",
    "\n",
    "    # Step 4: Get the embeddings for each word\n",
    "    def get_embeddings(word):\n",
    "        inputs = tokenizer(word, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    embeddings = np.array([get_embeddings(word) for word in df['Word']])\n",
    "\n",
    "    # Step 5: Perform t-SNE (now in 2D)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=5)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Step 6: Convert string labels to numeric labels for coloring\n",
    "    label_encoder = LabelEncoder()\n",
    "    numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Step 7: Create the 2D scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Plot the 2D scatter with the numeric labels for colors\n",
    "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         c=numeric_labels, cmap='Set1', s=100)\n",
    "\n",
    "    # Annotate each point with the word\n",
    "    for i, word in enumerate(df['Word']):\n",
    "        ax.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, word, fontsize=9)\n",
    "\n",
    "    # Step 8: Add labels and title\n",
    "    ax.set_title(\"2D t-SNE Visualization of Word Embeddings\")\n",
    "    ax.set_xlabel(\"t-SNE Dimension 1\")\n",
    "    ax.set_ylabel(\"t-SNE Dimension 2\")\n",
    "\n",
    "    # Step 9: Move the legend outside of the plot\n",
    "    legend_labels = label_encoder.classes_\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                          markerfacecolor=plt.cm.Set2(i / len(legend_labels)), markersize=5) \n",
    "               for i in range(len(legend_labels))]\n",
    "    ax.legend(handles, legend_labels, title=\"Set\", loc=\"center left\", bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)\n",
    "\n",
    "    # Step 10: Show the plot\n",
    "    plt.tight_layout()  # Ensures proper spacing with the legend outside\n",
    "    plt.savefig('word_embeddings.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# K-means Plot\n",
    "\n",
    "``` {python}\n",
    "import random\n",
    "import umap.umap_ as umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib.lines import Line2D  # Add this import at the top of your code\n",
    "colorstyle = \"Set2\"\n",
    "seed=5\n",
    "marker_styles = ['o', '^', 's', 'p', '*', 'D']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # You can change this to any available style\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "\n",
    "# Step 2: Flatten the sets into a dataframe\n",
    "words = []\n",
    "labels = []\n",
    "for label, words_set in sets.items():\n",
    "    for word in words_set:\n",
    "        words.append(word.lower())\n",
    "        labels.append(label)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'Word': words, 'Set': labels})\n",
    "\n",
    "# Step 3: Load the pre-trained model and tokenizer\n",
    "model_name = \"jinaai/jina-embeddings-v3\"\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    print(\"Model and tokenizer are already loaded.\")\n",
    "\n",
    "# Step 4: Get the embeddings for each word\n",
    "def get_embeddings(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings = np.array([get_embeddings(word) for word in df['Word']])\n",
    "\n",
    "# Step 5: Perform 2D UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=5)\n",
    "embeddings_2d = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# Step 6: Create a color map that reflects the set labels\n",
    "unique_labels = list(df['Set'].unique())  # Get the unique set labels\n",
    "cmap = plt.cm.get_cmap(colorstyle, len(unique_labels))  # Create a colormap with enough colors\n",
    "\n",
    "# Step 7: Run K-means on UMAP embeddings\n",
    "num_clusters = len(unique_labels)  # Set number of clusters to match unique labels\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=5)\n",
    "kmeans_labels = kmeans.fit_predict(embeddings_2d)\n",
    "\n",
    "# Step 8: Generate top 4 names for each cluster\n",
    "top_n = 3  # Set how many top words to display for each cluster\n",
    "cluster_names = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # Get the embeddings for words in the current cluster\n",
    "    cluster_indices = np.where(kmeans_labels == i)[0]\n",
    "    cluster_embeddings = embeddings[cluster_indices]\n",
    "    \n",
    "    # Calculate the centroid of the cluster\n",
    "    cluster_centroid = np.mean(cluster_embeddings, axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity of centroid to all words' embeddings to find closest words\n",
    "    similarities = cosine_similarity(cluster_centroid, embeddings).flatten()\n",
    "    \n",
    "    # Get the indices of the top 4 closest words\n",
    "    closest_word_indices = np.argsort(similarities)[-top_n:][::-1]  # Get indices of top 4 closest words\n",
    "    \n",
    "    # Get the words corresponding to these indices\n",
    "    closest_words = df['Word'].iloc[closest_word_indices].tolist()\n",
    "    \n",
    "    # Store the top 4 closest words as the cluster name\n",
    "    cluster_names.append(closest_words)\n",
    "\n",
    "# Step 9: Plot with translucent shapes for each K-means cluster and annotate with top 4 names\n",
    "plt.figure(figsize=(10, 7))\n",
    "color_map = {label: cmap(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Create a list of marker styles to use for each label\n",
    "marker_styles = ['o', '^', 's', 'p', '*', 'D']  # Add more marker styles if needed\n",
    "\n",
    "# Loop through each label and plot with the corresponding marker style\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    # Get the data for the current label\n",
    "    label_data = df[df['Set'] == label]\n",
    "    \n",
    "    # Plot with a different marker for each label\n",
    "    plt.scatter(embeddings_2d[df['Set'] == label, 0], \n",
    "                embeddings_2d[df['Set'] == label, 1],\n",
    "                c=[color_map[label]] * len(label_data), \n",
    "                s=80, \n",
    "                label=label,\n",
    "                marker=marker_styles[i % len(marker_styles)], alpha=0.6)  # Use modulo to cycle through marker styles\n",
    "\n",
    "\n",
    "# Draw convex hulls around each cluster and annotate with cluster names\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = embeddings_2d[kmeans_labels == i]\n",
    "    \n",
    "    if len(cluster_points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        hull_points = cluster_points[hull.vertices]\n",
    "        plt.fill(hull_points[:, 0], hull_points[:, 1], alpha=0.2, \n",
    "                 color=cmap(i), label=f'Cluster {i+1}')\n",
    "    \n",
    "    # Annotate with the top 4 cluster names at the centroid location\n",
    "    cluster_centroid_2d = np.mean(cluster_points, axis=0)\n",
    "    # Join the top 4 words into a string with commas for cleaner display\n",
    "    cluster_name_text = '\\n'.join(cluster_names[i]).upper() \n",
    "    \n",
    "    # Annotate with the top words at the centroid, with slightly smaller font size\n",
    "    plt.text(cluster_centroid_2d[0], cluster_centroid_2d[1], cluster_name_text, \n",
    "             fontsize=8, ha='center', color='black')\n",
    "\n",
    "# Step 10: Custom legend to show colors and shapes for each label\n",
    "plt.title(\"2D UMAP Visualization of Word Embeddings with K-means Clusters\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker=marker_styles[i % len(marker_styles)], color='w', \n",
    "                          markerfacecolor=color_map[label], markersize=10, label=label)\n",
    "                   for i, label in enumerate(unique_labels)]\n",
    "plt.legend(\n",
    "    handles=legend_elements,\n",
    "    title=\"Literature\",\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.2),  # Position it just below the plot\n",
    "    ncol=len(unique_labels),      # Arrange legend items in a single row\n",
    "    frameon=False                 # Optional: Remove legend box frame\n",
    ")\n",
    "# Adjust layout to ensure the legend is not clipped\n",
    "plt.tight_layout()\n",
    "\n",
    "# Step 11: Save the plot in high resolution\n",
    "plt.savefig('word_embeddings_kmeans.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` {python}\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # You can change this to any available style\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Define the sets of words\n",
    "\n",
    "# Combine all sets into a single list with labels\n",
    "word_sets = sets\n",
    "\n",
    "word_sets = {label: {word.lower() for word in words} for label, words in word_sets.items()}\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"jinaai/jina-embeddings-v3\"\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    print(\"Model and tokenizer are already loaded.\")\n",
    "\n",
    "# Function to get embedding for a word\n",
    "def get_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Collect embeddings\n",
    "embeddings = []\n",
    "words = []\n",
    "labels = []\n",
    "\n",
    "for label, words_set in word_sets.items():\n",
    "    for word in words_set:\n",
    "        embedding = get_embedding(word)\n",
    "        embeddings.append(embedding)\n",
    "        words.append(word)\n",
    "        labels.append(label)\n",
    "\n",
    "# Create a DataFrame with words, labels, and embeddings\n",
    "embedding_df = pd.DataFrame({\n",
    "    \"Word\": words,\n",
    "    \"Label\": labels,\n",
    "    \"Embedding\": [emb[0] for emb in embeddings]\n",
    "})\n",
    "\n",
    "# Pivot the DataFrame to have the set labels as columns\n",
    "pivoted_df = embedding_df.pivot(index=\"Word\", columns=\"Label\", values=\"Embedding\")\n",
    "\n",
    "# Flatten the embeddings (if you want to display them properly as vectors, you might want to separate them)\n",
    "# Convert the embedding vectors to string for display purposes (or keep them as arrays if you're working with them in computations)\n",
    "pivoted_df = pivoted_df.applymap(lambda x: str(x.tolist()) if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "# Display the pivoted DataFrame\n",
    "print(pivoted_df)\n",
    "```\n",
    "\n",
    "#3D PLOT\n",
    "\n",
    "``` {python}\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # You can change this to any available style\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "# Step 2: Flatten the sets into a dataframe (assuming sets is already defined)\n",
    "words = []\n",
    "labels = []\n",
    "for label, words_set in sets.items():\n",
    "    for word in words_set:\n",
    "        words.append(word)\n",
    "        labels.append(label)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'Word': words, 'Set': labels})\n",
    "\n",
    "# Step 3: Load the pre-trained model and tokenizer\n",
    "model_name = \"jinaai/jina-embeddings-v3\"\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    print(\"Model and tokenizer are already loaded.\")\n",
    "\n",
    "# Step 4: Get the embeddings for each word\n",
    "def get_embeddings(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings = np.array([get_embeddings(word) for word in df['Word']])\n",
    "\n",
    "# Step 5: Perform 3D UMAP (with 3 components)\n",
    "umap_model = umap.UMAP(n_components=3, random_state=5)\n",
    "embeddings_3d = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# Step 6: Convert string labels to numeric labels for coloring\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Step 7: Create the interactive 3D plot with Plotly\n",
    "fig = px.scatter_3d(df, x=embeddings_3d[:, 0], y=embeddings_3d[:, 1], z=embeddings_3d[:, 2],\n",
    "                    color=labels, text=words,\n",
    "                    labels={'x': 'UMAP Dimension 1', 'y': 'UMAP Dimension 2', 'z': 'UMAP Dimension 3'},\n",
    "                    title=\"3D UMAP Visualization of Word Embeddings\")\n",
    "\n",
    "# Customize the layout for better viewing\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.8), selector=dict(mode='markers+text'))\n",
    "fig.update_layout(scene=dict(xaxis_title='UMAP Dimension 1',\n",
    "                             yaxis_title='UMAP Dimension 2',\n",
    "                             zaxis_title='UMAP Dimension 3'))\n",
    "plt.savefig('3d_word_embedding.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "# Another table showing the common words between papers, a bit harder to read #The defining one. Needed to execute the following chunks of code\n",
    "\n",
    "``` {python}\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # You can change this to any available style\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Define the sets of words\n",
    "sets=sets\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"jinaai/jina-embeddings-v3\"\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    print(\"Model and tokenizer are already loaded.\")\n",
    "\n",
    "# Function to normalize text to lowercase\n",
    "def normalize_words(words):\n",
    "    return {word.lower() for word in words}\n",
    "\n",
    "# Normalize all words in the sets to lowercase\n",
    "normalized_sets = {set_name: normalize_words(word_set) for set_name, word_set in sets.items()}\n",
    "\n",
    "# Function to get embeddings for a list of words\n",
    "def get_embeddings(words):\n",
    "    inputs = tokenizer(list(words), padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings\n",
    "\n",
    "# Create a dictionary to store the embeddings of each set\n",
    "embeddings = {}\n",
    "for set_name, word_set in normalized_sets.items():\n",
    "    embeddings[set_name] = get_embeddings(word_set)\n",
    "\n",
    "# Create a function to calculate the semantic similarity between sets\n",
    "def compute_similarity(set1, set2):\n",
    "    # Get the embeddings for both sets\n",
    "    embeddings1 = embeddings[set1]\n",
    "    embeddings2 = embeddings[set2]\n",
    "    \n",
    "    # Calculate cosine similarity between all pairs of words in set1 and set2\n",
    "    sim_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "# Create a similarity matrix for each pair of sets\n",
    "similarity_results = {}\n",
    "for set1 in normalized_sets.keys():\n",
    "    for set2 in normalized_sets.keys():\n",
    "        if set1 != set2:\n",
    "            sim_matrix = compute_similarity(set1, set2)\n",
    "            similarity_results[(set1, set2)] = sim_matrix\n",
    "\n",
    "# Create a simple table to store the similarity values\n",
    "similarity_table = []\n",
    "\n",
    "# Populate the table with word pairs and their cosine similarity values\n",
    "for (set1, set2), sim_matrix in similarity_results.items():\n",
    "    for i, word1 in enumerate(normalized_sets[set1]):\n",
    "        for j, word2 in enumerate(normalized_sets[set2]):\n",
    "            similarity_table.append({\n",
    "                \"Set 1\": set1,\n",
    "                \"Word 1\": word1,\n",
    "                \"Set 2\": set2,\n",
    "                \"Word 2\": word2,\n",
    "                \"Cosine Similarity\": sim_matrix[i, j]\n",
    "            })\n",
    "\n",
    "# Convert the table to a DataFrame for better display\n",
    "similarity_df = pd.DataFrame(similarity_table)\n",
    "\n",
    "# Filter the DataFrame to keep only cosine similarities above 0.7\n",
    "similarity_df_filtered = similarity_df[similarity_df['Cosine Similarity'] > 0]\n",
    "\n",
    "# Create an empty table to store the words that are similar\n",
    "common_words_table = pd.DataFrame(index=sets.keys(), columns=sets.keys(), dtype=object)\n",
    "\n",
    "# Populate the table with word pairs that have similarity above 0.7\n",
    "for index, row in similarity_df_filtered.iterrows():\n",
    "    set1 = row['Set 1']\n",
    "    word1 = row['Word 1']\n",
    "    set2 = row['Set 2']\n",
    "    word2 = row['Word 2']\n",
    "    \n",
    "    # Check if the cell is empty or needs to be updated with word pairs\n",
    "    if pd.isna(common_words_table.at[set1, set2]):\n",
    "        common_words_table.at[set1, set2] = f\"{word1} - {word2}\"\n",
    "    else:\n",
    "        common_words_table.at[set1, set2] += f\", {word1} - {word2}\"\n",
    "\n",
    "# Display the table showing the common word pairs\n",
    "print(common_words_table)\n",
    "```\n",
    "\n",
    "# making a color table\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "\n",
    "# Step 1: Define the color map for each set\n",
    "set_colors = {\n",
    "    \"Bajta\": \"yellow\",\n",
    "    \"Britto_2016\": \"blue\",\n",
    "    \"Britto_2017\": \"green\",\n",
    "    \"Dashti\": \"red\",\n",
    "    \"Mendes\": \"purple\",\n",
    "    \"Usman\": \"orange\"\n",
    "}\n",
    "\n",
    "# Create the HTML content for the legend\n",
    "legend_html = \"<div style='font-weight: bold; margin-bottom: 10px;'>Legend:</div>\"\n",
    "for set_name, color in set_colors.items():\n",
    "    legend_html += f\"<div><span style='color:{color};'>●</span> {set_name}</div>\"\n",
    "\n",
    "\n",
    "# Step 2: Create the common words table (we will assume this step has already been completed and filtered)\n",
    "common_words_table = pd.DataFrame(index=sets.keys(), columns=[\"Words\", \"Relations\"])\n",
    "\n",
    "# Step 3: Populate the common words table with colored word pairs\n",
    "for index, row in similarity_df_filtered.iterrows():\n",
    "    set1 = row['Set 1']\n",
    "    word1 = row['Word 1']\n",
    "    set2 = row['Set 2']\n",
    "    word2 = row['Word 2']\n",
    "    \n",
    "    # Color the words based on the sets\n",
    "    word1_colored = f'<span style=\"color:{set_colors[set1]}\">{word1}</span>'\n",
    "    word2_colored = f'<span style=\"color:{set_colors[set2]}\">{word2}</span>'\n",
    "    \n",
    "    # Find the row corresponding to set1\n",
    "    current_row = common_words_table.loc[set1]\n",
    "    \n",
    "    if pd.isna(current_row['Words']):\n",
    "        common_words_table.at[set1, 'Words'] = word1\n",
    "        common_words_table.at[set1, 'Relations'] = word2_colored\n",
    "    else:\n",
    "        common_words_table.at[set1, 'Words'] += f\", {word1}\"\n",
    "        common_words_table.at[set1, 'Relations'] += f\", {word2_colored}\"\n",
    "\n",
    "# Step 4: Use `map` to apply the coloring function\n",
    "def colorize_words(word):\n",
    "    \"\"\"\n",
    "    Function to apply color to words using the <span> HTML tag.\n",
    "    \"\"\"\n",
    "    return f\"color: {word.split(':')[1]}\" if isinstance(word, str) and word.startswith('<span') else ''\n",
    "\n",
    "# Step 5: Apply the styling to the DataFrame\n",
    "styled_table = common_words_table.style.applymap(colorize_words, subset=[\"Words\", \"Relations\"])\n",
    "\n",
    "# Display the styled table (if using Jupyter or IPython environment)\n",
    "styled_table\n",
    "\n",
    "\n",
    "# Save the styled table to an HTML file using to_html()\n",
    "html_output = styled_table.to_html()\n",
    "html_output = legend_html + \"<br>\" + html_output\n",
    "\n",
    "# Specify the filename where you want to save the table\n",
    "file_path = 'colored_table.html'\n",
    "\n",
    "# Write the HTML content to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(html_output)\n",
    "\n",
    "print(f\"Styled table saved to {file_path}\")\n",
    "```\n",
    "\n",
    "# New table\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles.colors import Color\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the color map for sets\n",
    "set_colors = {\n",
    "    \"Bajta\": \"#66c2a5\",  # Light green\n",
    "    \"Britto_2016\": \"#fc8d62\",  # Orange\n",
    "    \"Britto_2017\": \"#8da0cb\",  # Blue\n",
    "    \"Dashti\": \"#e78ac3\",  # Pink\n",
    "    \"Mendes\": \"#a6d854\",  # Light green\n",
    "    \"Usman\": \"#ff7f00\"  # Dark orange\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store words grouped by similarity\n",
    "grouped_words = defaultdict(set)\n",
    "\n",
    "# Iterate over the similarity dataframe\n",
    "\n",
    "similarity_df_filtered.loc[similarity_df_filtered[\"Set 1\"] == \"Bajta\", \"Word 1\"].unique()\n",
    "\n",
    "similarity=0.9\n",
    "for index, row in similarity_df_filtered.iterrows():\n",
    "    if row['Cosine Similarity'] >= similarity:\n",
    "        word1 = row['Word 1']\n",
    "        word2 = row['Word 2']\n",
    "        set1 = row['Set 1']\n",
    "        set2 = row['Set 2']\n",
    "        \n",
    "        # Add words to the grouped dictionary with their respective sets\n",
    "        grouped_words[word1].add(set1)\n",
    "        grouped_words[word2].add(set2)\n",
    "\n",
    "# Prepare the data for the final table\n",
    "table_rows = []\n",
    "for word, sets in grouped_words.items():\n",
    "    # Color the words based on the sets they belong to\n",
    "    color_coded_words = []\n",
    "    for set_name in sets:\n",
    "        # Assign the color based on the set\n",
    "        color = set_colors[set_name]\n",
    "        colored_word = f'<span style=\"color:{color}\">{word}</span>'\n",
    "        color_coded_words.append(colored_word)\n",
    "    \n",
    "    # Prepare the row for this word and its sets\n",
    "    sets_str = \", \".join([f'<span style=\"color:{set_colors[set_name]}\">{set_name}</span>' for set_name in sets])\n",
    "    table_rows.append({\n",
    "        \"Categories\": \", \".join(color_coded_words),  # Join words with their color applied\n",
    "        \"Taxonomies\": sets_str  # Color-coded sets\n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame\n",
    "final_table = pd.DataFrame(table_rows)\n",
    "\n",
    "# Collapse the table by the \"Sets\" column, merging words that have the same \"Sets\"\n",
    "collapsed_table = final_table.groupby('Taxonomies', as_index=False).agg({\n",
    "    'Categories': lambda x: ''.join(\n",
    "        [f'<div style=\"text-align:center;\">{word.title()}</div>' for word in sorted(x.str.cat(sep=', ').split(', '))]\n",
    "    )  # Capitalize, center-align each word, and add line breaks\n",
    "})\n",
    "\n",
    "\n",
    "# Display the collapsed table with HTML rendering\n",
    "table_title = f\"<h1>Grouped Words with Color-Coded Taxonomies (Similarity ≥ {similarity})</h1>\"\n",
    "\n",
    "collapsed_html_output_table = collapsed_table.to_html(escape=False)  # escape=False allows HTML in the table\n",
    "\n",
    "collapsed_html_output = table_title + collapsed_html_output_table\n",
    "\n",
    "\n",
    "# Save the collapsed HTML output to a file\n",
    "collapsed_file_path = 'collapsed_grouped_words_table_colored.html'\n",
    "with open(collapsed_file_path, 'w') as file:\n",
    "    file.write(collapsed_html_output)\n",
    "\n",
    "print(f\"Collapsed styled table saved to {collapsed_file_path}\")\n",
    "\n",
    "# Save the Excel file with colors\n",
    "excel_file_path = 'collapsed_grouped_words_table_colored.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Colored Table\"\n",
    "\n",
    "# Write the header\n",
    "header = list(collapsed_table.columns)\n",
    "ws.append(header)\n",
    "\n",
    "# Write the rows with formatting\n",
    "for row in collapsed_table.itertuples(index=False):\n",
    "    # Parse \"Categories\" HTML for coloring\n",
    "    categories_cell = row.Categories\n",
    "    taxonomies_cell = row.Taxonomies\n",
    "\n",
    "    # Parse HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(categories_cell, \"html.parser\")\n",
    "    formatted_words = []\n",
    "    for div in soup.find_all('div'):\n",
    "        word = div.text.strip()\n",
    "        style = div.get('style', '')\n",
    "        color = None\n",
    "        if 'color:' in style:\n",
    "            color = style.split('color:')[1].split(';')[0].strip()\n",
    "        formatted_words.append((word, color))\n",
    "    \n",
    "    # Create the row for Excel\n",
    "    ws_row = [None] * len(header)\n",
    "    ws_row[0] = formatted_words  # Categories with colors\n",
    "    ws_row[1] = taxonomies_cell  # Taxonomies plain text\n",
    "\n",
    "    # Write the row to the sheet\n",
    "    row_num = ws.max_row + 1\n",
    "    for col_num, value in enumerate(ws_row, start=1):\n",
    "        cell = ws.cell(row=row_num, column=col_num)\n",
    "        if col_num == 1 and value:  # Apply formatting to \"Categories\"\n",
    "            for word, color in value:\n",
    "                cell.value = word\n",
    "                cell.alignment = Alignment(horizontal=\"center\", wrap_text=True)\n",
    "                if color:\n",
    "                    cell.font = Font(color=color)\n",
    "        else:\n",
    "            cell.value = value\n",
    "\n",
    "# Save Excel file\n",
    "wb.save(excel_file_path)\n",
    "print(f\"Excel file saved to {excel_file_path}\")\n",
    "```\n",
    "\n",
    "# Colorless table\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Prepare the data for the final table (no grouping by cosine similarity or color coding)\n",
    "table_rows = []\n",
    "for index, row in similarity_df_filtered.iterrows():\n",
    "    word1 = row['Word 1']\n",
    "    set1 = row['Set 1']\n",
    "    \n",
    "    # Add the word and its associated taxonomy\n",
    "    table_rows.append({\n",
    "        \"Categories\": word1.capitalize(),  # Capitalize the word\n",
    "        \"Taxonomies\": set1    # The taxonomy (set)\n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame\n",
    "final_table = pd.DataFrame(table_rows)\n",
    "\n",
    "# Collapse the table by the \"Taxonomies\" column, merging words that have the same \"Taxonomies\" and removing duplicates\n",
    "collapsed_table = final_table.groupby('Taxonomies', as_index=False).agg({\n",
    "    'Categories': lambda x: ', '.join(sorted(set(x)))  # Remove duplicates by converting to set and sort alphabetically\n",
    "})\n",
    "\n",
    "# Now we need to group the words in \"Categories\" by their starting letter\n",
    "def group_by_first_letter(categories):\n",
    "    # Split categories into a list\n",
    "    words = categories.split(', ')\n",
    "    # Group words by the first letter\n",
    "    grouped = {}\n",
    "    for word in words:\n",
    "        first_letter = word[0].upper()  # Get the first letter and capitalize it\n",
    "        if first_letter not in grouped:\n",
    "            grouped[first_letter] = []\n",
    "        grouped[first_letter].append(word)\n",
    "    \n",
    "    # Sort each group alphabetically\n",
    "    for letter in grouped:\n",
    "        grouped[letter] = sorted(grouped[letter])\n",
    "\n",
    "    # Create the formatted string with bold letter and line breaks\n",
    "    formatted_output = \"\"\n",
    "    for letter, words in sorted(grouped.items()):\n",
    "        # Bold the first letter and add a line break after each group\n",
    "        formatted_output += f\"<b>{letter}:</b> {', '.join(words)}<br><br>\"\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# Apply the grouping function to the \"Categories\" column\n",
    "collapsed_table['Categories'] = collapsed_table['Categories'].apply(group_by_first_letter)\n",
    "\n",
    "# Display the collapsed table without HTML rendering (plain table)\n",
    "collapsed_html_output_table = collapsed_table.to_html(escape=False)  # escape=False if any HTML is present\n",
    "\n",
    "# Save the collapsed HTML output to a file\n",
    "collapsed_file_path = 'collapsed_grouped_words_table_grouped_bold.html'\n",
    "with open(collapsed_file_path, 'w') as file:\n",
    "    file.write(collapsed_html_output_table)\n",
    "\n",
    "print(f\"Collapsed grouped table saved to {collapsed_file_path}\")\n",
    "\n",
    "# Save the Excel file (plain, no color formatting)\n",
    "excel_file_path = 'collapsed_grouped_words_table_grouped_bold.xlsx'\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Grouped Table\"\n",
    "\n",
    "# Write the header\n",
    "header = list(collapsed_table.columns)\n",
    "ws.append(header)\n",
    "\n",
    "# Write the rows without any color formatting\n",
    "for row in collapsed_table.itertuples(index=False):\n",
    "    ws_row = [row.Categories, row.Taxonomies]\n",
    "    ws.append(ws_row)\n",
    "\n",
    "# Save Excel file\n",
    "wb.save(excel_file_path)\n",
    "print(f\"Excel file saved to {excel_file_path}\")\n",
    "```\n",
    "\n",
    "# Improving the color table\n",
    "\n",
    "\\`\\`\\`{python, eval=FALSE} import pandas as pd import numpy as np from\n",
    "matplotlib import cm\n",
    "\n",
    "# Define the colormap\n",
    "\n",
    "viridis_colormap = cm.Set2(np.linspace(0, 1, 6)) \\# Get 6 distinct\n",
    "colors from the colormap\n",
    "\n",
    "# Create a color mapping based on the viridis colormap\n",
    "\n",
    "color_mapping = { “Bajta”: f’rgb({int(viridis_colormap\\[0, 0\\]*255)},\n",
    "{int(viridis_colormap\\[0, 1\\]*255)}, {int(viridis_colormap\\[0,\n",
    "2\\]*255)})‘, “Britto_2016”: f’rgb({int(viridis_colormap\\[1, 0\\]*255)},\n",
    "{int(viridis_colormap\\[1, 1\\]*255)}, {int(viridis_colormap\\[1,\n",
    "2\\]*255)})’, “Britto_2017”: f’rgb({int(viridis_colormap\\[2, 0\\]*255)},\n",
    "{int(viridis_colormap\\[2, 1\\]*255)}, {int(viridis_colormap\\[2,\n",
    "2\\]*255)})’, “Dashti”: f’rgb({int(viridis_colormap\\[3, 0\\]*255)},\n",
    "{int(viridis_colormap\\[3, 1\\]*255)}, {int(viridis_colormap\\[3,\n",
    "2\\]*255)})‘, “Mendes”: f’rgb({int(viridis_colormap\\[4, 0\\]*255)},\n",
    "{int(viridis_colormap\\[4, 1\\]*255)}, {int(viridis_colormap\\[4,\n",
    "2\\]*255)})’, “Usman”: f’rgb({int(viridis_colormap\\[5, 0\\]*255)},\n",
    "{int(viridis_colormap\\[5, 1\\]*255)}, {int(viridis_colormap\\[5,\n",
    "2\\]*255)})’ }\n",
    "\n",
    "# Create an empty DataFrame to store the new table\n",
    "\n",
    "colored_words_table = pd.DataFrame(index=sets.keys(),\n",
    "columns=\\[“Words”\\])\n",
    "\n",
    "# Iterate through the filtered similarity DataFrame to populate the colored table\n",
    "\n",
    "for index, row in similarity_df_filtered.iterrows(): set1 = row\\[‘Set\n",
    "1’\\] word1 = row\\[‘Word 1’\\] set2 = row\\[‘Set 2’\\] word2 = row\\[‘Word\n",
    "2’\\]\n",
    "\n",
    "    # Apply colors to words\n",
    "    colored_word1 = f'<span style=\"color: {color_mapping[set1]}\">{word1}</span>'\n",
    "    colored_word2 = f'<span style=\"color: {color_mapping[set2]}\">{word2}</span>'\n",
    "\n",
    "    # For each set, append the related words in colored format\n",
    "    for set_name in sets.keys():\n",
    "        if set_name == set1:\n",
    "            current_words = colored_words_table.at[set_name, \"Words\"]\n",
    "            new_entry = f\"{colored_word1} --> {colored_word2}\"\n",
    "            if pd.isna(current_words):\n",
    "                colored_words_table.at[set_name, \"Words\"] = new_entry\n",
    "            else:\n",
    "                # Add the new entry and sort all entries alphabetically\n",
    "                current_entries = current_words.split(\"<br>\")\n",
    "                current_entries.append(new_entry)\n",
    "                sorted_entries = sorted(current_entries, key=lambda x: x.lower())  # Sort alphabetically (case insensitive)\n",
    "                colored_words_table.at[set_name, \"Words\"] = \"<br>\".join(sorted_entries)\n",
    "\n",
    "# Generate the legend HTML\n",
    "\n",
    "legend_html = “\n",
    "\n",
    "<strong>Legend:</strong><br>” for set_name, color in\n",
    "color_mapping.items(): legend_html += f’<span\n",
    "style=\"background-color: {color}; padding: 5px; color: white; margin-right: 10px;\">{set_name}</span>’\n",
    "legend_html += “\n",
    "\n",
    "<br>”\n",
    "\n",
    "# Convert the DataFrame to an HTML string, preserving the inline style\n",
    "\n",
    "html_output = legend_html + colored_words_table.to_html(escape=False)\n",
    "\n",
    "# Specify the filename where you want to save the table\n",
    "\n",
    "file_path = ‘colored_table_with_breaks_and_sorted.html’\n",
    "\n",
    "# Write the HTML content to the file\n",
    "\n",
    "with open(file_path, ‘w’) as file: file.write(html_output)\n",
    "\n",
    "print(f”Styled and sorted table saved to {file_path}“)\n",
    "\n",
    "\n",
    "\n",
    "    ```{python, eval=FALSE}\n",
    "    import pandas as pd\n",
    "\n",
    "    # Define the color for each set\n",
    "    set_colors = {\n",
    "        \"Bajta\": \"yellow\",\n",
    "        \"Britto_2016\": \"blue\",\n",
    "        \"Britto_2017\": \"green\",\n",
    "        \"Dashti\": \"red\",\n",
    "        \"Mendes\": \"purple\",\n",
    "        \"Usman\": \"orange\"\n",
    "    }\n",
    "\n",
    "    # Define the word pairs between sets (example, based on your actual word pairs)\n",
    "    # The format will be like (Set name, Word, Related Set, Related Word)\n",
    "    word_pairs = [\n",
    "        (\"Bajta\", \"word1\", \"Britto_2016\", \"word2\"),\n",
    "        (\"Bajta\", \"word3\", \"Britto_2017\", \"word4\"),\n",
    "        (\"Britto_2016\", \"word5\", \"Dashti\", \"word6\"),\n",
    "        (\"Mendes\", \"word7\", \"Usman\", \"word8\"),\n",
    "        # Add more word pairs here as needed\n",
    "    ]\n",
    "\n",
    "    # Create a dictionary to hold the word pairs for each set\n",
    "    word_dict = {set_name: [] for set_name in set_colors.keys()}\n",
    "\n",
    "    # Fill the word dictionary with word pairs\n",
    "    for set1, word1, set2, word2 in word_pairs:\n",
    "        word_dict[set1].append(f\"<span style='color:{set_colors[set1]};'>{word1}</span>\")\n",
    "        word_dict[set2].append(f\"<span style='color:{set_colors[set2]};'>{word2}</span>\")\n",
    "\n",
    "    # Create a DataFrame to store the table data\n",
    "    # Each row will represent a set and the column will contain the words in that set\n",
    "    table_data = []\n",
    "\n",
    "    # For each set, add the words it contains and their related words from other sets\n",
    "    for set_name, words in word_dict.items():\n",
    "        related_words = ', '.join(words)\n",
    "        table_data.append([set_name, related_words])\n",
    "\n",
    "    # Convert the data to a DataFrame\n",
    "    df = pd.DataFrame(table_data, columns=[\"Set\", \"Related Words\"])\n",
    "\n",
    "    # Function to style the table (custom colorize applied already in the word pairs)\n",
    "    def colorize_table(val):\n",
    "        return val  # No extra styling is needed as the words are already colored\n",
    "\n",
    "    # Apply the style\n",
    "    styled_table = df.style.applymap(colorize_table)\n",
    "\n",
    "    # Add the legend at the top (HTML)\n",
    "    legend_html = \"<div style='font-weight: bold; margin-bottom: 10px;'>Legend:</div>\"\n",
    "    for set_name, color in set_colors.items():\n",
    "        legend_html += f\"<div><span style='color:{color};'>●</span> {set_name}</div>\"\n",
    "\n",
    "    # Convert the styled table to HTML (use to_html instead of render)\n",
    "    html_output = styled_table.to_html()\n",
    "\n",
    "    # Combine the legend and the table\n",
    "    full_html_output = legend_html + \"<br>\" + html_output\n",
    "\n",
    "    # Specify the filename where you want to save the table\n",
    "    file_path = 'colored_table_with_legend.html'\n",
    "\n",
    "    # Write the combined HTML (legend + table) to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(full_html_output)\n",
    "\n",
    "    print(f\"Styled table with legend saved to {file_path}\")\n",
    "\n",
    "\n",
    "# t sne\n",
    "\n",
    "``` {python}\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sets = {\n",
    "    'Bajta': {\"Agile\", \"Analysis\", \"Availability\", \"Baseline comparison\", \"Bidding\", \"CBR\", \"CMMI\", \"COCOMO\", \"Commissioning\", \"Conceptualization\", \"Delphi\", \"Detail planning\", \"Design\", \"Distant onshore\", \"Expert judgment\", \"Estimated value\", \"Execution\", \"Effort hours\", \"Feasibility study\", \"Finance\", \"Fuzzy similarity\", \"GA\", \"Group-based estimation\", \"Healthcare\", \"Hardware\", \"Implementation\", \"Individual\", \"Machine learning\", \"Maintainability\", \"Maintenance\", \"Near offshore\", \"Non-machine learning\", \"Not considered\", \"Number of team members\", \"Performance\", \"Portfolio\", \"Preliminary planning\", \"Reliability\", \"Research & development\", \"Risk\", \"Security\", \"Sensitivity analysis\", \"Size report\", \"Socio-cultural distance\", \"Statistical analysis\", \"Staff/cost\", \"System investigation\", \"Temporal distance\", \"Testing\", \"Value\", \"Variation reduction\"},\n",
    "    'Britto_2017': {\"Accessibility level\", \"Adaptation complexity\", \"Anchor count\", \"Architecture\", \"Association center slot count\", \"Association slot size\", \"Attribute count\", \"Authoring tool type\", \"Availability level\", \"Class complexity\", \"Class coupling\", \"Client script count\", \"Cluster count\", \"Cluster node size\", \"Cluster slot count\", \"Cohesion\", \"Cohesion complexity\", \"Collection center slot count\", \"Collection slot size\", \"Comment count\", \"Communication level\", \"Compactness\", \"Component complexity\", \"Component count\", \"Component granularity level\", \"Component slot count\", \"Concern coupling\", \"Concern module count\", \"Concern operation count\", \"Concurrency level\", \"Connectivity density\", \"Control flow complexity\", \"Cyclomatic complexity\", \"Data Web points\", \"Data flow complexity\", \"Data usage complexity\", \"Database size\", \"Deployment platform experience level\", \"Design volatility\", \"Development restriction\", \"Difficulty level\", \"Diffusion cut count\", \"Documentation level\", \"Domain experience level\", \"Entity count\", \"Experience level\", \"Feature count\", \"Flexibility level\", \"Focus factor\", \"High feature count\", \"IT literacy\", \"In-house experience\", \"Indifferent concern count\", \"Information slot count\", \"Infrastructure\", \"Inner/sub concern count\", \"Innovation level\", \"Input complexity\", \"Installability level\", \"Integration with legacy systems\", \"Interface complexity\", \"International Function Point Users Group\", \"Layout complexity\", \"Lessons learned repository\", \"Lines of code\", \"Link count\", \"Low feature count\", \"Maintainability level\", \"Mapped workflows\", \"Media allocation\", \"Media count\", \"Media duration\", \"Memory efficiency level\", \"Metrics program\", \"Model association complexity\", \"Model collection complexity\", \"Model link complexity\", \"Model node size\", \"Model slot size\", \"Modularity level\", \"Module attribute count\", \"Module count\", \"Module point cut count\", \"Motivation level\", \"New Web page count\", \"New complexity\", \"New media count\", \"Node count\", \"Node slot size\", \"Novelty level\", \"Number of programming languages\", \"Number of projects in parallel\", \"OO experience level\", \"Object-Oriented Function Points\", \"Operation count\", \"Operational mode\", \"Output complexity\", \"Page complexity\", \"Personality\", \"Platform support level\", \"Platform volatility level\", \"Portability level\", \"Process efficiency level\", \"Processing requirements\", \"Productivity level\", \"Program count\", \"Programming language experience level\", \"Project management level\", \"Publishing model unit count\", \"Publishing unit count\", \"Quality level\", \"Rapid app development\", \"Readability level\", \"Reliability level\", \"Requirements clarity level\", \"Requirements novelty level\", \"Requirements volatility level\", \"Resource level\", \"Reusability level\", \"Reused comment count\", \"Reused component count\", \"Reused high feature count\", \"Reused lines of code\", \"Reused low feature count\", \"Reused media allocation\", \"Reused media count\", \"Reused program count\", \"Risk level\", \"Robustness level\", \"SPI program\", \"Scalability level\", \"Section count\", \"Security level\", \"Segment count\", \"Semantic association count\", \"Server script count\", \"Slot count\", \"Slot granularity level\", \"Software development experience\", \"Software reuse\", \"Stability level\", \"Statement count\", \"Storage constraint\", \"Structure\", \"Team capability\", \"Team size\", \"Technical factors\", \"Testability level\", \"Time efficiency level\", \"Time restriction\", \"Tool experience level\", \"Total complexity\", \"Trainability level\", \"Type\", \"Usability level\", \"Use case count\", \"Web objects\", \"Web page allocation\", \"Web page count\", \"Work Team level\"},\n",
    "    'Britto_2016': {\"Centralized\", \"distributed\", \"Early\", \"Estimator\", \"Early & Late\", \"Estimator & Provider\", \"geographic distance\", \"geographic distance\", \"late\", \"legal entity\", \"location\", \"provider\", \"semi-distributed\", \"temporal distance\", \"temporal distance\"},\n",
    "    'Dasthi': {\"ANN\", \"Analogy Base\", \"COCOMO\", \"Evolutionary\", \"Expert Judgment\", \"FUZZY\", \"SEER-SEM\", \"SLIM\", \"Swarm\"},\n",
    "    'Mendes': {\"Absolute\", \"both\", \"complexity\", \"functionality\", \"Directly\", \"Early size metric\", \"Empirically\", \"indirectly\", \"interval\", \"Length\", \"late size metric\", \"media\", \"none\", \"Nominal\", \"nonspecific\", \"ordinal\", \"other\", \"Problem oriented metric\", \"program/script\", \"ratio\", \"solution oriented metric\", \"Specific\", \"theoretically\", \"Web application\", \"Web hypermedia application\", \"Web software application\"},\n",
    "    'Usman': {\"Analysis\", \"all\", \"analogy\", \"availability\", \"bidding\", \"Close Onshore\", \"Co-located\", \"Communications industry\", \"Considered\", \"crystal\", \"customized XP\", \"customized scrum\", \"daily\", \"design\", \"distribution\", \"education\", \"expert judgement\", \"DSDM\", \"Distant Onshore\", \"Estimate value(s)\", \"FDD\", \"Far Offshore\", \"financial\", \"function points\", \"Hours/days\", \"health\", \"ideal hours\", \"implementation\", \"kanban\", \"maintainability\", \"maintenance\", \"manufacturing\", \"MMRE\", \"MdMRE\", \"Near Offshore\", \"No. of team members\", \"not applicable\", \"not considered\", \"not used\", \"Other\", \"Performance\", \"Planning poker\", \"Point\", \"pair days\", \"Release\", \"reliability\", \"retail/wholesale\", \"Single\", \"scrum\", \"security\", \"sprint\", \"Story points\", \"testing\", \"three point\", \"task\", \"transportation\", \"UC points\", \"User story\", \"Value\", \"XP\"}\n",
    "}\n",
    "\n",
    "normalized_sets = {set_name: normalize_words(word_set) for set_name, word_set in sets.items()}\n",
    "\n",
    "\n",
    "# Create a dictionary to store the embeddings of each set\n",
    "embeddings = {}\n",
    "all_words = []\n",
    "word_to_set = {}\n",
    "for set_name, word_set in normalized_sets.items():\n",
    "    embeddings[set_name] = get_embeddings(word_set)\n",
    "    all_words.extend(list(word_set))\n",
    "    for word in word_set:\n",
    "        word_to_set[word] = set_name\n",
    "\n",
    "# Create an array of all embeddings\n",
    "all_embeddings = torch.cat([embeddings[set_name] for set_name in normalized_sets], dim=0)\n",
    "\n",
    "\n",
    "# Map sets to colors\n",
    "set_colors = {set_name: sns.color_palette(\"Set2\")[i] for i, set_name in enumerate(sets.keys())}\n",
    "word_colors = [set_colors[word_to_set[word]] for word in all_words]\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the embeddings to 2D\n",
    "tsne = TSNE(n_components=2, random_state=5)\n",
    "reduced_embeddings = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Initialize figure\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Track words already labeled\n",
    "labeled_words = {}\n",
    "\n",
    "# Scatter plot with words colored by their set and label duplicates only once\n",
    "for i, word in enumerate(all_words):\n",
    "    # Color and position each word's dot\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], \n",
    "                c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)\n",
    "    \n",
    "    # Check if the word has appeared before\n",
    "    if word not in labeled_words:\n",
    "        # If first occurrence, label it and choose red if shared\n",
    "        color = 'red' if all_words.count(word) > 1 else 'black'\n",
    "        text = plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word.upper(), \n",
    "                        fontsize=5, color=color)\n",
    "        labeled_words[word] = text  # Track labeled words for adjustText\n",
    "    \n",
    "# Adjust the positions of labels to avoid overlap\n",
    "adjust_text(list(labeled_words.values()), only_move={'points': 'xy'}, force_text=0.75, expand_text=(1.5, 1.5))\n",
    "\n",
    "# First legend for word sets\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in set_colors.values()]\n",
    "labels = list(sets.keys())\n",
    "legend1 = plt.legend(handles=handles, labels=labels, title=\"Literature\", loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=6)\n",
    "\n",
    "# Second legend for duplicate words\n",
    "duplicate_legend = plt.Line2D([0], [1], color='red', lw=2)\n",
    "legend2 = plt.legend([duplicate_legend], [\"In red: duplicate words\"], loc='upper center', bbox_to_anchor=(0.5, -0.12), frameon=False)\n",
    "\n",
    "# Re-add the first legend\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('tsne_word_embeddings.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` {python}\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "\n",
    "# Create a dictionary to store the embeddings of each set\n",
    "embeddings = {}\n",
    "all_words = []\n",
    "word_to_set = {}\n",
    "for set_name, word_set in normalized_sets.items():\n",
    "    embeddings[set_name] = get_embeddings(word_set)\n",
    "    all_words.extend(list(word_set))\n",
    "    for word in word_set:\n",
    "        word_to_set[word] = set_name\n",
    "\n",
    "# Create an array of all embeddings\n",
    "all_embeddings = torch.cat([embeddings[set_name] for set_name in normalized_sets], dim=0)\n",
    "\n",
    "# Map sets to colors\n",
    "set_colors = {set_name: sns.color_palette(\"Set2\")[i] for i, set_name in enumerate(sets.keys())}\n",
    "word_colors = [set_colors[word_to_set[word]] for word in all_words]\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the embeddings to 3D\n",
    "tsne = TSNE(n_components=3, random_state=5)\n",
    "reduced_embeddings = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Initialize figure for 3D plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Track words already labeled\n",
    "labeled_words = {}\n",
    "\n",
    "# Scatter plot with words colored by their set and label duplicates only once\n",
    "for i, word in enumerate(all_words):\n",
    "    # Color and position each word's dot in 3D\n",
    "    ax.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], \n",
    "               c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)\n",
    "    \n",
    "    # Check if the word has appeared before\n",
    "    if word not in labeled_words:\n",
    "        # If first occurrence, label it and choose red if shared\n",
    "        color = 'red' if all_words.count(word) > 1 else 'black'\n",
    "        ax.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], \n",
    "                word.upper(), fontsize=5, color=color)\n",
    "        labeled_words[word] = True  # Track labeled words for avoid overlap\n",
    "    \n",
    "# Adjust the positions of labels to avoid overlap (this part doesn't adjust in 3D directly, \n",
    "# but you could explore 3D label adjustments using other techniques like manually adjusting the positions)\n",
    "# For now, we keep the label text without adjustment in 3D (more complex adjustments can be done with other libraries).\n",
    "\n",
    "# First legend for word sets\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in set_colors.values()]\n",
    "labels = list(sets.keys())\n",
    "legend1 = plt.legend(handles=handles, labels=labels, title=\"Literature\", loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=6)\n",
    "\n",
    "# Second legend for duplicate words\n",
    "duplicate_legend = plt.Line2D([0], [1], color='red', lw=2)\n",
    "legend2 = plt.legend([duplicate_legend], [\"In red: duplicate words\"], loc='upper center', bbox_to_anchor=(0.5, -0.12), frameon=False)\n",
    "\n",
    "# Re-add the first legend\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "ax.set_title(\"t-SNE Visualization of Word Embeddings in 3D\")\n",
    "ax.set_xlabel(\"t-SNE Component 1\")\n",
    "ax.set_ylabel(\"t-SNE Component 2\")\n",
    "ax.set_zlabel(\"t-SNE Component 3\")\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('tsne_word_embeddings_3d.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# Same as before but UMAP\n",
    "\n",
    "``` {python}\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the embeddings to 2D\n",
    "umap_model = umap.UMAP(n_components=2, random_state=5)\n",
    "reduced_embeddings = umap_model.fit_transform(all_embeddings)\n",
    "\n",
    "# Initialize figure\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Track words already labeled\n",
    "labeled_words = {}\n",
    "\n",
    "# Scatter plot with words colored by their set and label duplicates only once\n",
    "for i, word in enumerate(all_words):\n",
    "    # Color and position each word's dot\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], \n",
    "                c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)\n",
    "    \n",
    "    # Check if the word has appeared before\n",
    "    if word not in labeled_words:\n",
    "        # If first occurrence, label it and choose red if shared\n",
    "        color = 'red' if all_words.count(word) > 1 else 'black'\n",
    "        text = plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word.upper(), \n",
    "                        fontsize=5, color=color)\n",
    "        labeled_words[word] = text  # Track labeled words for adjustText\n",
    "    \n",
    "# Adjust the positions of labels to avoid overlap\n",
    "adjust_text(list(labeled_words.values()), only_move={'points': 'xy'}, force_text=0.75, expand_text=(1.5, 1.5))\n",
    "\n",
    "# First legend for word sets\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in set_colors.values()]\n",
    "labels = list(sets.keys())\n",
    "legend1 = plt.legend(handles=handles, labels=labels, title=\"Literature\", loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=6)\n",
    "\n",
    "# Second legend for duplicate words\n",
    "duplicate_legend = plt.Line2D([0], [1], color='red', lw=2)\n",
    "legend2 = plt.legend([duplicate_legend], [\"In red: duplicate words\"], loc='upper center', bbox_to_anchor=(0.5, -0.12), frameon=False)\n",
    "\n",
    "# Re-add the first legend\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.title(\"UMAP Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig('umap_word_embeddings.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# Similarity Counts Heatmap\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Clear any existing plots and set plot style\n",
    "plt.clf()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "df = similarity_df_filtered\n",
    "\n",
    "# Group by Set 1 and Set 2 and count the number of shared words\n",
    "count_table = df.groupby([\"Set 1\", \"Set 2\"]).size().reset_index(name=\"Shared Word Count\")\n",
    "\n",
    "# Pivot the DataFrame to create a matrix of shared word counts\n",
    "pivot_count = count_table.pivot(index=\"Set 1\", columns=\"Set 2\", values=\"Shared Word Count\").fillna(0)\n",
    "\n",
    "# Reindex the pivot table to ensure symmetry in rows and columns\n",
    "pivot_count = pivot_count.reindex(index=pivot_count.columns, columns=pivot_count.columns, fill_value=0)\n",
    "\n",
    "# Mask only the upper triangle, excluding the diagonal\n",
    "mask = np.triu(np.ones_like(pivot_count, dtype=bool), k=1)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "#sns.heatmap(pivot_count, annot=True, mask=mask, cmap=\"Blues\", cbar_kws={'label': 'Number of Shared Words'})\n",
    "sns.heatmap(pivot_count, annot=True, mask=mask, cmap=\"Blues\", cbar_kws={'label': 'Number of Similar Words'})\n",
    "\n",
    "plt.title(\"Heatmap of Similar Words between Literature\")\n",
    "plt.xlabel(\"Literature\")\n",
    "plt.ylabel(\"Literature\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# Barplot\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clear any existing plots and set plot style\n",
    "plt.clf()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Data preparation\n",
    "df = similarity_df_filtered\n",
    "\n",
    "# Group by Set 1 and Set 2 and count the number of shared words\n",
    "count_table = df.groupby([\"Set 1\", \"Set 2\"]).size().reset_index(name=\"Shared Word Count\")\n",
    "\n",
    "# Remove redundant comparisons (keeping only Set1 < Set2)\n",
    "count_table = count_table[count_table[\"Set 1\"] < count_table[\"Set 2\"]]\n",
    "\n",
    "# Create all possible combinations of Set 1 and Set 2\n",
    "all_sets = pd.MultiIndex.from_product([count_table[\"Set 1\"].unique(), count_table[\"Set 2\"].unique()], names=[\"Set 1\", \"Set 2\"])\n",
    "\n",
    "# Create a DataFrame with all combinations and zero counts\n",
    "count_table_full = pd.DataFrame(index=all_sets).reset_index()\n",
    "\n",
    "# Merge count_table_full with the original count_table to get the actual shared word counts\n",
    "count_table_full = pd.merge(count_table_full, count_table, on=[\"Set 1\", \"Set 2\"], how=\"left\").fillna(0)\n",
    "\n",
    "# Create the bar plot (dodge=True)\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=\"Set 1\", y=\"Shared Word Count\", hue=\"Set 2\", data=count_table_full, palette=\"Set2\", width=0.8, dodge=True)\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Barplot of Similar Words Between Literature (Upper 70% similarity threshold)\")\n",
    "plt.xlabel(\"Number of Similar Words\")\n",
    "plt.ylabel(\"Literature\")\n",
    "\n",
    "# Add value labels inside each bar with the same color as the bars\n",
    "for container in ax.containers:\n",
    "    for bar in container:\n",
    "        bar_color = bar.get_facecolor()  # Get the color of the bar\n",
    "        ax.bar_label(container, label_type=\"edge\", padding=3, fontsize=12, color=bar_color, fontweight='bold')  # Set the label color to the bar's color\n",
    "\n",
    "# Change the legend title and position it at the bottom horizontally\n",
    "plt.legend(title=\"Literature\", loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=len(count_table['Set 2'].unique()))\n",
    "\n",
    "# Adjust layout to prevent clipping and save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('shared_words_barplot_with_labels.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "# Table showing what words are related to words in other papers\n",
    "\n",
    "``` {python}\n",
    "# Group by Set 1 and Set 2\n",
    "count_table = df.groupby([\"Set 1\", \"Set 2\"]).size().reset_index(name=\"Shared Word Count\")\n",
    "\n",
    "# Create empty lists to store the shared words for Set 1 and Set 2\n",
    "words_set1 = []\n",
    "words_set2 = []\n",
    "\n",
    "# For each pair of sets, collect the shared words\n",
    "for _, row in count_table.iterrows():\n",
    "    set1 = row['Set 1']\n",
    "    set2 = row['Set 2']\n",
    "    \n",
    "    # Get the rows from the filtered DataFrame that match the current pair of sets\n",
    "    shared_rows = df[(df['Set 1'] == set1) & (df['Set 2'] == set2)]\n",
    "    \n",
    "    # Collect the shared words for Set 1 and Set 2\n",
    "    set1_words = sorted(shared_rows['Word 1'].unique())  # Unique words from Set 1\n",
    "    set2_words = sorted(shared_rows['Word 2'].unique())  # Unique words from Set 2\n",
    "    \n",
    "    # Join the words into a comma-separated string\n",
    "    words_set1.append(\", \".join(set1_words))\n",
    "    words_set2.append(\", \".join(set2_words))\n",
    "\n",
    "# Add the new columns to the count_table\n",
    "count_table['Words From Set 1'] = words_set1\n",
    "count_table['Words From Set 2'] = words_set2\n",
    "\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(count_table)\n",
    "\n",
    "# Make sure that Set 1 is always smaller than Set 2 (lexicographically)\n",
    "count_table['Set Pair'] = count_table.apply(lambda row: tuple(sorted([row['Set 1'], row['Set 2']])), axis=1)\n",
    "\n",
    "# Drop duplicates based on the 'Set Pair' column\n",
    "count_table_filtered = count_table.drop_duplicates(subset=['Set Pair'])\n",
    "\n",
    "# Drop the 'Set Pair' column now that we don't need it anymore\n",
    "count_table_filtered = count_table_filtered.drop(columns=['Set Pair'])\n",
    "\n",
    "# Display the filtered result\n",
    "print(count_table_filtered)\n",
    "```\n",
    "\n",
    "``` {python}\n",
    "\n",
    "##############################COLORIZE TABLE############################\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "\n",
    "color_palette = [\n",
    "    'ADD8E6',  # Light Blue\n",
    "    'FFFFFF',  # White\n",
    "]\n",
    "\n",
    "# Initialize tracking variables\n",
    "word1_to_color = {}\n",
    "color_index = 0\n",
    "previous_word = None\n",
    "previous_color = None\n",
    "\n",
    "# Create a new column \"Marker\" to hold \"*\" if \"Word 1\" equals \"Word 2\"\n",
    "similarity_df_filtered['Marker'] = similarity_df_filtered.apply(\n",
    "    lambda row: '*' if row['Word 1'] == row['Word 2'] else '', axis=1\n",
    ")\n",
    "\n",
    "# Function to assign colors to rows, skipping color assignment if the \"Word 1\" is the same as the previous row's \"Word 1\"\n",
    "def assign_colors(row):\n",
    "    global previous_word, previous_color, color_index\n",
    "\n",
    "    word1 = row['Word 1']\n",
    "    \n",
    "    # If the current \"Word 1\" is the same as the previous \"Word 1\", reuse the color\n",
    "    if word1 == previous_word:\n",
    "        color = previous_color\n",
    "    else:\n",
    "        # Otherwise, assign the next color and update tracking variables\n",
    "        color = color_palette[color_index % len(color_palette)]\n",
    "        previous_color = color\n",
    "        previous_word = word1\n",
    "        color_index += 1\n",
    "    \n",
    "    # Return a list of styles for all columns except for \"Marker\" (bold)\n",
    "    return [f'background-color: {color}'] * (len(row) - 1) + ['font-weight: bold; color: black;']\n",
    "\n",
    "# Apply the function to style the DataFrame\n",
    "styled_similarity_df = similarity_df_filtered.style.apply(assign_colors, axis=1)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_similarity_df\n",
    "styled_similarity_df.to_html('similarity_table.html')\n",
    "\n",
    "######################## EXCEL ###################\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "\n",
    "# Initialize the Excel workbook and sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Similarity Results\"\n",
    "\n",
    "# Write the headers to the Excel sheet\n",
    "headers = list(similarity_df_filtered.columns)\n",
    "ws.append(headers)\n",
    "\n",
    "# Define the color palette (matching Excel-friendly hex colors)\n",
    "excel_palette = [\n",
    "    'FFFFFF',  # Soft Orange\n",
    "    'cfebff',  # Soft Blue\n",
    "]\n",
    "\n",
    "# Initialize tracking variables for coloring\n",
    "word1_to_color = {}\n",
    "color_index = 0\n",
    "previous_word = None\n",
    "previous_color = None\n",
    "\n",
    "# Function to get the next color for a word pair, reusing previous color if the word matches\n",
    "def get_color(word1):\n",
    "    global previous_word, previous_color, color_index\n",
    "    if word1 == previous_word:\n",
    "        color = previous_color\n",
    "    else:\n",
    "        color = excel_palette[color_index % len(excel_palette)]\n",
    "        previous_word = word1\n",
    "        previous_color = color\n",
    "        color_index += 1\n",
    "    return color\n",
    "\n",
    "# Populate the Excel sheet with data and apply conditional coloring\n",
    "for _, row in similarity_df_filtered.iterrows():\n",
    "    # Determine the color for this row\n",
    "    color_hex = get_color(row['Word 1'])\n",
    "    fill = PatternFill(start_color=color_hex, end_color=color_hex, fill_type=\"solid\")\n",
    "    \n",
    "    # Create a list to hold row values, including the new \"Marker\" column\n",
    "    row_values = list(row)\n",
    "\n",
    "    # Add an asterisk \"*\" in the \"Marker\" column if Word 1 == Word 2\n",
    "    marker = \"*\" if row['Word 1'] == row['Word 2'] else \"\"\n",
    "    row_values.append(marker)  # Append the marker to the row values\n",
    "\n",
    "    # Write row to Excel and apply styling\n",
    "    ws.append(row_values)\n",
    "    for col_idx in range(1, len(row_values)):\n",
    "        cell = ws.cell(row=ws.max_row, column=col_idx)\n",
    "        cell.fill = fill  # Apply the color fill to each cell in the row\n",
    "\n",
    "    # Apply bold font to the marker cell if it contains \"*\"\n",
    "    marker_cell = ws.cell(row=ws.max_row, column=len(row_values))\n",
    "    if marker_cell.value == \"*\":\n",
    "        marker_cell.font = Font(bold=True)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(\"similarity_results_colored.xlsx\")\n",
    "```\n",
    "\n",
    "#Trying to collapse the excel\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# Initialize Excel workbook and worksheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Collapsed Similarity Results\"\n",
    "\n",
    "# Create a collapsed column header\n",
    "headers = [\"Set Pair\", \"Word Pair\", \"Cosine Similarity\", \"Marker\"]\n",
    "ws.append(headers)\n",
    "\n",
    "# Define color palette for coloring words\n",
    "excel_palette = [\n",
    "    'ADD8E6',  # Light Blue\n",
    "    'FFFFFF',  # Sky Blue\n",
    "]\n",
    "\n",
    "# Initialize tracking for word color and color index\n",
    "word1_to_color = {}\n",
    "color_index = 0\n",
    "previous_word = None\n",
    "previous_color = None\n",
    "\n",
    "# Function to get the next color for a word, preventing consecutive repetitions\n",
    "def get_color(word1):\n",
    "    global previous_word, previous_color, color_index\n",
    "    if word1 == previous_word:\n",
    "        color = previous_color\n",
    "    else:\n",
    "        color = excel_palette[color_index % len(excel_palette)]\n",
    "        previous_word = word1\n",
    "        previous_color = color\n",
    "        color_index += 1\n",
    "    return color\n",
    "\n",
    "# Populate the sheet with merged \"Set Pair\" and \"Word Pair\" columns\n",
    "for _, row in similarity_df_filtered.iterrows():\n",
    "    set_pair = f\"{row['Set 1']} - {row['Set 2']}\"\n",
    "    word_pair = f\"{row['Word 1']} - {row['Word 2']}\"\n",
    "    cosine_similarity = row['Cosine Similarity']\n",
    "    marker = \"*\" if row['Word 1'] == row['Word 2'] else \"\"\n",
    "\n",
    "    # Append collapsed data to Excel sheet\n",
    "    ws.append([set_pair, word_pair, cosine_similarity, marker])\n",
    "\n",
    "    # Retrieve the current row in Excel (last row)\n",
    "    current_row = ws.max_row\n",
    "\n",
    "    # Apply color fill to \"Word Pair\" cell, splitting the color between the words\n",
    "    word1_color = get_color(row['Word 1'])\n",
    "    word2_color = get_color(row['Word 2'])\n",
    "\n",
    "    # Use PatternFill to set color on individual words in the \"Word Pair\" cell\n",
    "    cell = ws.cell(row=current_row, column=2)\n",
    "    cell_value = cell.value\n",
    "\n",
    "    # Apply color fills\n",
    "    ws.cell(row=current_row, column=2).fill = PatternFill(\n",
    "        start_color=word1_color, end_color=word1_color, fill_type=\"solid\"\n",
    "    )\n",
    "    # Format each word with its designated color\n",
    "    # Since openpyxl doesn't support in-cell color splitting directly,\n",
    "    # we’d visually inspect it or use external software for more advanced formatting.\n",
    "\n",
    "    # Apply bold font to the \"Marker\" column if it has \"*\"\n",
    "    marker_cell = ws.cell(row=current_row, column=4)\n",
    "    if marker:\n",
    "        marker_cell.font = Font(bold=True)\n",
    "\n",
    "# Adjust column width for readability\n",
    "for col in range(1, ws.max_column + 1):\n",
    "    ws.column_dimensions[get_column_letter(col)].width = 20\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(\"collapsed_similarity_results_colored.xlsx\")\n",
    "```\n",
    "\n",
    "# Trying to collapse the excel even more\n",
    "\n",
    "``` {python}\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "# Create a workbook and a worksheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Collapsed Similarity Results\"\n",
    "\n",
    "# Define the headers for the new table\n",
    "headers = [\"Set Pair\", \"Word Pair\"]\n",
    "ws.append(headers)\n",
    "\n",
    "# Define color palette for coloring words\n",
    "excel_palette = [\n",
    "    'FF6384',  # Light Red\n",
    "    '36A2EB',  # Light Blue\n",
    "    'FFCE56',  # Light Yellow\n",
    "    '4BC0C0',  # Light Green\n",
    "    '9966FF',  # Light Purple\n",
    "    'FF9F40'   # Light Orange\n",
    "]\n",
    "\n",
    "# Initialize the color index and word-to-color mapping\n",
    "color_index = 0\n",
    "word_colors = {}\n",
    "\n",
    "# Function to assign a color to a word, and avoid repeating colors for the same word in consecutive rows\n",
    "def get_word_color(word):\n",
    "    global color_index\n",
    "    if word not in word_colors:\n",
    "        word_colors[word] = excel_palette[color_index % len(excel_palette)]\n",
    "        color_index += 1\n",
    "    return word_colors[word]\n",
    "\n",
    "# Group the words by set pair and track the font color for each word\n",
    "set_pairs_dict = {}\n",
    "\n",
    "# Populate the set_pairs_dict with the relevant data\n",
    "for _, row in similarity_df_filtered.iterrows():\n",
    "    set_pair = f\"{row['Set 1']} - {row['Set 2']}\"\n",
    "    word_pair = f\"{row['Word 1']} - {row['Word 2']}\"\n",
    "    \n",
    "    # Group words by set pairs\n",
    "    if set_pair not in set_pairs_dict:\n",
    "        set_pairs_dict[set_pair] = []\n",
    "    \n",
    "    set_pairs_dict[set_pair].append((row['Word 1'], row['Word 2'], row['Cosine Similarity']))\n",
    "\n",
    "# Function to apply colors to words in the final merged table\n",
    "def apply_word_colors(word_pair_str, word_colors):\n",
    "    colored_str = []\n",
    "    for word in word_pair_str.split(\" - \"):\n",
    "        color = word_colors.get(word, '000000')  # Default to black if no color is found\n",
    "        word_font = Font(color=color)\n",
    "        colored_str.append(f\"{word} ({color})\")  # Track color alongside the word\n",
    "    return \" - \".join(colored_str)\n",
    "\n",
    "# Now we will merge words and apply colors to the font\n",
    "for set_pair, word_pairs in set_pairs_dict.items():\n",
    "    combined_words = []\n",
    "    for word1, word2, _ in word_pairs:\n",
    "        color_word1 = get_word_color(word1)\n",
    "        color_word2 = get_word_color(word2)\n",
    "\n",
    "        # Append words to the combined list, ensuring no duplicates\n",
    "        combined_words.extend([word1, word2])\n",
    "\n",
    "    # Remove duplicates and join them into one string for the word pair column\n",
    "    combined_words = sorted(set(combined_words), key=lambda x: combined_words.index(x))\n",
    "    word_pair_str = \" - \".join(combined_words)\n",
    "\n",
    "    # Add the set pair and word pair to the Excel sheet\n",
    "    current_row = ws.max_row + 1\n",
    "    ws.append([set_pair, word_pair_str])\n",
    "\n",
    "    # Apply font color for each word in the final word pair\n",
    "    current_cell = ws.cell(row=current_row, column=2)\n",
    "    current_cell.value = word_pair_str\n",
    "\n",
    "    for word in word_pair_str.split(\" - \"):\n",
    "        # Set font color for each word (as per the color assigned previously)\n",
    "        color = word_colors.get(word, '000000')\n",
    "\n",
    "        # Check if the word is identical and bold it\n",
    "        is_bold = False\n",
    "        for word1, word2, _ in word_pairs:\n",
    "            if word1 == word2:\n",
    "                is_bold = True\n",
    "\n",
    "        # Apply the font color and bold if necessary\n",
    "        current_cell.font = Font(color=color, bold=is_bold)\n",
    "\n",
    "# Save the workbook to a file\n",
    "wb.save(\"colored_word_pairs.xlsx\")\n",
    "\n",
    "print(\"Excel file with colored words and bold identical words has been created!\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "``` {python}\n",
    "# Initialize tracking variables\n",
    "word1_to_color = {}\n",
    "color_index = 0\n",
    "previous_word = None\n",
    "previous_color = None\n",
    "\n",
    "# Create a new column \"Marker\" to hold \"*\" if \"Word 1\" equals \"Word 2\"\n",
    "similarity_df_filtered['Marker'] = similarity_df_filtered.apply(\n",
    "    lambda row: '*' if row['Word 1'] == row['Word 2'] else '', axis=1\n",
    ")\n",
    "\n",
    "# Function to assign colors to rows, skipping color assignment if the \"Word 1\" is the same as the previous row's \"Word 1\"\n",
    "def assign_colors(row):\n",
    "    global previous_word, previous_color, color_index\n",
    "\n",
    "    word1 = row['Word 1']\n",
    "    \n",
    "    # If the current \"Word 1\" is the same as the previous \"Word 1\", reuse the color\n",
    "    if word1 == previous_word:\n",
    "        color = previous_color\n",
    "    else:\n",
    "        # Otherwise, assign the next color and update tracking variables\n",
    "        color = color_palette[color_index % len(color_palette)]\n",
    "        previous_color = color\n",
    "        previous_word = word1\n",
    "        color_index += 1\n",
    "    \n",
    "    # Return a list of styles for all columns except for \"Marker\" (bold)\n",
    "    return [f'background-color: {color}'] * (len(row) - 1) + ['font-weight: bold; color: black;']\n",
    "\n",
    "# Apply the function to style the DataFrame\n",
    "styled_similarity_df = similarity_df_filtered.style.apply(assign_colors, axis=1)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_similarity_df\n",
    "styled_similarity_df.to_html('similarity_table.html')\n",
    "```"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
