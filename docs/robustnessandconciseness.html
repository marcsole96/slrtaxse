<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Marc" />

<meta name="date" content="2025-07-29" />

<title>Robustness and Conciseness</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/simplex.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/main/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">slrtaxse</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="bib-file-parser.html">Code used for Google Scholar literature collection</a>
</li>
<li>
  <a href="robustnessandconciseness.html">Code used to analyze the collected taxonomies</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/marcsole96/slrtaxse">
    <span class="fab fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Robustness and Conciseness</h1>
<h4 class="author">Marc</h4>
<h4 class="date">2025-07-29</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr <span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2025-07-29
</p>
<p>
<strong>Checks:</strong> <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7
<span class="glyphicon glyphicon-exclamation-sign text-danger"
aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>slr_taxonomies_workflowr/</code>
<span class="glyphicon glyphicon-question-sign" aria-hidden="true"
title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="https://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/workflowr/workflowr">workflowr</a> (version
1.7.1). The <em>Checks</em> tab describes the reproducibility checks
that were applied when the results were created. The <em>Past
versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git
repository, you know the exact version of the code that produced these
results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the
global environment can affect the analysis in your R Markdown file in
unknown ways. For reproduciblity it’s best to always run the code in an
empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20250128code">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Seed:</strong>
<code>set.seed(20250128)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20250128code"
class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20250128)</code> was run prior to running
the code in the R Markdown file. Setting a seed ensures that any results
that rely on randomness, e.g. subsampling or permutations, are
reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Session information:</strong>
recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package
versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be
confident that you successfully produced the results during this
run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr
project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcommarcsole96slrtaxsetree4eaeec6fa01c2c64e77dc14403d660d348d70e20targetblank4eaeec6a">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Repository version:</strong>
<a href="https://github.com/marcsole96/slrtaxse/tree/4eaeec6fa01c2c64e77dc14403d660d348d70e20" target="_blank">4eaeec6</a>
</a>
</p>
</div>
<div
id="strongRepositoryversionstrongahrefhttpsgithubcommarcsole96slrtaxsetree4eaeec6fa01c2c64e77dc14403d660d348d70e20targetblank4eaeec6a"
class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development
and connecting the code version to the results is critical for
reproducibility.
</p>
<p>
The results in this page were generated with repository version
<a href="https://github.com/marcsole96/slrtaxse/tree/4eaeec6fa01c2c64e77dc14403d660d348d70e20" target="_blank">4eaeec6</a>.
See the <em>Past versions</em> tab to see a history of the changes made
to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for
the analysis have been committed to Git prior to generating the results
(you can use <code>wflow_publish</code> or
<code>wflow_git_commit</code>). workflowr only checks the R Markdown
file, but you know if there are other scripts or data files that it
depends on. Below is the status of the Git repository when the results
were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  collapsed_grouped_words_table_colored_new.html
    Untracked:  unique_characteristics_analysis.csv

Unstaged changes:
    Modified:   analysis/desktop.ini
    Modified:   code/desktop.ini
    Modified:   collapsed_similarity_results_colored.xlsx
    Modified:   colored_word_pairs.xlsx
    Modified:   data/desktop.ini
    Modified:   desktop.ini
    Modified:   output/desktop.ini
    Modified:   shared_words_barplot_with_labels.png
    Modified:   similarity_results_colored.xlsx
    Modified:   similarity_table.html
    Modified:   tsne_word_embeddings.png
    Modified:   tsne_word_embeddings_3d.png
    Modified:   umap_word_embeddings.png
    Modified:   word_counts.png
    Modified:   word_embeddings.png
    Modified:   word_embeddings_kmeans.png

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not
included in this status report because it is ok for generated content to
have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were
made to the R Markdown
(<code>analysis/robustnessandconciseness.Rmd</code>) and HTML
(<code>docs/robustnessandconciseness.html</code>) files. If you’ve
configured a remote Git repository (see <code>?wflow_git_remote</code>),
click on the hyperlinks in the table below to view the files as they
were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/marcsole96/slrtaxse/blob/4eaeec6fa01c2c64e77dc14403d660d348d70e20/analysis/robustnessandconciseness.Rmd" target="_blank">4eaeec6</a>
</td>
<td>
marcsole96
</td>
<td>
2025-07-29
</td>
<td>
Updated analysis with new code modifications
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/marcsole96/slrtaxse/97dcf4df7ff4f7c62a6875d544c65aa5b9e6534c/docs/robustnessandconciseness.html" target="_blank">97dcf4d</a>
</td>
<td>
marcsole96
</td>
<td>
2025-06-13
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/marcsole96/slrtaxse/blob/f10200cde21fe37b1427b5394d9b4ea26b8dbb4c/analysis/robustnessandconciseness.Rmd" target="_blank">f10200c</a>
</td>
<td>
marcsole96
</td>
<td>
2025-06-13
</td>
<td>
Publishing robustness and conciseness analysis
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/marcsole96/slrtaxse/3618f253bdfd7bdc4bc937093329a0088bed9c9d/docs/robustnessandconciseness.html" target="_blank">3618f25</a>
</td>
<td>
marcsole96
</td>
<td>
2025-01-29
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/marcsole96/slrtaxse/blob/9b4cff1086d10a4d9e0c4b6078d09c896fef3b72/analysis/robustnessandconciseness.Rmd" target="_blank">9b4cff1</a>
</td>
<td>
marcsole96
</td>
<td>
2025-01-28
</td>
<td>
Test
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/marcsole96/slrtaxse/eeb963a176c2e35c4d3b678dfafbecbc9b54c95f/docs/robustnessandconciseness.html" target="_blank">eeb963a</a>
</td>
<td>
marcsole96
</td>
<td>
2025-01-28
</td>
<td>
Add generated site and update _site.yml
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="representing-a-tree-with-dictionaries" class="section level1">
<h1>Representing a tree with dictionaries</h1>
<p><a
href="https://blog.finxter.com/5-best-ways-to-construct-and-manage-a-tree-in-python/"
class="uri">https://blog.finxter.com/5-best-ways-to-construct-and-manage-a-tree-in-python/</a>
<a href="https://builtin.com/articles/tree-python"
class="uri">https://builtin.com/articles/tree-python</a> #This one is
more complex <a href="https://bigtree.readthedocs.io/en/0.14.8/"
class="uri">https://bigtree.readthedocs.io/en/0.14.8/</a> #There is this
package to create trees, but maybe it is too complex for us Pouly, Marc.
“Estimating Text Similarity based on Semantic Concept Embeddings.” arXiv
preprint arXiv:2401.04422 (2024).</p>
<pre class="python"><code>import torch
import einops
import math

from transformers import AutoModel
# Load the Jina AI embeddings model
model = AutoModel.from_pretrained(&quot;jinaai/jina-embeddings-v3&quot;, trust_remote_code=True)

taxonomy_tree = {
    &#39;1&#39;: {
        &#39;2&#39;: {
            &#39;A&#39;: &#39;Lake&#39;,
            &#39;B&#39;: &#39;River&#39;
        },
        &#39;C&#39;: &#39;House&#39;,
        &#39;3&#39;: {
            &#39;4&#39;: {
                &#39;D&#39;: &#39;Mountain&#39;,
                &#39;E&#39;: &#39;Everest&#39;,
                &#39;F&#39;: &#39;Volcano&#39;
            }
        }
    }
}

# Function to extract leaf nodes
def get_leaf_nodes(taxonomy):
    leaves = {}
    def traverse(node, path):
        if isinstance(node, dict):
            for k, v in node.items():
                traverse(v, path + [k])
        else:
            leaves[path[-1]] = node  # Leaf node with its path
    traverse(taxonomy, [])
    return leaves

# Function to extract groups based on the taxonomy structure
def extract_groups_from_taxonomy(taxonomy):
    groups = []
    
    def collect_groups(node, path=[]):
        if isinstance(node, dict):
            # For each child that is a dictionary (has children)
            dict_children = {k: v for k, v in node.items() if isinstance(v, dict)}
            
            # For each child that is a leaf node
            leaf_children = {k: v for k, v in node.items() if not isinstance(v, dict)}
            
            # If we have multiple leaf children, they form a group
            if len(leaf_children) &gt; 1:
                groups.append(list(leaf_children.values()))
            
            # Recursively process dictionary children
            for child_key, child_node in dict_children.items():
                collect_groups(child_node, path + [child_key])
    
    collect_groups(taxonomy)
    return groups

# Function to calculate similarity using the Jina AI embeddings model
def calculate_similarity(text1, text2):
    # Encode texts to get embeddings
    embeddings = model.encode([text1, text2])
    # Calculate cosine similarity
    sim = torch.nn.functional.cosine_similarity(torch.tensor(embeddings[0]), torch.tensor(embeddings[1]), dim=0)
    return sim.item()

# Function to calculate R(T)
def calculate_r_t(taxonomy):
    # Extract groups based on the taxonomy structure
    groups = extract_groups_from_taxonomy(taxonomy)
    
    leaves = get_leaf_nodes(taxonomy)
    leaf_names = list(leaves.values())
    
    total_groups = len(groups)
    r_t_values = []

    for idx, group in enumerate(groups):
        print(f&quot;\n--- Analyzing Group {idx+1}: {group} ---&quot;)
        
        # Skip groups with less than 2 characteristics (can&#39;t calculate similarity)
        if len(group) &lt; 2:
            print(f&quot;  Skipping group (less than 2 characteristics)&quot;)
            continue
            
        # Calculate pairwise similarities within the group
        similarities = []
        for i in range(len(group)):
            for j in range(i + 1, len(group)):
                sim = calculate_similarity(group[i], group[j])
                print(f&quot;  Similarity between &#39;{group[i]}&#39; and &#39;{group[j]}&#39;: {sim:.4f}&quot;)
                similarities.append(sim)

        min_similarity = min(similarities) if similarities else 0
        print(f&quot;  Minimum similarity within group: {min_similarity:.4f}&quot;)

        # Count intruders
        intruder_count = 0
        intruder_names = []
        
        for leaf in leaf_names:
            if leaf not in group:
                is_intruder = False
                max_sim = 0
                best_match = &quot;&quot;
                
                # Check similarity with EACH characteristic in the group
                for group_char in group:
                    sim_with_group = calculate_similarity(leaf, group_char)
                    if sim_with_group &gt; max_sim:
                        max_sim = sim_with_group
                        best_match = group_char
                    
                    if sim_with_group &gt; min_similarity:
                        is_intruder = True
                
                if is_intruder:
                    intruder_count += 1
                    intruder_names.append(leaf)
                    print(f&quot;  INTRUDER: &#39;{leaf}&#39; with similarity {max_sim:.4f} to &#39;{best_match}&#39; (exceeds min similarity)&quot;)
                else:
                    print(f&quot;  Not an intruder: &#39;{leaf}&#39; with max similarity {max_sim:.4f} to &#39;{best_match}&#39;&quot;)
        
        n_ic = intruder_count
        n_gc = len(group)
        n_ac = len(leaf_names)
        
        r_t = (1 - (n_ic / (n_gc * (n_ac - n_gc)))) if n_gc * (n_ac - n_gc) &gt; 0 else 0
        r_t_values.append(r_t)
        
        print(f&quot;  Group R(T): {r_t:.4f} (intruders: {intruder_count}, group size: {n_gc}, total chars: {n_ac})&quot;)
        if intruder_names:
            print(f&quot;  Intruders for this group: {intruder_names}&quot;)
        else:
            print(f&quot;  No intruders found for this group&quot;)

    final_rt = sum(r_t_values) / total_groups if total_groups &gt; 0 else 0
    print(f&quot;\nOverall R(T) = {final_rt:.4f} (average of {len(r_t_values)} group scores)&quot;)
    return final_rt

def extract_ncat(taxonomy):
    ncat = 0
    first_category_found = False  # Flag to track if the first category has been encountered

    def count_categories(node, is_root=True):
        nonlocal ncat, first_category_found
        if isinstance(node, dict):
            # Only count nodes that are not the root and not leaves
            if not is_root:
                if not first_category_found:
                    first_category_found = True  # Set the flag after the first category is found
                else:
                    ncat += 1  # Count the intermediate category
                    print(f&quot;Found category: {list(node.keys())}&quot;)  # Print the keys of the current category
            # Recursively process children, marking them as non-root
            for child in node.values():
                count_categories(child, is_root=False)

    count_categories(taxonomy)
    return ncat

def extract_nchar(taxonomy):
    nchar = 0

    def count_characteristics(node):
        nonlocal nchar
        if isinstance(node, dict):
            for child in node.values():
                count_characteristics(child)
        else:
            nchar += 1  # Count the current characteristic

    count_characteristics(taxonomy)
    return nchar

def extract_depths_cat(taxonomy):
    depths_cat = []

    def find_depths(node, depth):
        if isinstance(node, dict):
            depths_cat.append(depth)  # Record the depth of this category
            for child in node.values():
                find_depths(child, depth + 1)

    find_depths(taxonomy, 0)  # Start from depth 0
    return depths_cat
  
def extract_depths_char(taxonomy):
    depths_char = []

    def find_characteristic_depths(node, depth):
        if isinstance(node, dict):
            for child in node.values():
                find_characteristic_depths(child, depth + 1)
        else:
            depths_char.append(depth)  # Record the depth of this characteristic

    find_characteristic_depths(taxonomy, 0)  # Start from depth 0
    return depths_char

def calculate_conciseness(ncat, nchar, depths_cat, depths_char):
    &quot;&quot;&quot;
    Calculate the conciseness of the taxonomy using the proposed formula.

    Parameters:
    ncat (int): The number of categories.
    nchar (int): The number of characteristics.
    depths_cat (list): A list of depths for categories.
    depths_char (list): A list of depths for characteristics.

    Returns:
    float: The conciseness value of the taxonomy.
    &quot;&quot;&quot;
    # Calculate the sum of the inverses of the depths for categories and characteristics
    # Only include depths greater than 0 to avoid division by zero
    sum_cat = sum(1 / d for d in depths_cat if d &gt; 0) if ncat &gt; 0 else 0  # Sum for categories
    sum_char = sum(1 / d for d in depths_char if d &gt; 0) if nchar &gt; 0 else 0  # Sum for characteristics

    # Calculate the total sum of inverses of depths
    total_sum = sum_cat + sum_char

    # Calculate conciseness using the provided formula
    if total_sum &gt; 0:
        C_T = 1 / (1 + math.log(total_sum - 1))
    else:
        C_T = 0  # Return 0 if total_sum is not positive

    return C_T

# Execute the analysis
ncat = extract_ncat(taxonomy_tree)
nchar = extract_nchar(taxonomy_tree)
depths_cat = extract_depths_cat(taxonomy_tree)
depths_char = extract_depths_char(taxonomy_tree)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

# Display the groups that will be used for robustness calculation
groups = extract_groups_from_taxonomy(taxonomy_tree)
print(&quot;Groups identified for robustness calculation:&quot;)
for i, group in enumerate(groups):
    print(f&quot;Group {i+1}: {group}&quot;)

# Calculate R(T) for the given taxonomy
leaves = get_leaf_nodes(taxonomy_tree)
print(&quot;Leaf nodes:&quot;, leaves)
robustness_value = calculate_r_t(taxonomy_tree)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness = calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div
id="st-paper-a-software-cost-estimation-taxonomy-for-global-software-development-projects"
class="section level1">
<h1>1st paper a software cost estimation taxonomy for global software
development projects</h1>
<pre class="python"><code>new_taxonomy = {
    &#39;Cost estimation for GSD&#39;: {
        &#39;Cost estimation context&#39;: {
            &#39;Planning&#39;: {
                &quot;Conceptualization&quot;: &quot;Conceptualization&quot;,
                &quot;Feasibility study&quot;: &quot;Feasibility study&quot;,
                &quot;Preliminary planning&quot;: &quot;Preliminary planning&quot;,
                &quot;Detail Planning&quot;: &quot;Detail planning&quot;,
                &quot;Execution&quot;: &quot;Execution&quot;,
                &quot;Commissioning&quot;: &quot;Commissioning&quot;
            },
            &#39;Project activities&#39;: {
                &quot;System investigation&quot;: &quot;System investigation&quot;,
                &quot;Analysis&quot;: &quot;Analysis&quot;,
                &quot;Design&quot;: &quot;Design&quot;,
                &quot;Implementation&quot;: &quot;Implementation&quot;,
                &quot;Testing&quot;: &quot;Testing&quot;,
                &quot;Maintenance&quot;: &quot;Maintenance&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &#39;Project domain&#39;: {
                &quot;SE&quot;: &quot;Systems Engineering&quot;,
                &quot;Research &amp; Dev&quot;: {
                    &quot;Telecommunication&quot;: &quot;Telecommunication&quot;
                },
                &quot;Finance&quot;: &quot;Finance&quot;,
                &quot;Healthcare&quot;: &quot;Healthcare&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &#39;Project setting&#39;: {
                &quot;Close onshore&quot;: &quot;Close onshore&quot;,
                &quot;Distant onshore&quot;: &quot;Distant onshore&quot;,
                &quot;Near offshore&quot;: &quot;Near offshore&quot;,
                &quot;Far offshore&quot;: &quot;Far offshore&quot;
            },
            &#39;Planning approaches&#39;: {
                &quot;Constructive Cost Model&quot;: &quot;Constructive Cost Model&quot;,
                &quot;Capability Maturity Model Integration&quot;: &quot;Capability Maturity Model Integration&quot;,
                &quot;Agile&quot;: &quot;Agile&quot;,
                &quot;Delphi&quot;: &quot;Delphi&quot;,
                &quot;GA&quot;: &quot;Genetic Algorithms&quot;,
                &quot;CBR&quot;: &quot;Case-Based Reasoning&quot;,
                &quot;Fuzzy similar&quot;: &quot;Fuzzy similar&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &#39;Number of sites&#39;: {
                &quot;Value&quot;: &quot;Value&quot;
            },
            &#39;Team size&#39;: {
                &quot;No of team members&quot;: &quot;Number of team members&quot;
            }
        },
        &#39;Estimation technique&#39;: {
            &#39;Estimation technique&#39;: {
                &quot;Expert judgment&quot;: &quot;Expert judgment&quot;,
                &quot;Machine learning&quot;: &quot;Machine learning&quot;,
                &quot;Non-machine learning&quot;: &quot;Non-machine learning&quot;
            },
            &#39;Use technique&#39;: {
                &quot;Individual&quot;: &quot;Individual&quot;,
                &quot;Group-based estimation&quot;: &quot;Group-based estimation&quot;
            }
        },
        &#39;Cost estimate&#39;: {
            &#39;Estimated cost&#39;: {
                &quot;Estimate value&quot;: &quot;Estimated cost value&quot;
            },
            &#39;Actual cost&#39;: {
                &quot;Value&quot;: &quot;Actual cost value&quot;
            },
            &#39;Estimation dimension&#39;: {
                &quot;Effort hours&quot;: &quot;Effort hours&quot;,
                &quot;Staff/cost&quot;: &quot;Staff/cost&quot;,
                &quot;Hardware&quot;: &quot;Hardware&quot;,
                &quot;Risk&quot;: &quot;Risk&quot;,
                &quot;Portfolio&quot;: &quot;Portfolio&quot;
            },
            &#39;Accuracy measure&#39;: {
                &quot;Baseline comparison&quot;: &quot;Baseline comparison&quot;,
                &quot;Variation reduction&quot;: &quot;Variation reduction&quot;,
                &quot;Sensitivity analysis&quot;: &quot;Sensitivity analysis&quot;
            }
        },
        &#39;Cost estimators&#39;: {
            &#39;Product size&#39;: {
                &quot;Size report&quot;: &quot;Size report&quot;,
                &quot;Statistics analysis&quot;: &quot;Statistics analysis&quot;
            },
            &#39;Team experience&#39;: {
                &quot;Considered&quot;: &quot;Considered experience&quot;,
                &quot;Not considered&quot;: &quot;Not considered experience&quot;
            },
            &#39;Team structure&#39;: {
                &quot;Considered&quot;: &quot;Considered structure&quot;,
                &quot;Not Considered&quot;: &quot;Not considered structure&quot;
            },
            &#39;Product requirement&#39;: {
                &quot;Performance&quot;: &quot;Performance&quot;,
                &quot;Security&quot;: &quot;Security&quot;,
                &quot;Availability&quot;: &quot;Availability&quot;,
                &quot;Reliability&quot;: &quot;Reliability&quot;,
                &quot;Maintainability&quot;: &quot;Maintainability&quot;,
                &quot;Other&quot;: &quot;Other requirement&quot;
            },
            &#39;Distributed teams distances&#39;: {
                &quot;Geographical distance&quot;: &quot;Geographical distance&quot;,
                &quot;Temporal distance&quot;: &quot;Temporal distance&quot;,
                &quot;Socio-cultural distance&quot;: &quot;Socio-cultural distance&quot;
            }
        }
    }
}


#### Labeling ambiguous characteristics

new_taxonomy = {
    &#39;Cost estimation for GSD&#39;: {
        &#39;Cost estimation context&#39;: {
            &#39;Planning&#39;: {
                &quot;Conceptualization&quot;: &quot;Conceptualization&quot;,
                &quot;Feasibility study&quot;: &quot;Feasibility study&quot;,
                &quot;Preliminary planning&quot;: &quot;Preliminary planning&quot;,
                &quot;Detail Planning&quot;: &quot;Detail planning&quot;,
                &quot;Execution&quot;: &quot;Execution&quot;,
                &quot;Commissioning&quot;: &quot;Commissioning&quot;
            },
            &#39;Project activities&#39;: {
                &quot;System investigation&quot;: &quot;System investigation&quot;,
                &quot;Analysis&quot;: &quot;Analysis&quot;,
                &quot;Design&quot;: &quot;Design&quot;,
                &quot;Implementation&quot;: &quot;Implementation&quot;,
                &quot;Testing&quot;: &quot;Testing&quot;,
                &quot;Maintenance&quot;: &quot;Maintenance&quot;,
                &quot;Project activities.Other&quot;: &quot;Project activities.Other&quot;
            },
            &#39;Project domain&#39;: {
                &quot;SE&quot;: &quot;Systems Engineering&quot;,
                &quot;Research &amp; Dev&quot;:&quot;Research &amp; Dev&quot;,
                &quot;Telecommunication&quot;: &quot;Telecommunication&quot;,
                &quot;Finance&quot;: &quot;Finance&quot;,
                &quot;Healthcare&quot;: &quot;Healthcare&quot;,
                &quot;Project domain.Other&quot;: &quot;Project domain.Other&quot;
            },
            &#39;Project setting&#39;: {
                &quot;Close onshore&quot;: &quot;Close onshore&quot;,
                &quot;Distant onshore&quot;: &quot;Distant onshore&quot;,
                &quot;Near offshore&quot;: &quot;Near offshore&quot;,
                &quot;Far offshore&quot;: &quot;Far offshore&quot;
            },
            &#39;Planning approaches&#39;: {
                &quot;Constructive Cost Model&quot;: &quot;Constructive Cost Model&quot;,
                &quot;Capability Maturity Model Integration&quot;: &quot;Capability Maturity Model Integration&quot;,
                &quot;Agile&quot;: &quot;Agile&quot;,
                &quot;Delphi&quot;: &quot;Delphi&quot;,
                &quot;GA&quot;: &quot;Genetic Algorithms&quot;,
                &quot;CBR&quot;: &quot;Case-Based Reasoning&quot;,
                &quot;Fuzzy similar&quot;: &quot;Fuzzy similar&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &#39;Number of sites&#39;: {
                &quot;Number of sites.Value&quot;: &quot;Number of sites.Value&quot;
            },
            &#39;Team size&#39;: {
                &quot;No of team members&quot;: &quot;Number of team members&quot;
            }
        },
        &#39;Estimation technique&#39;: {
            &#39;Estimation technique&#39;: {
                &quot;Expert judgment&quot;: &quot;Expert judgment&quot;,
                &quot;Machine learning&quot;: &quot;Machine learning&quot;,
                &quot;Non-machine learning&quot;: &quot;Non-machine learning&quot;
            },
            &#39;Use technique&#39;: {
                &quot;Individual&quot;: &quot;Individual&quot;,
                &quot;Group-based estimation&quot;: &quot;Group-based estimation&quot;
            }
        },
        &#39;Cost estimate&#39;: {
            &#39;Estimated cost&#39;: {
                &quot;Estimate value&quot;: &quot;Estimated cost value&quot;
            },
            &#39;Actual cost&#39;: {
                &quot;Actual cost.Value&quot;: &quot;Actual cost.Value&quot;
            },
            &#39;Estimation dimension&#39;: {
                &quot;Effort hours&quot;: &quot;Effort hours&quot;,
                &quot;Staff/cost&quot;: &quot;Staff/cost&quot;,
                &quot;Hardware&quot;: &quot;Hardware&quot;,
                &quot;Risk&quot;: &quot;Risk&quot;,
                &quot;Portfolio&quot;: &quot;Portfolio&quot;
            },
            &#39;Accuracy measure&#39;: {
                &quot;Baseline comparison&quot;: &quot;Baseline comparison&quot;,
                &quot;Variation reduction&quot;: &quot;Variation reduction&quot;,
                &quot;Sensitivity analysis&quot;: &quot;Sensitivity analysis&quot;
            }
        },
        &#39;Cost estimators&#39;: {
            &#39;Product size&#39;: {
                &quot;Size report&quot;: &quot;Size report&quot;,
                &quot;Statistics analysis&quot;: &quot;Statistics analysis&quot;
            },
            &#39;Team experience&#39;: {
                &quot;Considered&quot;: &quot;Considered experience&quot;,
                &quot;Not considered&quot;: &quot;Not considered experience&quot;
            },
            &#39;Team structure&#39;: {
                &quot;Considered&quot;: &quot;Considered structure&quot;,
                &quot;Not Considered&quot;: &quot;Not considered structure&quot;
            },
            &#39;Product requirement&#39;: {
                &quot;Performance&quot;: &quot;Performance&quot;,
                &quot;Security&quot;: &quot;Security&quot;,
                &quot;Availability&quot;: &quot;Availability&quot;,
                &quot;Reliability&quot;: &quot;Reliability&quot;,
                &quot;Maintainability&quot;: &quot;Maintainability&quot;,
                &quot;Product requirement.Other&quot;: &quot;Product requirement.Other&quot;
            },
            &#39;Distributed teams distances&#39;: {
                &quot;Geographical distance&quot;: &quot;Geographical distance&quot;,
                &quot;Temporal distance&quot;: &quot;Temporal distance&quot;,
                &quot;Socio-cultural distance&quot;: &quot;Socio-cultural distance&quot;
            }
        }
    }
}



leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div id="nd-paper-a-taxonomy-of-web-effort-predictors"
class="section level1">
<h1>2nd paper, A taxonomy of web effort predictors</h1>
<pre class="python"><code>new_taxonomy = {
    &#39;Web Predictor&#39;: {
        &#39;Size Metric&#39;: {
            &#39;Length&#39;: {
                        &#39;Web page count&#39;: &#39;Web page count&#39;,
                        &#39;Media count&#39;: &#39;Media count&#39;,
                        &#39;New media count&#39;: &#39;New media count&#39;,
                        &#39;New Web page count&#39;: &#39;New Web page count&#39;,
                        &#39;Link count&#39;: &#39;Link count&#39;,
                        &#39;Program count&#39;: &#39;Program count&#39;,
                        &#39;Reused component count&#39;: &#39;Reused component count&#39;,
                        &#39;Lines of code&#39;: &#39;Lines of code&#39;,
                        &#39;Reused program count&#39;: &#39;Reused program count&#39;,
                        &#39;Reused media count&#39;: &#39;Reused media count&#39;,
                        &#39;Web page allocation&#39;: &#39;Web page allocation&#39;,
                        &#39;Reused lines of code&#39;: &#39;Reused lines of code&#39;,
                        &#39;Media allocation&#39;: &#39;Media allocation&#39;,
                        &#39;Reused media allocation&#39;: &#39;Reused media allocation&#39;,
                        &#39;Entity count&#39;: &#39;Entity count&#39;,
                        &#39;Attribute count&#39;: &#39;Attribute count&#39;,
                        &#39;Component count&#39;: &#39;Component count&#39;,
                        &#39;Statement count&#39;: &#39;Statement count&#39;,
                        &#39;Node count&#39;: &#39;Node count&#39;,
                        &#39;Collection slot size&#39;: &#39;Collection slot size&#39;,
                        &#39;Component granularity level&#39;: &#39;Component granularity level&#39;,
                        &#39;Slot granularity level&#39;: &#39;Slot granularity level&#39;,
                        &#39;Model node size&#39;: &#39;Model node size&#39;,
                        &#39;Cluster node size&#39;: &#39;Cluster node size&#39;,
                        &#39;Node slot size&#39;: &#39;Node slot size&#39;,
                        &#39;Publishing model unit count&#39;: &#39;Publishing model unit count&#39;,
                        &#39;Model slot size&#39;: &#39;Model slot size&#39;,
                        &#39;Association slot size&#39;: &#39;Association slot size&#39;,
                        &#39;Client script count&#39;: &#39;Client script count&#39;,
                        &#39;Server script count&#39;: &#39;Server script count&#39;,
                        &#39;Information slot count&#39;: &#39;Information slot count&#39;,
                        &#39;Association center slot count&#39;: &#39;Association center slot count&#39;,
                        &#39;Collection center slot count&#39;: &#39;Collection center slot count&#39;,
                        &#39;Component slot count&#39;: &#39;Component slot count&#39;,
                        &#39;Semantic association count&#39;: &#39;Semantic association count&#39;,
                        &#39;Segment count&#39;: &#39;Segment count&#39;,
                        &#39;Slot count&#39;: &#39;Slot count&#39;,
                        &#39;Cluster slot count&#39;: &#39;Cluster slot count&#39;,
                        &#39;Cluster count&#39;: &#39;Cluster count&#39;,
                        &#39;Publishing unit count&#39;: &#39;Publishing unit count&#39;,
                        &#39;Section count&#39;: &#39;Section count&#39;,
                        &#39;Inner/sub concern count&#39;: &#39;Inner/sub concern count&#39;,
                        &#39;Indifferent concern count&#39;: &#39;Indifferent concern count&#39;,
                        &#39;Module point cut count&#39;: &#39;Module point cut count&#39;,
                        &#39;Module count&#39;: &#39;Module count&#39;,
                        &#39;Module attribute count&#39;: &#39;Module attribute count&#39;,
                        &#39;Operation count&#39;: &#39;Operation count&#39;,
                        &#39;Comment count&#39;: &#39;Comment count&#39;,
                        &#39;Reused comment count&#39;: &#39;Reused comment count&#39;,
                        &#39;Media duration&#39;: &#39;Media duration&#39;,
                        &#39;Diffusion cut count&#39;: &#39;Diffusion cut count&#39;,
                        &#39;Concern module count&#39;: &#39;Concern module count&#39;,
                        &#39;Concern operation count&#39;: &#39;Concern operation count&#39;,
                        &#39;Anchor count&#39;: &#39;Anchor count&#39;},
            &#39;Functionality&#39;: {
                        &#39;High feature count&#39;: &#39;High feature count&#39;,
                        &#39;Low feature count&#39;: &#39;Low feature count&#39;,
                        &#39;Reused high feature count&#39;: &#39;Reused high feature count&#39;,
                        &#39;Reused low feature count&#39;: &#39;Reused low feature count&#39;,
                        &#39;Web objects&#39;: &#39;Web objects&#39;,
                        &#39;Common Software Measurement International Consortium&#39;: &#39;Common Software Measurement International Consortium&#39;,
                        &#39;International Function Point Users Group&#39;: &#39;International Function Point Users Group&#39;,
                        &#39;Object-Oriented Heuristic Function Points&#39;: &#39;Object-Oriented Heuristic Function Points&#39;,
                        &#39;Object-Oriented Function Points&#39;: &#39;Object-Oriented Function Points&#39;,
                        &#39;Use case count&#39;: &#39;Use case count&#39;,
                        &#39;Feature count&#39;: &#39;Feature count&#39;,
                        &#39;Data Web points&#39;: &#39;Data Web points&#39;},
            
            &#39;Object-oriented&#39;: {
                        &#39;Cohesion&#39;: &#39;Cohesion&#39;,
                        &#39;Class coupling&#39;: &#39;Class coupling&#39;,
                        &#39;Concern coupling&#39;: &#39;Concern coupling&#39;}, 

            &#39;Complexity&#39;: {
                        &#39;Connectivity density&#39;: &#39;Connectivity density&#39;,
                        &#39;Cyclomatic complexity&#39;: &#39;Cyclomatic complexity&#39;,
                        &#39;Model collection complexity&#39;: &#39;Model collection complexity&#39;,
                        &#39;Model association complexity&#39;: &#39;Model association complexity&#39;,
                        &#39;Model link complexity&#39;: &#39;Model link complexity&#39;,
                        &#39;Page complexity&#39;: &#39;Page complexity&#39;,
                        &#39;Component complexity&#39;: &#39;Component complexity&#39;,
                        &#39;Total complexity&#39;: &#39;Total complexity&#39;,
                        &#39;Adaptation complexity&#39;: &#39;Adaptation complexity&#39;,
                        &#39;New complexity&#39;: &#39;New complexity&#39;,
                        &#39;Data usage complexity&#39;: &#39;Data usage complexity&#39;,
                        &#39;Data flow complexity&#39;: &#39;Data flow complexity&#39;,
                        &#39;Cohesion complexity&#39;: &#39;Cohesion complexity&#39;,
                        &#39;Interface complexity&#39;: &#39;Interface complexity&#39;,
                        &#39;Control flow complexity&#39;: &#39;Control flow complexity&#39;,
                        &#39;Class complexity&#39;: &#39;Class complexity&#39;,
                        &#39;Layout complexity&#39;: &#39;Layout complexity&#39;,
                        &#39;Input complexity&#39;: &#39;Input complexity&#39;,
                        &#39;Output complexity&#39;: &#39;Output complexity&#39;} 
                        },
        &#39;Cost Driver&#39;: {
          &#39;Product&#39;:{
            &#39;Type&#39;: &#39;Type&#39;,
            &#39;Stratum&#39;: &#39;Stratum&#39;,
            &#39;Compactness&#39;: &#39;Compactness&#39;,
            &#39;Structure&#39;: &#39;Structure&#39;,
            &#39;Architecture&#39;: &#39;Architecture&#39;,
            &#39;Integration with legacy systems&#39;: &#39;Integration with legacy systems&#39;,
            &#39;Concurrency level&#39;: &#39;Concurrency level&#39;,
            &#39;Processing requirements&#39;: &#39;Processing requirements&#39;,
            &#39;Database size&#39;: &#39;Database size&#39;,
            &#39;Requirements volatility level&#39;: &#39;Requirements volatility level&#39;,
            &#39;Requirements novelty level&#39;: &#39;Requirements novelty level&#39;,
            &#39;Reliability level&#39;: &#39;Reliability level&#39;,
            &#39;Maintainability level&#39;: &#39;Maintainability level&#39;,
            &#39;Time efficiency level&#39;: &#39;Time efficiency level&#39;,
            &#39;Memory efficiency level&#39;: &#39;Memory efficiency level&#39;,
            &#39;Portability level&#39;: &#39;Portability level&#39;,
            &#39;Scalability level&#39;: &#39;Scalability level&#39;,
            &#39;Quality level&#39;: &#39;Quality level&#39;,
            &#39;Usability level&#39;: &#39;Usability level&#39;,
            &#39;Readability level&#39;: &#39;Readability level&#39;,
            &#39;Security level&#39;: &#39;Security level&#39;,
            &#39;Installability level&#39;: &#39;Installability level&#39;,
            &#39;Modularity level&#39;: &#39;Modularity level&#39;,
            &#39;Flexibility level&#39;: &#39;Flexibility level&#39;,
            &#39;Testability level&#39;: &#39;Testability level&#39;,
            &#39;Accessibility level&#39;: &#39;Accessibility level&#39;,
            &#39;Trainability level&#39;: &#39;Trainability level&#39;,
            &#39;Innovation level&#39;: &#39;Innovation level&#39;,
            &#39;Technical factors&#39;: &#39;Technical factors&#39;,
            &#39;Storage constraint&#39;: &#39;Storage constraint&#39;,
            &#39;Reusability level&#39;: &#39;Reusability level&#39;,
            &#39;Robustness level&#39;: &#39;Robustness level&#39;,
            &#39;Design volatility&#39;: &#39;Design volatility&#39;,
            &#39;Experience level&#39;: &#39;Experience level&#39;,
            &#39;Requirements clarity level&#39;: &#39;Requirements clarity level&#39;},
        &#39;Client&#39;: {
            &#39;Availability level&#39;: &#39;Availability level&#39;,
            &#39;IT literacy&#39;: &#39;IT literacy&#39;,
            &#39;Mapped workflows&#39;: &#39;Mapped workflows&#39;,
            &#39;Personality&#39;: &#39;Personality&#39;},
            
        &#39;Development Company&#39;: {
            &#39;SPI program&#39;: &#39;SPI program&#39;,
            &#39;Metrics’ program&#39;: &#39;Metrics’ program&#39;,
            &#39;Number of projects in parallel&#39;: &#39;Number of projects in parallel&#39;,
            &#39;Software reuse&#39;: &#39;Software reuse&#39;},
        &#39;Project&#39;: {
            &#39;Documentation level&#39;: &#39;Documentation level&#39;,
            &#39;Number of programming languages&#39;: &#39;Number of programming languages&#39;,
            &#39;Type&#39;: &#39;Type&#39;,
            &#39;Process efficiency level&#39;: &#39;Process efficiency level&#39;,
            &#39;Project management level&#39;: &#39;Project management level&#39;,
            &#39;Infrastructure&#39;: &#39;Infrastructure&#39;,
            &#39;Development restriction&#39;: &#39;Development restriction&#39;,
            &#39;Time restriction&#39;: &#39;Time restriction&#39;,
            &#39;Risk level&#39;: &#39;Risk level&#39;,
            &#39;Rapid app development&#39;: &#39;Rapid app development&#39;,
            &#39;Operational mode&#39;: &#39;Operational mode&#39;,
            &#39;Resource level&#39;: &#39;Resource level&#39;,
            &#39;Lessons learned repository&#39;: &#39;Lessons learned repository&#39;},            
        &#39;Team&#39;: {
            &#39;Domain experience level&#39;: &#39;Domain experience level&#39;,
            &#39;Team size&#39;: &#39;Team size&#39;,
            &#39;Deployment platform experience level&#39;: &#39;Deployment platform experience level&#39;,
            &#39;Team capability&#39;: &#39;Team capability&#39;,
            &#39;Programming language experience level&#39;: &#39;Programming language experience level&#39;,
            &#39;Tool experience level&#39;: &#39;Tool experience level&#39;,
            &#39;Communication level&#39;: &#39;Communication level&#39;,
            &#39;Software development experience&#39;: &#39;Software development experience&#39;,
            &#39;Work Team level&#39;: &#39;Work Team level&#39;,
            &#39;Stability level&#39;: &#39;Stability level&#39;,
            &#39;Motivation level&#39;: &#39;Motivation level&#39;,
            &#39;Focus factor&#39;: &#39;Focus factor&#39;,
            &#39;Tool experience level&#39;: &#39;Tool experience level&#39;,
            &#39;OO experience level&#39;: &#39;OO experience level&#39;,
            &#39;In-house experience&#39;: &#39;In-house experience&#39;},
        &#39;Technology&#39;: {
            &#39;Authoring tool type&#39;: &#39;Authoring tool type&#39;,
            &#39;Productivity level&#39;: &#39;Productivity level&#39;,
            &#39;Novelty level&#39;: &#39;Novelty level&#39;,
            &#39;Platform volatility level&#39;: &#39;Platform volatility level&#39;,
            &#39;Difficulty level&#39;: &#39;Difficulty level&#39;,
            &#39;Platform support level&#39;: &#39;Platform support level&#39;}}
          
}
}

#### Labeling ambiguous characteristics

new_taxonomy = {
    &#39;Web Predictor&#39;: {
        &#39;Size Metric&#39;: {
            &#39;Length&#39;: {
                        &#39;Web page count&#39;: &#39;Web page count&#39;,
                        &#39;Media count&#39;: &#39;Media count&#39;,
                        &#39;New media count&#39;: &#39;New media count&#39;,
                        &#39;New Web page count&#39;: &#39;New Web page count&#39;,
                        &#39;Link count&#39;: &#39;Link count&#39;,
                        &#39;Program count&#39;: &#39;Program count&#39;,
                        &#39;Reused component count&#39;: &#39;Reused component count&#39;,
                        &#39;Lines of code&#39;: &#39;Lines of code&#39;,
                        &#39;Reused program count&#39;: &#39;Reused program count&#39;,
                        &#39;Reused media count&#39;: &#39;Reused media count&#39;,
                        &#39;Web page allocation&#39;: &#39;Web page allocation&#39;,
                        &#39;Reused lines of code&#39;: &#39;Reused lines of code&#39;,
                        &#39;Media allocation&#39;: &#39;Media allocation&#39;,
                        &#39;Reused media allocation&#39;: &#39;Reused media allocation&#39;,
                        &#39;Entity count&#39;: &#39;Entity count&#39;,
                        &#39;Attribute count&#39;: &#39;Attribute count&#39;,
                        &#39;Component count&#39;: &#39;Component count&#39;,
                        &#39;Statement count&#39;: &#39;Statement count&#39;,
                        &#39;Node count&#39;: &#39;Node count&#39;,
                        &#39;Collection slot size&#39;: &#39;Collection slot size&#39;,
                        &#39;Component granularity level&#39;: &#39;Component granularity level&#39;,
                        &#39;Slot granularity level&#39;: &#39;Slot granularity level&#39;,
                        &#39;Model node size&#39;: &#39;Model node size&#39;,
                        &#39;Cluster node size&#39;: &#39;Cluster node size&#39;,
                        &#39;Node slot size&#39;: &#39;Node slot size&#39;,
                        &#39;Publishing model unit count&#39;: &#39;Publishing model unit count&#39;,
                        &#39;Model slot size&#39;: &#39;Model slot size&#39;,
                        &#39;Association slot size&#39;: &#39;Association slot size&#39;,
                        &#39;Client script count&#39;: &#39;Client script count&#39;,
                        &#39;Server script count&#39;: &#39;Server script count&#39;,
                        &#39;Information slot count&#39;: &#39;Information slot count&#39;,
                        &#39;Association center slot count&#39;: &#39;Association center slot count&#39;,
                        &#39;Collection center slot count&#39;: &#39;Collection center slot count&#39;,
                        &#39;Component slot count&#39;: &#39;Component slot count&#39;,
                        &#39;Semantic association count&#39;: &#39;Semantic association count&#39;,
                        &#39;Segment count&#39;: &#39;Segment count&#39;,
                        &#39;Slot count&#39;: &#39;Slot count&#39;,
                        &#39;Cluster slot count&#39;: &#39;Cluster slot count&#39;,
                        &#39;Cluster count&#39;: &#39;Cluster count&#39;,
                        &#39;Publishing unit count&#39;: &#39;Publishing unit count&#39;,
                        &#39;Section count&#39;: &#39;Section count&#39;,
                        &#39;Inner/sub concern count&#39;: &#39;Inner/sub concern count&#39;,
                        &#39;Indifferent concern count&#39;: &#39;Indifferent concern count&#39;,
                        &#39;Module point cut count&#39;: &#39;Module point cut count&#39;,
                        &#39;Module count&#39;: &#39;Module count&#39;,
                        &#39;Module attribute count&#39;: &#39;Module attribute count&#39;,
                        &#39;Operation count&#39;: &#39;Operation count&#39;,
                        &#39;Comment count&#39;: &#39;Comment count&#39;,
                        &#39;Reused comment count&#39;: &#39;Reused comment count&#39;,
                        &#39;Media duration&#39;: &#39;Media duration&#39;,
                        &#39;Diffusion cut count&#39;: &#39;Diffusion cut count&#39;,
                        &#39;Concern module count&#39;: &#39;Concern module count&#39;,
                        &#39;Concern operation count&#39;: &#39;Concern operation count&#39;,
                        &#39;Anchor count&#39;: &#39;Anchor count&#39;},
            &#39;Functionality&#39;: {
                        &#39;High feature count&#39;: &#39;High feature count&#39;,
                        &#39;Low feature count&#39;: &#39;Low feature count&#39;,
                        &#39;Reused high feature count&#39;: &#39;Reused high feature count&#39;,
                        &#39;Reused low feature count&#39;: &#39;Reused low feature count&#39;,
                        &#39;Web objects&#39;: &#39;Web objects&#39;,
                        &#39;Common Software Measurement International Consortium&#39;: &#39;Common Software Measurement International Consortium&#39;,
                        &#39;International Function Point Users Group&#39;: &#39;International Function Point Users Group&#39;,
                        &#39;Object-Oriented Heuristic Function Points&#39;: &#39;Object-Oriented Heuristic Function Points&#39;,
                        &#39;Object-Oriented Function Points&#39;: &#39;Object-Oriented Function Points&#39;,
                        &#39;Use case count&#39;: &#39;Use case count&#39;,
                        &#39;Feature count&#39;: &#39;Feature count&#39;,
                        &#39;Data Web points&#39;: &#39;Data Web points&#39;},
            
            &#39;Object-oriented&#39;: {
                        &#39;Cohesion&#39;: &#39;Cohesion&#39;,
                        &#39;Class coupling&#39;: &#39;Class coupling&#39;,
                        &#39;Concern coupling&#39;: &#39;Concern coupling&#39;}, 

            &#39;Complexity&#39;: {
                        &#39;Connectivity density&#39;: &#39;Connectivity density&#39;,
                        &#39;Cyclomatic complexity&#39;: &#39;Cyclomatic complexity&#39;,
                        &#39;Model collection complexity&#39;: &#39;Model collection complexity&#39;,
                        &#39;Model association complexity&#39;: &#39;Model association complexity&#39;,
                        &#39;Model link complexity&#39;: &#39;Model link complexity&#39;,
                        &#39;Page complexity&#39;: &#39;Page complexity&#39;,
                        &#39;Component complexity&#39;: &#39;Component complexity&#39;,
                        &#39;Total complexity&#39;: &#39;Total complexity&#39;,
                        &#39;Adaptation complexity&#39;: &#39;Adaptation complexity&#39;,
                        &#39;New complexity&#39;: &#39;New complexity&#39;,
                        &#39;Data usage complexity&#39;: &#39;Data usage complexity&#39;,
                        &#39;Data flow complexity&#39;: &#39;Data flow complexity&#39;,
                        &#39;Cohesion complexity&#39;: &#39;Cohesion complexity&#39;,
                        &#39;Interface complexity&#39;: &#39;Interface complexity&#39;,
                        &#39;Control flow complexity&#39;: &#39;Control flow complexity&#39;,
                        &#39;Class complexity&#39;: &#39;Class complexity&#39;,
                        &#39;Layout complexity&#39;: &#39;Layout complexity&#39;,
                        &#39;Input complexity&#39;: &#39;Input complexity&#39;,
                        &#39;Output complexity&#39;: &#39;Output complexity&#39;} 
                        },
        &#39;Cost Driver&#39;: {
          &#39;Product&#39;:{
            &#39;Product.Type&#39;: &#39;Product.Type&#39;,
            &#39;Stratum&#39;: &#39;Stratum&#39;,
            &#39;Compactness&#39;: &#39;Compactness&#39;,
            &#39;Product.Structure&#39;: &#39;Product.Structure&#39;,
            &#39;Architecture&#39;: &#39;Architecture&#39;,
            &#39;Integration with legacy systems&#39;: &#39;Integration with legacy systems&#39;,
            &#39;Concurrency level&#39;: &#39;Concurrency level&#39;,
            &#39;Processing requirements&#39;: &#39;Processing requirements&#39;,
            &#39;Database size&#39;: &#39;Database size&#39;,
            &#39;Requirements volatility level&#39;: &#39;Requirements volatility level&#39;,
            &#39;Requirements novelty level&#39;: &#39;Requirements novelty level&#39;,
            &#39;Reliability level&#39;: &#39;Reliability level&#39;,
            &#39;Maintainability level&#39;: &#39;Maintainability level&#39;,
            &#39;Time efficiency level&#39;: &#39;Time efficiency level&#39;,
            &#39;Memory efficiency level&#39;: &#39;Memory efficiency level&#39;,
            &#39;Portability level&#39;: &#39;Portability level&#39;,
            &#39;Scalability level&#39;: &#39;Scalability level&#39;,
            &#39;Quality level&#39;: &#39;Quality level&#39;,
            &#39;Usability level&#39;: &#39;Usability level&#39;,
            &#39;Readability level&#39;: &#39;Readability level&#39;,
            &#39;Security level&#39;: &#39;Security level&#39;,
            &#39;Installability level&#39;: &#39;Installability level&#39;,
            &#39;Modularity level&#39;: &#39;Modularity level&#39;,
            &#39;Flexibility level&#39;: &#39;Flexibility level&#39;,
            &#39;Testability level&#39;: &#39;Testability level&#39;,
            &#39;Accessibility level&#39;: &#39;Accessibility level&#39;,
            &#39;Trainability level&#39;: &#39;Trainability level&#39;,
            &#39;Innovation level&#39;: &#39;Innovation level&#39;,
            &#39;Technical factors&#39;: &#39;Technical factors&#39;,
            &#39;Storage constraint&#39;: &#39;Storage constraint&#39;,
            &#39;Reusability level&#39;: &#39;Reusability level&#39;,
            &#39;Robustness level&#39;: &#39;Robustness level&#39;,
            &#39;Design volatility&#39;: &#39;Design volatility&#39;,
            &#39;Product.Experience level&#39;: &#39;Product.Experience level&#39;,
            &#39;Requirements clarity level&#39;: &#39;Requirements clarity level&#39;},
        &#39;Client&#39;: {
            &#39;Availability level&#39;: &#39;Availability level&#39;,
            &#39;IT literacy&#39;: &#39;IT literacy&#39;,
            &#39;Mapped workflows&#39;: &#39;Mapped workflows&#39;,
            &#39;Personality&#39;: &#39;Personality&#39;},
            
        &#39;Development Company&#39;: {
            &#39;SPI program&#39;: &#39;SPI program&#39;,
            &#39;Metrics program&#39;: &#39;Metrics program&#39;,
            &#39;Number of projects in parallel&#39;: &#39;Number of projects in parallel&#39;,
            &#39;Software reuse&#39;: &#39;Software reuse&#39;},
        &#39;Project&#39;: {
            &#39;Documentation level&#39;: &#39;Documentation level&#39;,
            &#39;Number of programming languages&#39;: &#39;Number of programming languages&#39;,
            &#39;Project.Type&#39;: &#39;Project.Type&#39;,
            &#39;Process efficiency level&#39;: &#39;Process efficiency level&#39;,
            &#39;Project management level&#39;: &#39;Project management level&#39;,
            &#39;Project.Infrastructure&#39;: &#39;Project.Infrastructure&#39;,
            &#39;Development restriction&#39;: &#39;Development restriction&#39;,
            &#39;Time restriction&#39;: &#39;Time restriction&#39;,
            &#39;Risk level&#39;: &#39;Risk level&#39;,
            &#39;Rapid app development&#39;: &#39;Rapid app development&#39;,
            &#39;Operational mode&#39;: &#39;Operational mode&#39;,
            &#39;Resource level&#39;: &#39;Resource level&#39;,
            &#39;Lessons learned repository&#39;: &#39;Lessons learned repository&#39;},            
        &#39;Team&#39;: {
            &#39;Domain experience level&#39;: &#39;Domain experience level&#39;,
            &#39;Team size&#39;: &#39;Team size&#39;,
            &#39;Deployment platform experience level&#39;: &#39;Deployment platform experience level&#39;,
            &#39;Team capability&#39;: &#39;Team capability&#39;,
            &#39;Programming language experience level&#39;: &#39;Programming language experience level&#39;,
            &#39;Tool experience level&#39;: &#39;Tool experience level&#39;,
            &#39;Communication level&#39;: &#39;Communication level&#39;,
            &#39;Software development experience&#39;: &#39;Software development experience&#39;,
            &#39;Work Team level&#39;: &#39;Work Team level&#39;,
            &#39;Stability level&#39;: &#39;Stability level&#39;,
            &#39;Motivation level&#39;: &#39;Motivation level&#39;,
            &#39;Focus factor&#39;: &#39;Focus factor&#39;,
            &#39;OO experience level&#39;: &#39;OO experience level&#39;,
            &#39;In-house experience&#39;: &#39;In-house experience&#39;},
        &#39;Technology&#39;: {
            &#39;Authoring tool type&#39;: &#39;Authoring tool type&#39;,
            &#39;Productivity level&#39;: &#39;Productivity level&#39;,
            &#39;Novelty level&#39;: &#39;Novelty level&#39;,
            &#39;Platform volatility level&#39;: &#39;Platform volatility level&#39;,
            &#39;Difficulty level&#39;: &#39;Difficulty level&#39;,
            &#39;Platform support level&#39;: &#39;Platform support level&#39;}}
          
}
}

leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div
id="rd-paper-a-specialized-global-software-engineering-taxonomy-for-effort-estimation"
class="section level1">
<h1>3rd Paper A specialized global software engineering taxonomy for
effort estimation</h1>
<pre class="python"><code>new_taxonomy = {
    &#39;GSE&#39;: {
        &#39;Project&#39;: {
            &#39;Site&#39;: {
                &quot;Location&quot;: &quot;Location&quot;,
                &quot;Legal Entity&quot;: &quot;Legal Entity&quot;,
                &quot;Geographic Distance&quot;: &quot;Geographic Distance&quot;,
                &quot;Temporal Distance&quot;: &quot;Temporal Distance&quot;,
                &quot;Estimation stage&quot;: {
                    &quot;Early&quot;: &quot;Early&quot;,
                    &quot;Early &amp; Late&quot;: &quot;Early &amp; Late&quot;,
                    &quot;Late&quot;: &quot;Late&quot;
                },
                &quot;Estimation process role&quot;: {
                    &quot;Estimator&quot;: &quot;Estimator&quot;,
                    &quot;Estimator &amp; Provider&quot;: &quot;Estimator &amp; Provider&quot;,
                    &quot;Provider&quot;: &quot;Provider&quot;
                }
            },
            &#39;Relationship&#39;: {
                &quot;Location&quot;: &quot;Location&quot;,
                &quot;Legal Entity&quot;: &quot;Legal Entity&quot;,
                &quot;Geographic Distance&quot;: &quot;Geographic Distance&quot;,
                &quot;Temporal Distance&quot;: &quot;Temporal Distance&quot;,
                &quot;Estimation process architectural model&quot;: {
                    &quot;Centralized&quot;: &quot;Centralized&quot;,
                    &quot;Distributed&quot;: &quot;Distributed&quot;,
                    &quot;Semi-distributed&quot;: &quot;Semi-distributed&quot;
                }
            }
        }
    }
}

new_taxonomy = {
    &#39;GSE&#39;: {
        &#39;Project&#39;: {
            &#39;Site&#39;: {
                &quot;Site.Location&quot;: &quot;Site.Location&quot;,
                &quot;Site.Legal Entity&quot;: &quot;Site.Legal Entity&quot;,
                &quot;Site.Geographic Distance&quot;: &quot;Site.Geographic Distance&quot;,
                &quot;Site.Temporal Distance&quot;: &quot;Site.Temporal Distance&quot;,
                &quot;Estimation stage&quot;: {
                    &quot;Early&quot;: &quot;Early&quot;,
                    &quot;Early &amp; Late&quot;: &quot;Early &amp; Late&quot;,
                    &quot;Late&quot;: &quot;Late&quot;
                },
                &quot;Estimation process role&quot;: {
                    &quot;Estimator&quot;: &quot;Estimator&quot;,
                    &quot;Estimator &amp; Provider&quot;: &quot;Estimator &amp; Provider&quot;,
                    &quot;Provider&quot;: &quot;Provider&quot;
                }
            },
            &#39;Relationship&#39;: {
                &quot;Relationship.Location&quot;: &quot;Relationship.Location&quot;,
                &quot;Relationship.Legal Entity&quot;: &quot;Relationship.Legal Entity&quot;,
                &quot;Relationship.Geographic Distance&quot;: &quot;Relationship.Geographic Distance&quot;,
                &quot;Relationship.Temporal Distance&quot;: &quot;Relationship.Temporal Distance&quot;,
                &quot;Estimation process architectural model&quot;: {
                    &quot;Centralized&quot;: &quot;Centralized&quot;,
                    &quot;Distributed&quot;: &quot;Distributed&quot;,
                    &quot;Semi-distributed&quot;: &quot;Semi-distributed&quot;
                }
            }
        }
    }
}


leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div
id="rth-paper-a-taxonomy-of-approaches-and-methods-for-software-effort-estimation"
class="section level1">
<h1>4rth Paper: A taxonomy of Approaches and Methods for Software Effort
Estimation</h1>
<pre class="python"><code>new_taxonomy = {
    &#39;Software estimation&#39;: {
        &#39;Basic Estimating Methods&#39;: {
            &quot;Algorithmic&quot;: {
                &quot;Constructive Cost Model&quot;: &quot;Constructive Cost Model&quot;,
                &quot;Software Life Cycle Management&quot;: &quot;Software Life Cycle Management&quot;,
                &quot;Software Evaluation and Estimation for Risk&quot;: &quot;Software Evaluation and Estimation for Risk&quot;
            },
            &quot;Non-Algorithmic&quot;: {
                &quot;Expert Judgment&quot;: &quot;Expert Judgment&quot;,  # Corrected spelling
                &quot;Analogy-Based&quot;: &quot;Analogy-Based&quot;
            }
        },
        &#39;Combined Estimating Methods&#39;: {
            &quot;Basic-Combination&quot;: &quot;Basic-Combination&quot;,
            &quot;AI-Combination&quot;: {
                &quot;Fuzzy Logic&quot;: &quot;Fuzzy Logic&quot;,
                &quot;Artificial Neural Networks&quot;: &quot;Artificial Neural Networks&quot;,
                &quot;Computational Intelligence&quot;: {  # Corrected spelling
                    &quot;swarm&quot;: &quot;swarm&quot;,
                    &quot;evolutionary&quot;: &quot;&quot;
                }
            },
            &quot;AI-Combined hybrid&quot;: &quot;AI-Combined hybrid&quot;
        }
    }
}


new_taxonomy = {
    &#39;Software estimation&#39;: {
        &#39;Basic Estimating Methods&#39;: {
            &quot;Algorithmic&quot;: {
                &quot;Constructive Cost Model&quot;: &quot;Constructive Cost Model&quot;,
                &quot;Software Life Cycle Management&quot;: &quot;Software Life Cycle Management&quot;,
                &quot;Software Evaluation and Estimation for Risk&quot;: &quot;Software Evaluation and Estimation for Risk&quot;
            },
            &quot;Non-Algorithmic&quot;: {
                &quot;Expert Judgment&quot;: &quot;Expert Judgment&quot;,
                &quot;Analogy-Based&quot;: &quot;Analogy-Based&quot;
            }
        },
        &#39;Combined Estimating Methods&#39;: {
            &quot;Basic-Combination&quot;: &quot;Basic-Combination&quot;,
            &quot;AI-Combination&quot;: {
                &quot;Fuzzy Logic&quot;: &quot;Fuzzy Logic&quot;,
                &quot;Artificial Neural Networks&quot;: &quot;Artificial Neural Networks&quot;,
                &quot;Computational Intelligence&quot;: {
                    &quot;Computational Intelligence.swarm&quot;: &quot;Computational Intelligence.swarm&quot;,
                    &quot;Computational Intelligence.evolutionary&quot;: &quot;Computational Intelligence.evolutionary&quot;
                }
            },
            &quot;AI-Combined hybrid&quot;: &quot;AI-Combined hybrid&quot;
        }
    }
}


leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div
id="th-paper-towards-a-taxonomy-of-hypermedia-and-web-application-size-metrics."
class="section level1">
<h1>5th Paper, Towards a Taxonomy of Hypermedia and Web Application Size
Metrics.</h1>
<pre class="python"><code>new_taxonomy = {
  &quot;Hypermedia and Web Application Size Metrics&quot;:{
    &quot;Motivation&quot;:&quot;Motivation&quot;,
    &quot;Harvesting time&quot;:{
      &quot;Early size metric&quot;:&quot;Early size metric&quot;,
      &quot;Late size metric&quot;:&quot;Late size metric&quot;},
    &quot;Metric foundation&quot;:{
      &quot;Problem-oriented metric&quot;:&quot;Problem-oriented metric&quot;,
      &quot;Solution-oriented metric&quot;:&quot;Solution-oriented metric&quot;},
    &quot;Class&quot;:{
      &quot;Length&quot;:&quot;Length&quot;,
      &quot;Functionality&quot;:&quot;Functionality&quot;,
      &quot;Complexity&quot;:&quot;Complexity&quot;},
    &quot;Entity&quot;:{
      &quot;Web hypermedia application&quot;:&quot;Web hypermedia application&quot;,
      &quot;Web software application&quot;:&quot;Web software application&quot;,
      &quot;Web application&quot;:&quot;Web application&quot;,
      &quot;Media&quot;:&quot;Media&quot;,
      &quot;Program/Script&quot;:&quot;Program/Sript&quot;},
    &quot;Measurement Scale&quot;:{
      &quot;Nominal&quot;:&quot;Nominal&quot;,
      &quot;Ordinal&quot;:&quot;Ordinal&quot;,
      &quot;Interval&quot;:&quot;Interval&quot;,
      &quot;Ratio&quot;:&quot;Ratio&quot;,
      &quot;Absolute&quot;:&quot;Absolute&quot;},
    &quot;Computation&quot;:{
      &quot;Direct&quot;:&quot;Direct&quot;,
      &quot;Indirect&quot;:&quot;Indirect&quot;},
    &quot;Validation&quot;:{
      &quot;Validated Empirically&quot;:&quot;Validated Empirically&quot;,
      &quot;Validated Theoretically&quot;:&quot;Validated Theoretically&quot;,
      &quot;Both&quot;:&quot;Both&quot;,
      &quot;None&quot;:&quot;None&quot;},
    &quot;Model dependency&quot;:{
      &quot;Specific&quot;:&quot;Specific&quot;,
      &quot;Nonspecific&quot;:&quot;Nonspecific&quot;}
}
}

new_taxonomy = {
  &quot;Hypermedia and Web Application Size Metrics&quot;:{
    &quot;Motivation&quot;:&quot;Motivation&quot;,
    &quot;Harvesting time&quot;:{
      &quot;Early size metric&quot;:&quot;Early size metric&quot;,
      &quot;Late size metric&quot;:&quot;Late size metric&quot;},
    &quot;Metric foundation&quot;:{
      &quot;Problem-oriented metric&quot;:&quot;Problem-oriented metric&quot;,
      &quot;Solution-oriented metric&quot;:&quot;Solution-oriented metric&quot;},
    &quot;Class&quot;:{
      &quot;Class.Length&quot;:&quot;Class.Length&quot;,
      &quot;Functionality&quot;:&quot;Functionality&quot;,
      &quot;Complexity&quot;:&quot;Complexity&quot;},
    &quot;Entity&quot;:{
      &quot;Web hypermedia application&quot;:&quot;Web hypermedia application&quot;,
      &quot;Web software application&quot;:&quot;Web software application&quot;,
      &quot;Web application&quot;:&quot;Web application&quot;,
      &quot;Media&quot;:&quot;Media&quot;,
      &quot;Program/Script&quot;:&quot;Program/Script&quot;},
    &quot;Measurement Scale&quot;:{
      &quot;Nominal&quot;:&quot;Nominal&quot;,
      &quot;Ordinal&quot;:&quot;Ordinal&quot;,
      &quot;Interval&quot;:&quot;Interval&quot;,
      &quot;Ratio&quot;:&quot;Ratio&quot;,
      &quot;Absolute&quot;:&quot;Absolute&quot;},
    &quot;Computation&quot;:{
      &quot;Computation.Direct&quot;:&quot;Computation.Direct&quot;,
      &quot;Computation.Indirect&quot;:&quot;Computation.Indirect&quot;},
    &quot;Validation&quot;:{
      &quot;Validated Empirically&quot;:&quot;Validated Empirically&quot;,
      &quot;Validated Theoretically&quot;:&quot;Validated Theoretically&quot;,
      &quot;Validation.Both&quot;:&quot;Validation.Both&quot;,
      &quot;Validation.None&quot;:&quot;Validation.None&quot;},
    &quot;Model dependency&quot;:{
      &quot;Model dependency.Specific&quot;:&quot;Model dependency.Specific&quot;,
      &quot;Model dependency.Nonspecific&quot;:&quot;Model dependency.Nonspecific&quot;}
}
}

leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
</div>
<div
id="th-paper-an-effort-estimation-taxonomy-for-agile-software-development"
class="section level1">
<h1>6th Paper, An Effort Estimation Taxonomy for Agile Software
Development</h1>
<pre class="python"><code>new_taxonomy = {
    &#39;Effort Estimation in ASD&#39;: {
        &#39;Estimation context&#39;: {
            &quot;Planning level&quot;: {
                &quot;Release&quot;: &quot;Release&quot;,
                &quot;Sprint&quot;: &quot;Sprint&quot;,
                &quot;Daily&quot;: &quot;Daily&quot;,
                &quot;Bidding&quot;: &quot;Bidding&quot;
            },
            &quot;Estimated activities&quot;: {
                &quot;Analysis&quot;: &quot;Analysis&quot;,
                &quot;Design&quot;: &quot;Design&quot;,
                &quot;Implementation&quot;: &quot;Implementation&quot;,
                &quot;Testing&quot;: &quot;Testing&quot;,
                &quot;Maintenance&quot;: &quot;Maintenance&quot;,
                &quot;All&quot;: &quot;All&quot;
            },
            &quot;Agile methods&quot;: {
                &quot;Extreme Programming&quot;: &quot;Extreme Programming&quot;,
                &quot;Scrum&quot;: &quot;Scrum&quot;,
                &quot;Customized Extreme Programming&quot;: &quot;Customized Extreme Programming&quot;,
                &quot;Customized Scrum&quot;: &quot;Customized Scrum&quot;,
                &quot;Dynamic Systems Development Method&quot;: &quot;Dynamic Systems Development Method&quot;,
                &quot;Crystal&quot;: &quot;Crystal&quot;,
                &quot;Feature-Driven Development&quot;: &quot;Feature-Driven Development&quot;,
                &quot;Kanban&quot;: &quot;Kanban&quot;
            },
            &quot;Project domain&quot;: {
                &quot;Communications industry&quot;: &quot;Communications industry&quot;,
                &quot;Transportation&quot;: &quot;Transportation&quot;,
                &quot;Financial&quot;: &quot;Financial&quot;,
                &quot;Education&quot;: &quot;Education&quot;,
                &quot;Health&quot;: &quot;Health&quot;,
                &quot;Retail/Wholesale&quot;: &quot;Retail/Wholesale&quot;,
                &quot;Manufacturing&quot;: &quot;Manufacturing&quot;,
                &quot;Government/Military&quot;: &quot;Government/Military&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &quot;Project setting&quot;: {
                &quot;Co-located&quot;: &quot;Co-located&quot;,
                &quot;Distributed: Close Onshore&quot;: &quot;Distributed: Close Onshore&quot;,
                &quot;Distributed: Distant Onshore&quot;: &quot;Distributed: Distant Onshore&quot;,
                &quot;Distributed: Near Offshore&quot;: &quot;Distributed: Near Offshore&quot;,
                &quot;Distributed: Far Offshore&quot;: &quot;Distributed: Far Offshore&quot;
            },
            &quot;Estimation entity&quot;: {
                &quot;User story&quot;: &quot;User story&quot;,
                &quot;Task&quot;: &quot;Task&quot;,
                &quot;Use case&quot;: &quot;Use case&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &quot;Number of entities estimated&quot;: {
                &quot;Value&quot;: &quot;Value&quot;
            },
            &quot;Team size&quot;: {
                &quot;No. of team members&quot;: &quot;No. of team members&quot;
            }
        },
        &#39;Estimation technique&#39;: {
            &quot;Estimation Techniques&quot;: {
                &quot;Planning Poker&quot;: &quot;Planning Poker&quot;,
                &quot;Expert Judgement&quot;: &quot;Expert Judgement&quot;,
                &quot;Analogy&quot;: &quot;Analogy&quot;,
                &quot;Use case points method&quot;: &quot;Use case points method&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &quot;Type&quot;: {
                &quot;Single&quot;: &quot;Single&quot;,
                &quot;Group&quot;: &quot;Group&quot;
            }
        },
        &#39;Effort predictors&#39;: {
            &quot;Size&quot;: {
                &quot;Story points&quot;: &quot;Story points&quot;,
                &quot;User case points&quot;: &quot;User case points&quot;,
                &quot;Function points&quot;: &quot;Function points&quot;,
                &quot;Other&quot;: &quot;Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;,
                &quot;Considered without any metric&quot;: &quot;Considered without any metric&quot;
            },
            &quot;Team&#39;s prior experience&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Team&#39;s skill level&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Non functional requirements&quot;: {
                &quot;Performance&quot;: &quot;Performance&quot;,
                &quot;Security&quot;: &quot;Security&quot;,
                &quot;Availability&quot;: &quot;Availability&quot;,
                &quot;Reliability&quot;: &quot;Reliability&quot;,
                &quot;Maintainability&quot;: &quot;Maintainability&quot;,
                &quot;Other&quot;: &quot;Other&quot;,  # Changed period to comma
                &quot;Not considered&quot;: &quot;Not considered&quot;
            },
            &quot;Distributed teams&#39; issues&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;,
                &quot;Not applicable&quot;: &quot;Not applicable&quot;
            },
            &quot;Customer Communication&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            }
        },
        &#39;Effort estimate&#39;: {
            &quot;Estimated effort&quot;: {
                &quot;Estimate value(s)&quot;: &quot;Estimate value(s)&quot;
            },
            &quot;Actual effort&quot;: {
                &quot;Value&quot;: &quot;Value&quot;
            },
            &quot;Type&quot;: {
                &quot;Point&quot;: &quot;Point&quot;,
                &quot;Three point&quot;: &quot;Three point&quot;,
                &quot;Distribution&quot;: &quot;Distribution&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &quot;Unit&quot;: {
                &quot;House/days&quot;: &quot;House/days&quot;,
                &quot;Pair days&quot;: &quot;Pair/days&quot;,
                &quot;Ideal hours&quot;: &quot;Ideal hours&quot;,
                &quot;Other&quot;: &quot;Other&quot;
            },
            &quot;Accuracy Level&quot;: {
                &quot;Value&quot;: &quot;Value&quot;
            },
            &quot;Accuracy measure&quot;: {
                &quot;Mean Magnitude of Relative Error&quot;: &quot;Mean Magnitude of Relative Error&quot;,
                &quot;Median Magnitude of Relative Error&quot;: &quot;Median Magnitude of Relative Error&quot;,
                &quot;Bias of Relative Error&quot;: &quot;Bias of Relative Error&quot;,
                &quot;Other&quot;: &quot;Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;
            }
        }
    }
}

new_taxonomy = {
    &#39;Effort Estimation in ASD&#39;: {
        &#39;Estimation context&#39;: {
            &quot;Planning level&quot;: {
                &quot;Release&quot;: &quot;Release&quot;,
                &quot;Sprint&quot;: &quot;Sprint&quot;,
                &quot;Daily&quot;: &quot;Daily&quot;,
                &quot;Bidding&quot;: &quot;Bidding&quot;
            },
            &quot;Estimated activities&quot;: {
                &quot;Analysis&quot;: &quot;Analysis&quot;,
                &quot;Design&quot;: &quot;Design&quot;,
                &quot;Implementation&quot;: &quot;Implementation&quot;,
                &quot;Testing&quot;: &quot;Testing&quot;,
                &quot;Maintenance&quot;: &quot;Maintenance&quot;,
                &quot;Estimated activities.All&quot;: &quot;Estimated activities.All&quot;
            },
            &quot;Agile methods&quot;: {
                &quot;Extreme Programming&quot;: &quot;Extreme Programming&quot;,
                &quot;Scrum&quot;: &quot;Scrum&quot;,
                &quot;Customized Extreme Programming&quot;: &quot;Customized Extreme Programming&quot;,
                &quot;Customized Scrum&quot;: &quot;Customized Scrum&quot;,
                &quot;Dynamic Systems Development Method&quot;: &quot;Dynamic Systems Development Method&quot;,
                &quot;Crystal&quot;: &quot;Crystal&quot;,
                &quot;Feature-Driven Development&quot;: &quot;Feature-Driven Development&quot;,
                &quot;Kanban&quot;: &quot;Kanban&quot;
            },
            &quot;Project domain&quot;: {
                &quot;Communications industry&quot;: &quot;Communications industry&quot;,
                &quot;Transportation&quot;: &quot;Transportation&quot;,
                &quot;Financial&quot;: &quot;Financial&quot;,
                &quot;Education&quot;: &quot;Education&quot;,
                &quot;Health&quot;: &quot;Health&quot;,
                &quot;Retail/Wholesale&quot;: &quot;Retail/Wholesale&quot;,
                &quot;Manufacturing&quot;: &quot;Manufacturing&quot;,
                &quot;Government/Military&quot;: &quot;Government/Military&quot;,
                &quot;Project domain.Other&quot;: &quot;Project domain.Other&quot;
            },
            &quot;Project setting&quot;: {
                &quot;Co-located&quot;: &quot;Co-located&quot;,
                &quot;Distributed: Close Onshore&quot;: &quot;Distributed: Close Onshore&quot;,
                &quot;Distributed: Distant Onshore&quot;: &quot;Distributed: Distant Onshore&quot;,
                &quot;Distributed: Near Offshore&quot;: &quot;Distributed: Near Offshore&quot;,
                &quot;Distributed: Far Offshore&quot;: &quot;Distributed: Far Offshore&quot;
            },
            &quot;Estimation entity&quot;: {
                &quot;User story&quot;: &quot;User story&quot;,
                &quot;Task&quot;: &quot;Task&quot;,
                &quot;Use case&quot;: &quot;Use case&quot;,
                &quot;Estimation entity.Other&quot;: &quot;Estimation entity.Other&quot;
            },
            &quot;Number of entities estimated&quot;: {
                &quot;Number of entities estimated.Value&quot;: &quot;Number of entities estimated.Value&quot;
            },
            &quot;Team size&quot;: {
                &quot;No. of team members&quot;: &quot;No. of team members&quot;
            }
        },
        &#39;Estimation technique&#39;: {
            &quot;Estimation Techniques&quot;: {
                &quot;Planning Poker&quot;: &quot;Planning Poker&quot;,
                &quot;Expert Judgement&quot;: &quot;Expert Judgement&quot;,
                &quot;Analogy&quot;: &quot;Analogy&quot;,
                &quot;Use case points method&quot;: &quot;Use case points method&quot;,
                &quot;Estimation Techniques.Other&quot;: &quot;Estimation Techniques.Other&quot;
            },
            &quot;Type&quot;: {
                &quot;Single&quot;: &quot;Single&quot;,
                &quot;Group&quot;: &quot;Group&quot;
            }
        },
        &#39;Effort predictors&#39;: {
            &quot;Size&quot;: {
                &quot;Story points&quot;: &quot;Story points&quot;,
                &quot;User case points&quot;: &quot;User case points&quot;,
                &quot;Function points&quot;: &quot;Function points&quot;,
                &quot;Size.Other&quot;: &quot;Size.Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;,
                &quot;Considered without any metric&quot;: &quot;Considered without any metric&quot;
            },
            &quot;Team&#39;s prior experience&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Team&#39;s skill level&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Non functional requirements&quot;: {
                &quot;Performance&quot;: &quot;Performance&quot;,
                &quot;Security&quot;: &quot;Security&quot;,
                &quot;Availability&quot;: &quot;Availability&quot;,
                &quot;Reliability&quot;: &quot;Reliability&quot;,
                &quot;Maintainability&quot;: &quot;Maintainability&quot;,
                &quot;Non functional requirements.Other&quot;: &quot;Non functional requirements.Other&quot;,
                &quot;Not considered&quot;: &quot;Not considered&quot;
            },
            &quot;Distributed teams&#39; issues&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;,
                &quot;Not applicable&quot;: &quot;Not applicable&quot;
            },
            &quot;Customer Communication&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            }
        },
        &#39;Effort estimate&#39;: {
            &quot;Estimated effort&quot;: {
                &quot;Estimate value(s)&quot;: &quot;Estimate value(s)&quot;
            },
            &quot;Actual effort&quot;: {
                &quot;Actual effort.Value&quot;: &quot;Actual effort.Value&quot;
            },
            &quot;Effort estimate.Type&quot;: {
                &quot;Point&quot;: &quot;Point&quot;,
                &quot;Three point&quot;: &quot;Three point&quot;,
                &quot;Distribution&quot;: &quot;Distribution&quot;,
                &quot;Effort estimate.Type.Other&quot;: &quot;Effort estimate.Type.Other&quot;
            },
            &quot;Unit&quot;: {
                &quot;Hours/days&quot;: &quot;Hours/days&quot;,
                &quot;Pair days&quot;: &quot;Pair/days&quot;,
                &quot;Ideal hours&quot;: &quot;Ideal hours&quot;,
                &quot;Unit.Other&quot;: &quot;Unit.Other&quot;
            },
            &quot;Accuracy Level&quot;: {
                &quot;Accuracy Level.Value&quot;: &quot;Accuracy Level.Value&quot;
            },
            &quot;Accuracy measure&quot;: {
                &quot;Mean Magnitude of Relative Error&quot;: &quot;Mean Magnitude of Relative Error&quot;,
                &quot;Median Magnitude of Relative Error&quot;: &quot;Median Magnitude of Relative Error&quot;,
                &quot;Bias of Relative Error&quot;: &quot;Bias of Relative Error&quot;,
                &quot;Accuracy measure.Other&quot;: &quot;Accuracy measure.Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;
            }
        }
    }
}


leaves = get_leaf_nodes(new_taxonomy)
print(leaves)

ncat = extract_ncat(new_taxonomy)
nchar = extract_nchar(new_taxonomy)
depths_cat = extract_depths_cat(new_taxonomy)
depths_char = extract_depths_char(new_taxonomy)

print(&quot;Number of categories (ncat):&quot;, ncat)
print(&quot;Number of characteristics (nchar):&quot;, nchar)
print(&quot;Depths of categories:&quot;, depths_cat)
print(&quot;Depths of characteristics:&quot;, depths_char)

robustness_value = calculate_r_t(new_taxonomy)
print(f&quot;Robustness R(T): {robustness_value:.4f}&quot;)
conciseness= calculate_conciseness(ncat, nchar, depths_cat, depths_char)
print(f&#39;The conciseness of the taxonomy is: {conciseness}&#39;)</code></pre>
<pre class="python"><code>import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting
from transformers import AutoTokenizer, AutoModel
import torch
import matplotlib
plt.clf()

plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)  # You can change this to any available style

plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;


# Step 1: Define the sets
# Combine the sets into a dictionary
Bajta = {&#39;Conceptualization&#39;, &#39;Feasibility study&#39;, &#39;Preliminary planning&#39;, &#39;Detail Planning&#39;, &#39;Execution&#39;, &#39;Commissioning&#39;, &#39;System investigation&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Other&#39;, &#39;SE&#39;, &#39;Telecommunication&#39;, &#39;Finance&#39;, &#39;Healthcare&#39;, &#39;Other&#39;, &#39;Close onshore&#39;, &#39;Distant onshore&#39;, &#39;Near offshore&#39;, &#39;Far offshore&#39;, &#39;Constructive Cost Model&#39;, &#39;Capability Maturity Model Integration&#39;, &#39;Agile&#39;, &#39;Delphi&#39;, &#39;GA&#39;, &#39;CBR&#39;, &#39;Fuzzy similar&#39;, &#39;Other&#39;, &#39;Value&#39;, &#39;No of team members&#39;, &#39;Expert judgment&#39;, &#39;Machine learning&#39;, &#39;Non-machine learning&#39;, &#39;Individual&#39;, &#39;Group-based estimation&#39;, &#39;Estimate value&#39;, &#39;Value&#39;, &#39;Effort hours&#39;, &#39;Staff/cost&#39;, &#39;Hardware&#39;, &#39;Risk&#39;, &#39;Portfolio&#39;, &#39;Baseline comparison&#39;, &#39;Variation reduction&#39;, &#39;Sensitivity analysis&#39;, &#39;Size report&#39;, &#39;Statistics analysis&#39;, &#39;Considered&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Other&#39;, &#39;Geographical distance&#39;, &#39;Temporal distance&#39;, &#39;Socio-cultural distance&#39;}
Britto_2017 = {&#39;Web page count&#39;, &#39;Media count&#39;, &#39;New media count&#39;, &#39;New Web page count&#39;, &#39;Link count&#39;, &#39;Program count&#39;, &#39;Reused component count&#39;, &#39;Lines of code&#39;, &#39;Reused program count&#39;, &#39;Reused media count&#39;, &#39;Web page allocation&#39;, &#39;Reused lines of code&#39;, &#39;Media allocation&#39;, &#39;Reused media allocation&#39;, &#39;Entity count&#39;, &#39;Attribute count&#39;, &#39;Component count&#39;, &#39;Statement count&#39;, &#39;Node count&#39;, &#39;Collection slot size&#39;, &#39;Component granularity level&#39;, &#39;Slot granularity level&#39;, &#39;Model node size&#39;, &#39;Cluster node size&#39;, &#39;Node slot size&#39;, &#39;Publishing model unit count&#39;, &#39;Model slot size&#39;, &#39;Association slot size&#39;, &#39;Client script count&#39;, &#39;Server script count&#39;, &#39;Information slot count&#39;, &#39;Association center slot count&#39;, &#39;Collection center slot count&#39;, &#39;Component slot count&#39;, &#39;Semantic association count&#39;, &#39;Segment count&#39;, &#39;Slot count&#39;, &#39;Cluster slot count&#39;, &#39;Cluster count&#39;, &#39;Publishing unit count&#39;, &#39;Section count&#39;, &#39;Inner/sub concern count&#39;, &#39;Indifferent concern count&#39;, &#39;Module point cut count&#39;, &#39;Module count&#39;, &#39;Module attribute count&#39;, &#39;Operation count&#39;, &#39;Comment count&#39;, &#39;Reused comment count&#39;, &#39;Media duration&#39;, &#39;Diffusion cut count&#39;, &#39;Concern module count&#39;, &#39;Concern operation count&#39;, &#39;Anchor count&#39;, &#39;High feature count&#39;, &#39;Low feature count&#39;, &#39;Reused high feature count&#39;, &#39;Reused low feature count&#39;, &#39;Web objects&#39;, &#39;Common Software Measurement International Consortium&#39;, &#39;International Function Point Users Group&#39;, &#39;Object-Oriented Heuristic Function Points&#39;, &#39;Object-Oriented Function Points&#39;, &#39;Use case count&#39;, &#39;Feature count&#39;, &#39;Data Web points&#39;, &#39;Cohesion&#39;, &#39;Class coupling&#39;, &#39;Concern coupling&#39;, &#39;Connectivity density&#39;, &#39;Cyclomatic complexity&#39;, &#39;Model collection complexity&#39;, &#39;Model association complexity&#39;, &#39;Model link complexity&#39;, &#39;Page complexity&#39;, &#39;Component complexity&#39;, &#39;Total complexity&#39;, &#39;Adaptation complexity&#39;, &#39;New complexity&#39;, &#39;Data usage complexity&#39;, &#39;Data flow complexity&#39;, &#39;Cohesion complexity&#39;, &#39;Interface complexity&#39;, &#39;Control flow complexity&#39;, &#39;Class complexity&#39;, &#39;Layout complexity&#39;, &#39;Input complexity&#39;, &#39;Output complexity&#39;, &#39;Product.Type&#39;, &#39;Stratum&#39;, &#39;Compactness&#39;, &#39;Product.Structure&#39;, &#39;Architecture&#39;, &#39;Integration with legacy systems&#39;, &#39;Concurrency level&#39;, &#39;Processing requirements&#39;, &#39;Database size&#39;, &#39;Requirements volatility level&#39;, &#39;Requirements novelty level&#39;, &#39;Reliability level&#39;, &#39;Maintainability level&#39;, &#39;Time efficiency level&#39;, &#39;Memory efficiency level&#39;, &#39;Portability level&#39;, &#39;Scalability level&#39;, &#39;Quality level&#39;, &#39;Usability level&#39;, &#39;Readability level&#39;, &#39;Security level&#39;, &#39;Installability level&#39;, &#39;Modularity level&#39;, &#39;Flexibility level&#39;, &#39;Testability level&#39;, &#39;Accessibility level&#39;, &#39;Trainability level&#39;, &#39;Innovation level&#39;, &#39;Technical factors&#39;, &#39;Storage constraint&#39;, &#39;Reusability level&#39;, &#39;Robustness level&#39;, &#39;Design volatility&#39;, &#39;Product.Experience level&#39;, &#39;Requirements clarity level&#39;, &#39;Availability level&#39;, &#39;IT literacy&#39;, &#39;Mapped workflows&#39;, &#39;Personality&#39;, &#39;SPI program&#39;, &#39;Metrics program&#39;, &#39;Number of projects in parallel&#39;, &#39;Software reuse&#39;, &#39;Documentation level&#39;, &#39;Number of programming languages&#39;, &#39;Project.Type&#39;, &#39;Process efficiency level&#39;, &#39;Project management level&#39;, &#39;Project.Infrastructure&#39;, &#39;Development restriction&#39;, &#39;Time restriction&#39;, &#39;Risk level&#39;, &#39;Rapid app development&#39;, &#39;Operational mode&#39;, &#39;Resource level&#39;, &#39;Lessons learned repository&#39;, &#39;Domain experience level&#39;, &#39;Team size&#39;, &#39;Deployment platform experience level&#39;, &#39;Team capability&#39;, &#39;Programming language experience level&#39;, &#39;Tool experience level&#39;, &#39;Communication level&#39;, &#39;Software development experience&#39;, &#39;Work Team level&#39;, &#39;Stability level&#39;, &#39;Motivation level&#39;, &#39;Focus factor&#39;, &#39;OO experience level&#39;, &#39;In-house experience&#39;, &#39;Authoring tool type&#39;, &#39;Productivity level&#39;, &#39;Novelty level&#39;, &#39;Platform volatility level&#39;, &#39;Difficulty level&#39;, &#39;Platform support level&#39;}
Britto_2016 = {&#39;Site.Location&#39;, &#39;Site.Legal Entity&#39;, &#39;Site.Geographic Distance&#39;, &#39;Site.Temporal Distance&#39;, &#39;Early&#39;, &#39;Early &amp; Late&#39;, &#39;Late&#39;, &#39;Estimator&#39;, &#39;Estimator &amp; Provider&#39;, &#39;Provider&#39;, &#39;Relationship.Location&#39;, &#39;Relationship.Legal Entity&#39;, &#39;Relationship.Geographic Distance&#39;, &#39;Relationship.Temporal Distance&#39;, &#39;Centralized&#39;, &#39;Distributed&#39;, &#39;Semi-distributed&#39;}
Dasthi = {&#39;Constructive Cost Model&#39;, &#39;Software Life Cycle Management&#39;, &#39;Software Evaluation and Estimation for Risk&#39;, &#39;Expert Judgment&#39;, &#39;Analogy-Based&#39;, &#39;Basic-Combination&#39;, &#39;Fuzzy Logic&#39;, &#39;Artificial Neural Networks&#39;, &#39;Computational Intelligence.swarm&#39;, &#39;Computational Intelligence.evolutionary&#39;, &#39;AI-Combined hybrid&#39;}
Mendes = {&#39;Motivation&#39;, &#39;Early size metric&#39;, &#39;Late size metric&#39;, &#39;Problem-oriented metric&#39;, &#39;Solution-oriented metric&#39;, &#39;Class.Length&#39;, &#39;Functionality&#39;, &#39;Complexity&#39;, &#39;Web hypermedia application&#39;, &#39;Web software application&#39;, &#39;Web application&#39;, &#39;Media&#39;, &#39;Program/Script&#39;, &#39;Nominal&#39;, &#39;Ordinal&#39;, &#39;Interval&#39;, &#39;Ratio&#39;, &#39;Absolute&#39;, &#39;Computation.Direct&#39;, &#39;Computation.Indirect&#39;, &#39;Validated Empirically&#39;, &#39;Validated Theoretically&#39;, &#39;Validation.Both&#39;, &#39;Validation.None&#39;, &#39;Model dependency.Specific&#39;, &#39;Model dependency.Nonspecific&#39;}
Usman = {&#39;Release&#39;, &#39;Sprint&#39;, &#39;Daily&#39;, &#39;Bidding&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Estimated activities.All&#39;, &#39;Extreme Programming&#39;, &#39;Scrum&#39;, &#39;Customized Extreme Programming&#39;, &#39;Customized Scrum&#39;, &#39;Dynamic Systems Development Method&#39;, &#39;Crystal&#39;, &#39;Feature-Driven Development&#39;, &#39;Kanban&#39;, &#39;Communications industry&#39;, &#39;Transportation&#39;, &#39;Financial&#39;, &#39;Education&#39;, &#39;Health&#39;, &#39;Retail/Wholesale&#39;, &#39;Manufacturing&#39;, &#39;Government/Military&#39;, &#39;Project domain.Other&#39;, &#39;Co-located&#39;, &#39;Distributed: Close Onshore&#39;, &#39;Distributed: Distant Onshore&#39;, &#39;Distributed: Near Offshore&#39;, &#39;Distributed: Far Offshore&#39;, &#39;User story&#39;, &#39;Task&#39;, &#39;Use case&#39;, &#39;Estimation entity.Other&#39;, &#39;Number of entities estimated.Value&#39;, &#39;No. of team members&#39;, &#39;Planning Poker&#39;, &#39;Expert Judgement&#39;, &#39;Analogy&#39;, &#39;Use case points method&#39;, &#39;Estimation Techniques.Other&#39;, &#39;Single&#39;, &#39;Group&#39;, &#39;Story points&#39;, &#39;User case points&#39;, &#39;Function points&#39;, &#39;Size.Other&#39;, &#39;Not used&#39;, &#39;Considered without any metric&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Non functional requirements.Other&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Not applicable&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Estimate value(s)&#39;, &#39;Actual effort.Value&#39;, &#39;Point&#39;, &#39;Three point&#39;, &#39;Distribution&#39;, &#39;Effort estimate.Type.Other&#39;, &#39;Hours/days&#39;, &#39;Pair days&#39;, &#39;Ideal hours&#39;, &#39;Unit.Other&#39;, &#39;Accuracy Level.Value&#39;, &#39;Mean Magnitude of Relative Error&#39;, &#39;Median Magnitude of Relative Error&#39;, &#39;Bias of Relative Error&#39;, &#39;Accuracy measure.Other&#39;, &#39;Not used&#39;}

sets = {
    &#39;Bajta&#39;: Bajta,
    &#39;Britto_2017&#39;: Britto_2017,
    &#39;Britto_2016&#39;: Britto_2016,
    &#39;Dasthi&#39;: Dasthi,
    &#39;Mendes&#39;: Mendes,
    &#39;Usman&#39;: Usman
}

# Step 2: Flatten the sets into a dataframe (assuming &#39;sets&#39; is already defined)
words = []
labels = []
for label, words_set in sets.items():
    for word in words_set:
        words.append(word)
        labels.append(label)

# Create a dataframe
df = pd.DataFrame({&#39;Word&#39;: words, &#39;Set&#39;: labels})

# Step 3: Load the pre-trained model and tokenizer
model_name = &quot;jinaai/jina-embeddings-v3&quot;
if &#39;model&#39; not in locals() or &#39;tokenizer&#39; not in locals():
    print(&quot;Loading model and tokenizer...&quot;)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
else:
    print(&quot;Model and tokenizer are already loaded.&quot;)</code></pre>
<pre><code>Loading model and tokenizer...</code></pre>
<pre class="python"><code># Step 4: Get the embeddings for each word
def get_embeddings(word):
    inputs = tokenizer(word, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

embeddings = np.array([get_embeddings(word) for word in df[&#39;Word&#39;]])

# Step 5: Perform t-SNE (now in 2D)
tsne = TSNE(n_components=2, perplexity=30, random_state=5)
embeddings_2d = tsne.fit_transform(embeddings)</code></pre>
<pre><code>C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores &lt; 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
  File &quot;C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\joblib\externals\loky\backend\context.py&quot;, line 282, in _count_physical_cores
    raise ValueError(f&quot;found {cpu_count_physical} physical cores &lt; 1&quot;)</code></pre>
<pre class="python"><code># Step 6: Convert string labels to numeric labels for coloring
label_encoder = LabelEncoder()
numeric_labels = label_encoder.fit_transform(labels)

# Step 7: Create the 2D scatter plot
fig, ax = plt.subplots(figsize=(10, 7))

# Plot the 2D scatter with the numeric labels for colors
scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                     c=numeric_labels, cmap=&#39;Set1&#39;, s=100)

# Annotate each point with the word
for i, word in enumerate(df[&#39;Word&#39;]):
    ax.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, word, fontsize=9)

# Step 8: Add labels and title
ax.set_title(&quot;2D t-SNE Visualization of Word Embeddings&quot;)
ax.set_xlabel(&quot;t-SNE Dimension 1&quot;)
ax.set_ylabel(&quot;t-SNE Dimension 2&quot;)

# Step 9: Move the legend outside of the plot
legend_labels = label_encoder.classes_
handles = [plt.Line2D([0], [0], marker=&#39;o&#39;, color=&#39;w&#39;, 
                      markerfacecolor=plt.cm.Set2(i / len(legend_labels)), markersize=5) 
           for i in range(len(legend_labels))]
ax.legend(handles, legend_labels, title=&quot;Set&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)

# Step 10: Show the plot
plt.tight_layout()  # Ensures proper spacing with the legend outside
plt.savefig(&#39;word_embeddings.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-8-2.png" style="display: block; margin: auto;" /></p>
<p>#3D PLOT</p>
<pre class="python"><code>import plotly.express as px
import pandas as pd
import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer
import umap.umap_ as umap
from sklearn.preprocessing import LabelEncoder
plt.clf()

plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)  # You can change this to any available style

plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;
# Step 2: Flatten the sets into a dataframe (assuming sets is already defined)
words = []
labels = []
for label, words_set in sets.items():
    for word in words_set:
        words.append(word)
        labels.append(label)

# Create a dataframe
df = pd.DataFrame({&#39;Word&#39;: words, &#39;Set&#39;: labels})

# Step 3: Load the pre-trained model and tokenizer
model_name = &quot;jinaai/jina-embeddings-v3&quot;
if &#39;model&#39; not in locals() or &#39;tokenizer&#39; not in locals():
    print(&quot;Loading model and tokenizer...&quot;)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
else:
    print(&quot;Model and tokenizer are already loaded.&quot;)</code></pre>
<pre><code>Model and tokenizer are already loaded.</code></pre>
<pre class="python"><code># Step 4: Get the embeddings for each word
def get_embeddings(word):
    inputs = tokenizer(word, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

embeddings = np.array([get_embeddings(word) for word in df[&#39;Word&#39;]])

# Step 5: Perform 3D UMAP (with 3 components)
umap_model = umap.UMAP(n_components=3, random_state=5)
embeddings_3d = umap_model.fit_transform(embeddings)</code></pre>
<pre><code>C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\sklearn\utils\deprecation.py:151: FutureWarning:

&#39;force_all_finite&#39; was renamed to &#39;ensure_all_finite&#39; in 1.6 and will be removed in 1.8.

C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\umap\umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.</code></pre>
<pre class="python"><code># Step 6: Convert string labels to numeric labels for coloring
label_encoder = LabelEncoder()
numeric_labels = label_encoder.fit_transform(labels)

# Step 7: Create the interactive 3D plot with Plotly
fig = px.scatter_3d(df, x=embeddings_3d[:, 0], y=embeddings_3d[:, 1], z=embeddings_3d[:, 2],
                    color=labels, text=words,
                    labels={&#39;x&#39;: &#39;UMAP Dimension 1&#39;, &#39;y&#39;: &#39;UMAP Dimension 2&#39;, &#39;z&#39;: &#39;UMAP Dimension 3&#39;},
                    title=&quot;3D UMAP Visualization of Word Embeddings&quot;)

# Customize the layout for better viewing
fig.update_traces(marker=dict(size=5, opacity=0.8), selector=dict(mode=&#39;markers+text&#39;))</code></pre>
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="4cc985d6-0b65-4b2b-b78e-ba0332367d79" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("4cc985d6-0b65-4b2b-b78e-ba0332367d79")) {                    Plotly.newPlot(                        "4cc985d6-0b65-4b2b-b78e-ba0332367d79",                        [{"hovertemplate":"color=Bajta\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Bajta","marker":{"color":"#636efa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Bajta","scene":"scene","showlegend":true,"text":["Baseline comparison","System investigation","Considered","Not Considered","Size report","Temporal distance","Delphi","Analysis","Maintenance","Statistics analysis","Near offshore","Fuzzy similar","Agile","Group-based estimation","Execution","GA","Variation reduction","Conceptualization","CBR","Finance","Commissioning","Effort hours","Performance","Machine learning","Sensitivity analysis","Estimate value","Preliminary planning","Feasibility study","Implementation","Capability Maturity Model Integration","Close onshore","No of team members","Expert judgment","Non-machine learning","SE","Portfolio","Not considered","Telecommunication","Hardware","Staff\u002fcost","Other","Socio-cultural distance","Security","Individual","Availability","Detail Planning","Testing","Healthcare","Value","Geographical distance","Design","Reliability","Constructive Cost Model","Far offshore","Distant onshore","Risk","Maintainability"],"x":{"dtype":"f4","bdata":"yFIgQVgPIEG+FSlBvsEqQcF09EA+bghBH\u002fIgQbLQJEH9AExB7q0mQdk+FkEj0hxBfn0bQS+CKUH4PBtBtjUrQeLDK0FdkCZB58kmQRGcLkGJlTNBB8sgQZs1I0EzIhZBQ\u002fYtQTrIKkFsBShB2qoeQTHDG0GnuDRB8ZQWQZdPIEEyBCdBKqwWQZtrLEFxHSdBgd0qQaMPNUHqcixBLS4jQdacJ0FfaAhBbJY8QVQ5JUG17kNBB1QoQaHaLEE2XjZBoSsqQQ7aCEHPRytBbb5HQRUUJEETZRVBmacVQdTTPUG9u0lB"},"y":{"dtype":"f4","bdata":"5HKgvwAT2r+QlbK\u002fcTO2v08kpL7z9hU\u002f2KZ\u002fvzWO0r+81Ba\u002frzbYv6H4az7dHeW\u002fLTORvn4Foz+fFVG\u002f8jxZv1aN5L+gh8C\u002fmXNOv1lkLb\u002fuExe\u002fW3kUP0Gc7758Nf+\u002fzSXav8NDkD8AJoe\u002fJ0npv7+Sk7\u002fThcm+UgtcPhVlKz9+6uW\u002flbL5v1XxQb\u002fzEBa\u002fWmy9v1jSIb9b25+\u002fZ6FqPxuLEb9ljRk\u002f\u002fopYv6dhxb5ziQy\u002fPF2Cv0gT6r+p8zu\u002fw6wlP+CHDj+NmmO\u002fqoAhv3iPZj+3vZI+pPmCPvqIUL+Czju\u002f"},"z":{"dtype":"f4","bdata":"FDV3QBNUnkDDtmZAjZxPQCKxskBqxDdAaM+gQElojUBmQMxAjtONQBWa7j86EIJAQjSOQDiMhkAtPZtAvKdrQFT1lEDaVoBA3ChoQHdTh0AaUpBAoDmNQG29l0DXv3VAdOSWQAcpgECSRZdAXvOiQEFtpUBEF85AB6rpPxdxtEB7yl1ARxxvQOAvcEB9GpBAtq9bQPc1kkDxA6lArdWWQLfKcEBb6C5AmnaRQJYCcEDHQshAZMmbQBmYdkDCOoRA3CqDQMJ8LkCEZaFAhzPFQL5qlEDwDe8\u002fEf\u002fsP74xnEBxs8pA"},"type":"scatter3d"},{"hovertemplate":"color=Britto_2017\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2017","marker":{"color":"#EF553B","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2017","scene":"scene","showlegend":true,"text":["Interface complexity","Innovation level","Cluster slot count","Cluster count","Component count","Integration with legacy systems","Accessibility level","Time restriction","Cyclomatic complexity","Model association complexity","Web page count","Data flow complexity","Module attribute count","Reused lines of code","Rapid app development","Component granularity level","Association center slot count","Metrics program","Object-Oriented Heuristic Function Points","Development restriction","Entity count","International Function Point Users Group","Security level","Section count","Flexibility level","Slot count","Publishing unit count","Model link complexity","Node count","Association slot size","Personality","Software development experience","Risk level","Media allocation","Motivation level","Statement count","Model node size","Web page allocation","Requirements volatility level","Operational mode","Testability level","Work Team level","Authoring tool type","Attribute count","Reused high feature count","Reused media allocation","Platform support level","Product.Structure","Operation count","Use case count","Model collection complexity","New media count","Productivity level","Storage constraint","In-house experience","Diffusion cut count","Collection center slot count","Object-Oriented Function Points","Number of projects in parallel","Model slot size","Communication level","Information slot count","Reliability level","Domain experience level","Link count","High feature count","Project management level","Reused comment count","Maintainability level","Concern coupling","Tool experience level","Program count","Media duration","Client script count","Control flow complexity","Team capability","Server script count","Low feature count","Readability level","Node slot size","Concurrency level","Data Web points","Data usage complexity","Novelty level","Software reuse","Collection slot size","Requirements novelty level","Requirements clarity level","Trainability level","Process efficiency level","New complexity","Robustness level","Concern operation count","Class coupling","Module count","Team size","Reused component count","Focus factor","Module point cut count","Project.Infrastructure","Feature count","Product.Experience level","Scalability level","Cohesion complexity","Design volatility","Availability level","Output complexity","Web objects","Product.Type","Project.Type","Total complexity","Inner\u002fsub concern count","Platform volatility level","Component slot count","Page complexity","IT literacy","Stability level","Database size","Concern module count","Input complexity","Layout complexity","Anchor count","Installability level","Usability level","Technical factors","Processing requirements","Component complexity","Connectivity density","Resource level","Cohesion","Lines of code","Modularity level","Comment count","Time efficiency level","SPI program","Reusability level","Reused low feature count","Difficulty level","Cluster node size","Architecture","Lessons learned repository","Reused program count","Publishing model unit count","Common Software Measurement International Consortium","Slot granularity level","OO experience level","Memory efficiency level","Segment count","Reused media count","Programming language experience level","Compactness","New Web page count","Semantic association count","Portability level","Number of programming languages","Quality level","Adaptation complexity","Mapped workflows","Documentation level","Media count","Class complexity","Stratum","Indifferent concern count","Deployment platform experience level"],"x":{"dtype":"f4","bdata":"VinCQCCHMEFRHeZAoO3wQNGt+kDwBh9BuoI+QWEuFkHnlsdAt9rMQBEx+0CT9cRAtMb7QFRbDkE37Q9B+f3mQCnN4UCYjARByygKQccZKEFPP\u002f1A+oQMQRugQEGYLPhACig\u002fQWic50AzZv9Av7HPQOwo8kB13+NAVVIlQQbRHkEEmkBB+CQGQciDKUH4vwNBZ47hQO5d+0AkXDlBlNsZQcWpQEFSkSNBD8UpQREc\u002f0BtuwZBQVMFQQuNOEH0SCpBMqMGQemTCkFZLMlAvSoDQcVCKkEtyglBZBUeQbST9kDap+JAjMEKQcfcGEFNM+JAylI5QZhr5UCMKUVBfOgqQVWO9EApRgRBadwhQQw9BEHYjURByVUEQTF7KUE8bgxBlv0GQZO9B0F6F8NAD3ciQRgUB0G1GARB7Fc+Qf9w4EDb\u002fDZBfn8LQQz8w0D6tjFBn64UQXPZ4UBC2zJBUgkzQRXSQEHN7ClBlrTLQIPiQkF0KAdBlD\u002fYQIuI+kBnliBB2w0DQbfDEEEgFvhAyiYhQeW7A0EdbitBj306Qe4w00AmKDlB+Ns\u002fQZpAw0D6lwpB74MqQbIKI0GALshAvLQDQVu5PEHbaedALSDJQGL4NkF2F0RBl9vxQLImAEG77cJA4IDBQD599kDxfjxBLYI7QTsbLEHL\u002fSpB94nMQHiY4kCniDxB4VzkQNZ1D0ETpvdA3cEBQYrtJ0HA2BVBokc4QeWzBkG55DxBmITjQLbJKUFm2RxB4vQLQW5j+UCW6g5BGcjeQJO+KUHv2SlBAev1QGWyA0GZPSdBLtLXQPFe+kAknexAAME7QSnuDUHP6z5BNCvKQNe4GkES\u002fDFBpM0BQa\u002f0zUA26B5BPYwFQQn5KkE="},"y":{"dtype":"f4","bdata":"vhVGvwbxCb9svdQ+8MmFPrpZDD08kL+\u002fP6I0v0P0Wj6lLme\u002fDIq6vvffs74WUTO\u002fVXT0Ps8sP7+w3rm\u002ftVKJvgqn+T7yAky\u002fxtDav7aapr\u002fkIDQ+bA\u002fhv9WkVL8EWq4++z4lv47iDD8PdIw+xEDAvlc+QT6\u002faBQ\u002fx9wIv0Pfpb+NvU6\u002fQCm2PkL5lj29iuA9qHiqPk6JGb\u002fnvpq\u002fhnBdv7hFb7+L5Ns+Aj6Nv7BHjT3fzqO+lDmjPsGpbL\u002f9akG\u002fxToqPrZEO79M4+2+YqCtPp7yNz6kImg+4Eq3v2kK1T71Pvo+Flzev9YRZL5ZS\u002fo+Zg7ivsY3Dj+Kbiq\u002fkz2CvzvV\u002fL2ibES+\u002fCanvvQiK74FsTG\u002fKygYPyaNjL9wSai+Kt7lPttapL5fSku\u002fG+8rP3tLpr7KERq+OQtOv8vPBD8wa0i+\u002fFvVvzi1PL9WdkO\u002fOFV2v2XeBD+xcZW\u002fR6Kjv+cRUL8ZUYE+1fxUv0EbSr\u002fKz9s+OlCpvs0IFj\u002fZdzc\u002ftd1nvly537\u002fUARo\u002fUrc3vzi3zL0R74G\u002f2mHRvjclL7+bnaC\u002fCI\u002f2vlg5U79cE7i\u002f2vg9v7XTCL85RE+\u002fTCEAP511hr\u002fnzPk+ZoGEvwaegr+lzSi\u002fFh9JvvBmMj8umkq\u002f4xKCv9IGC7xC70q\u002fDZdUvwgCs79ii62\u002f9LJHvx4w5L5zzvu+2W44vyB3O79dRDA\u002fcF9NPZVTtT4Pml2\u002fV0Yiv9b8lb4DlU6\u002fHKjOPtrvY788ssS\u002fVEXnvh+5mz5Wn2q\u002fY2SIPkk1h78eL6A+zZFqPqhjqz4e5Ye\u002fn9REv4kZmr57hZK79Nsvv49Xob7kjwy\u002fxNVDv9gIl7+C\u002f5G\u002fj6r4PtfwP78WVrK+9bUlPw9ReL8="},"z":{"dtype":"f4","bdata":"B47ZQFwxukDwSfdAkKHvQG9750ASDqxA+tvSQHUShECrzd9AYwveQI+U0EA4h9lAxkfmQI4I0EBlyr9A+aPvQAIp+UD2O6lAnpqpQAnJrUAmANRAyAaoQEZPokCgrOFApmDJQFem+EDdoMVAz7rZQILw4kC+CvZAAW2FQA0lykAn\u002fqhAo6WrQFRrpEAoT9RArbDrQB76ykCgkbtATFWdQAVqz0CTJbJAZ6q0QGBE5ED2IuZAavCuQLDNyEDX2ahAjD\u002fPQFoq0EA5BNxAQ\u002fO0QHxVp0DwJKdAm1vFQPlM5EBiT\u002ftAXXepQPT6wkC2ovZA41GrQENf\u002fECsxb9A\u002fRnMQHvD2kAwzuxA1d63QKKD2UCPZMtAztXRQIM2ykC9YstAmzGpQCBRzkBW7NtAnZG2QN9kzkCWne5AyJjVQALE9EA5qshA1Gu2QFA43EAPHr5ABaPPQOps\u002fEBwp71APtbAQDcd0kBlradAmojbQPApvUByAtFATfXPQAe25UBm8bNAAKzhQLLOnkDDNuZA3CWxQBqB60C1V8dASb3NQPbT2UBELLVAjMPIQPHe3kAfrr9AkEStQOZ2sEAeBt1AIhvWQFm1vUBpMfxACR7dQPdQlkD6ZLpAwRa6QKhG30Bhmt9AOkPdQFt030Cuf9JAvwXZQMpCqECXsa5Aej\u002fiQOgk20AmjcdAxnbRQNQTxECZOetAp8\u002fVQItGokCnGatA4\u002frVQC5350D1e9tATr3vQMG2pEC0y71Aie7SQAnoykDv+LJA5Hb2QJklzECwOKtAoaffQB1stkDFtM1ALMLFQLIQ0ECcyeFAhcDQQE9uzkDuebdAu1\u002fZQOFyqEBD5shAuF6xQOI320ByBoVA13bTQKjAzkA="},"type":"scatter3d"},{"hovertemplate":"color=Britto_2016\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2016","marker":{"color":"#00cc96","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2016","scene":"scene","showlegend":true,"text":["Relationship.Temporal Distance","Centralized","Estimator & Provider","Site.Legal Entity","Early","Relationship.Geographic Distance","Site.Location","Site.Temporal Distance","Provider","Relationship.Location","Late","Early & Late","Distributed","Estimator","Relationship.Legal Entity","Site.Geographic Distance","Semi-distributed"],"x":{"dtype":"f4","bdata":"yNoJQXaIGEGNuShBc+gQQc0oHEFhGAlBefAMQXRcCkE5oydBhdUNQd2BG0Ff6hpBqv0bQWrzKkGXexFBdXoKQXg8GkE="},"y":{"dtype":"f4","bdata":"fh4GP7iHO72etqA\u002fvH6tPt8PEb92yRU\u002f6emFPt2T9j40Z0g+epqnPjjfF7+JsP6+1Z+6vMSDqz94e8c+0UDsPlml+Lw="},"z":{"dtype":"f4","bdata":"kP8zQEx9L0ClP4BA1sBSQN6WakCIQzBA5\u002f4+QE7lN0D4Ol5AXfM2QIvZZEBXoWlA4eksQMmIekBf21FAOzE2QPemIEA="},"type":"scatter3d"},{"hovertemplate":"color=Dasthi\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Dasthi","marker":{"color":"#ab63fa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Dasthi","scene":"scene","showlegend":true,"text":["AI-Combined hybrid","Basic-Combination","Software Evaluation and Estimation for Risk","Computational Intelligence.swarm","Constructive Cost Model","Computational Intelligence.evolutionary","Fuzzy Logic","Analogy-Based","Software Life Cycle Management","Artificial Neural Networks","Expert Judgment"],"x":{"dtype":"f4","bdata":"CSIVQXn+HEH\u002f2j5B\u002f8QTQYDrI0G\u002fVxRBSScfQX1iH0EjxRpBkfEVQWOMJUE="},"y":{"dtype":"f4","bdata":"Fgn4v9Kfjr8SgTS\u002fscD5vy9\u002fej845gLAsxf6v7YZz786MZG\u002fxZ0BwNAS8L8="},"z":{"dtype":"f4","bdata":"4wxyQIwGbkCXQKRAnJByQNOZlUDlkHlAuZ1\u002fQDdGikAAyclAibV3QCnpYUA="},"type":"scatter3d"},{"hovertemplate":"color=Mendes\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Mendes","marker":{"color":"#FFA15A","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Mendes","scene":"scene","showlegend":true,"text":["Nominal","Interval","Validation.None","Web hypermedia application","Program\u002fScript","Functionality","Web software application","Computation.Direct","Model dependency.Nonspecific","Model dependency.Specific","Validated Theoretically","Validation.Both","Complexity","Late size metric","Problem-oriented metric","Early size metric","Ratio","Validated Empirically","Ordinal","Solution-oriented metric","Absolute","Motivation","Class.Length","Media","Computation.Indirect","Web application"],"x":{"dtype":"f4","bdata":"978oQbZeGkFpaytB\u002fMILQVTeDEGwVBxB6P4MQUXqFUEm7dNAJYvUQCecJUF8\u002fSlB3DbMQPc4+kCN0gBB6g77QOzjHkFZ+CZB2XAmQbaQAUGWrihB3g4nQf9N3kDLYwlBB64WQW6HDEE="},"y":{"dtype":"f4","bdata":"wcRzv8+3p75OdNm\u002fnCizv2A\u002fEr9Jwaq\u002fe0G2v10Ior9YJsq7aMcNPcNO9r\u002fG8+S\u002fJMhwv042Kr8sEWm\u002fbRkyvxypMr7LGeO\u002fpZpHv3xBab\u002fKWam\u002fFeOxvY5eoL7c38Q+VPSnvyAXuL8="},"z":{"dtype":"f4","bdata":"qTRQQORoXkBe7mNAE8nBQF7owkA8SptArBXCQETolEChTNVAK7DWQJBMekCas2lA4BrbQNlIpUD3ZaRAy0+mQHJrTkD5IYBAtQNZQKC7pEAIIF5AcxKZQPBE0kBdLaVAJ\u002fqSQKibwUA="},"type":"scatter3d"},{"hovertemplate":"color=Usman\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Usman","marker":{"color":"#19d3f3","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Usman","scene":"scene","showlegend":true,"text":["Project domain.Other","User case points","Considered","Pair days","Not Considered","Considered without any metric","Median Magnitude of Relative Error","Customized Extreme Programming","Health","Mean Magnitude of Relative Error","Maintenance","Daily","Release","Extreme Programming","No. of team members","Estimation Techniques.Other","Implementation","Hours\u002fdays","Government\u002fMilitary","Expert Judgement","Use case","Retail\u002fWholesale","Crystal","Accuracy measure.Other","Communications industry","Distributed: Close Onshore","Security","Availability","Distributed: Far Offshore","Financial","Three point","Analogy","Design","User story","Use case points method","Distributed: Near Offshore","Estimate value(s)","Accuracy Level.Value","Dynamic Systems Development Method","Transportation","Ideal hours","Not used","Distributed: Distant Onshore","Task","Estimated activities.All","Kanban","Analysis","Customized Scrum","Distribution","Non functional requirements.Other","Feature-Driven Development","Estimation entity.Other","Manufacturing","Point","Bias of Relative Error","Education","Story points","Bidding","Reliability","Performance","Actual effort.Value","Not considered","Single","Group","Sprint","Not applicable","Number of entities estimated.Value","Co-located","Size.Other","Testing","Scrum","Planning Poker","Function points","Unit.Other","Maintainability","Effort estimate.Type.Other"],"x":{"dtype":"f4","bdata":"gD8hQW3SEkFabilBPNAZQRzAKkGH4vxADjAxQV56F0F26TVBUvcwQVrZS0G7hxtBzm4fQQ8vF0HiDSFBaBQqQX\u002feG0GzHBpBUOE5QeRFJkE++xRBSeMuQRyPIkHsZTBBp7czQZGNF0FG7D1BRY1EQbaSFkHWby1BIxENQZ+SH0FcXypBTFgXQVeHEEFv7RZBSkwrQVSGMUEoNx5BOQw0QQ4AG0HXPyxB8rMWQXo6IEHjLSZBF7obQczbJEG9qBpBMsEdQZ7dL0EfSQlBhPUmQbLvLUGw4w1BzoYxQWXUM0GiJg5BG7orQelzSEGVwiNBPXAkQWvYKkG9sSRBMvIiQfvsGkHbRytB8GYmQSPxE0HkffVA7TUsQfhCG0GxoypBbvILQZNpJ0Ee3EpB\u002fdAjQQ=="},"y":{"dtype":"f4","bdata":"r9sbv8Yd7b9eb7y\u002f5Pp6ParFuL\u002fSSEi\u002foDODP+gT7L57Fz2\u002fyo6CP3GxGr\u002f\u002fPz6974KYvmGoyb7gxj4\u002fpwOlP6JBmr9bvFU+\u002fitNv17D879w8du\u002f+wo1v+XeAL9B2YE\u002f\u002frEkv\u002fc0GD5YtGC\u002fqnUhv\u002fn4XT5zSTm\u002fu0Pqv3uXy78FxGm\u002fkhbav5d47r9XUUk+KLOXPx3BbT+Gs9S\u002frfcsv\u002fCUdD5dw4m\u002fCZhTPk4WJr+PLYw\u002fFQB\u002fvuzp379f9oO+dsmUvSxMtb87\u002fOm+3XahP8VhQb8Dg+O\u002fmG59P9ZYVb\u002f+2Oa\u002fyaMfv2D9Ib+T6wC\u002fyHo3P79wsb92l6i+vaFCP3FkYL4tH6a\u002f\u002fEuuP\u002fIU5T1ga\u002fe+mHPmv7WYgL4CYFq\u002fy7ruv+rOnb7UzSK\u002fPMNsPw=="},"z":{"dtype":"f4","bdata":"Q3+tQI+\u002ftkAIcWpAhsZwQOk7UECxY6NA2M5uQOZXoEDqb4VAADxwQNrRy0BqYnFAxwtWQB65n0BXErNAF76AQHJ1pUAQ53lAP12MQFecXUCfI7hAiUOjQGRqe0B1v3RA23CVQKnR8D\u002f3oJNAcGbJQI058z8tZ4VAZeinQF8VikCMHaRAq8a9QJvdsUDsJ+g\u002fNuh\u002fQEjFdkB8V61A0XSIQMgUe0BRC1xACuTpP3GEk0DCD4ZANSeDQA1yjECdOo9A7YY9QFr3t0CfTuhAFI+BQKeGo0DixKRArmZuQMQig0BJ261AAp2MQN3uxEDCE5dAED+PQFA\u002fVEDgUmVAJJatQDLXiEDBv1ZAfrCAQPVrLEAPgKpAD7ZtQLmhjUDNiZJAAn2oQLojX0DR6MpA8H+NQA=="},"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"UMAP Dimension 1"}},"yaxis":{"title":{"text":"UMAP Dimension 2"}},"zaxis":{"title":{"text":"UMAP Dimension 3"}}},"legend":{"title":{"text":"color"},"tracegroupgap":0},"title":{"text":"3D UMAP Visualization of Word Embeddings"}},                        {"responsive": true}                    )                };            </script>        </div>
<pre class="python"><code>fig.update_layout(scene=dict(xaxis_title=&#39;UMAP Dimension 1&#39;,
                             yaxis_title=&#39;UMAP Dimension 2&#39;,
                             zaxis_title=&#39;UMAP Dimension 3&#39;))</code></pre>
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="d70bf9c0-61e2-4626-acd7-669a495eaab6" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("d70bf9c0-61e2-4626-acd7-669a495eaab6")) {                    Plotly.newPlot(                        "d70bf9c0-61e2-4626-acd7-669a495eaab6",                        [{"hovertemplate":"color=Bajta\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Bajta","marker":{"color":"#636efa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Bajta","scene":"scene","showlegend":true,"text":["Baseline comparison","System investigation","Considered","Not Considered","Size report","Temporal distance","Delphi","Analysis","Maintenance","Statistics analysis","Near offshore","Fuzzy similar","Agile","Group-based estimation","Execution","GA","Variation reduction","Conceptualization","CBR","Finance","Commissioning","Effort hours","Performance","Machine learning","Sensitivity analysis","Estimate value","Preliminary planning","Feasibility study","Implementation","Capability Maturity Model Integration","Close onshore","No of team members","Expert judgment","Non-machine learning","SE","Portfolio","Not considered","Telecommunication","Hardware","Staff\u002fcost","Other","Socio-cultural distance","Security","Individual","Availability","Detail Planning","Testing","Healthcare","Value","Geographical distance","Design","Reliability","Constructive Cost Model","Far offshore","Distant onshore","Risk","Maintainability"],"x":{"dtype":"f4","bdata":"yFIgQVgPIEG+FSlBvsEqQcF09EA+bghBH\u002fIgQbLQJEH9AExB7q0mQdk+FkEj0hxBfn0bQS+CKUH4PBtBtjUrQeLDK0FdkCZB58kmQRGcLkGJlTNBB8sgQZs1I0EzIhZBQ\u002fYtQTrIKkFsBShB2qoeQTHDG0GnuDRB8ZQWQZdPIEEyBCdBKqwWQZtrLEFxHSdBgd0qQaMPNUHqcixBLS4jQdacJ0FfaAhBbJY8QVQ5JUG17kNBB1QoQaHaLEE2XjZBoSsqQQ7aCEHPRytBbb5HQRUUJEETZRVBmacVQdTTPUG9u0lB"},"y":{"dtype":"f4","bdata":"5HKgvwAT2r+QlbK\u002fcTO2v08kpL7z9hU\u002f2KZ\u002fvzWO0r+81Ba\u002frzbYv6H4az7dHeW\u002fLTORvn4Foz+fFVG\u002f8jxZv1aN5L+gh8C\u002fmXNOv1lkLb\u002fuExe\u002fW3kUP0Gc7758Nf+\u002fzSXav8NDkD8AJoe\u002fJ0npv7+Sk7\u002fThcm+UgtcPhVlKz9+6uW\u002flbL5v1XxQb\u002fzEBa\u002fWmy9v1jSIb9b25+\u002fZ6FqPxuLEb9ljRk\u002f\u002fopYv6dhxb5ziQy\u002fPF2Cv0gT6r+p8zu\u002fw6wlP+CHDj+NmmO\u002fqoAhv3iPZj+3vZI+pPmCPvqIUL+Czju\u002f"},"z":{"dtype":"f4","bdata":"FDV3QBNUnkDDtmZAjZxPQCKxskBqxDdAaM+gQElojUBmQMxAjtONQBWa7j86EIJAQjSOQDiMhkAtPZtAvKdrQFT1lEDaVoBA3ChoQHdTh0AaUpBAoDmNQG29l0DXv3VAdOSWQAcpgECSRZdAXvOiQEFtpUBEF85AB6rpPxdxtEB7yl1ARxxvQOAvcEB9GpBAtq9bQPc1kkDxA6lArdWWQLfKcEBb6C5AmnaRQJYCcEDHQshAZMmbQBmYdkDCOoRA3CqDQMJ8LkCEZaFAhzPFQL5qlEDwDe8\u002fEf\u002fsP74xnEBxs8pA"},"type":"scatter3d"},{"hovertemplate":"color=Britto_2017\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2017","marker":{"color":"#EF553B","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2017","scene":"scene","showlegend":true,"text":["Interface complexity","Innovation level","Cluster slot count","Cluster count","Component count","Integration with legacy systems","Accessibility level","Time restriction","Cyclomatic complexity","Model association complexity","Web page count","Data flow complexity","Module attribute count","Reused lines of code","Rapid app development","Component granularity level","Association center slot count","Metrics program","Object-Oriented Heuristic Function Points","Development restriction","Entity count","International Function Point Users Group","Security level","Section count","Flexibility level","Slot count","Publishing unit count","Model link complexity","Node count","Association slot size","Personality","Software development experience","Risk level","Media allocation","Motivation level","Statement count","Model node size","Web page allocation","Requirements volatility level","Operational mode","Testability level","Work Team level","Authoring tool type","Attribute count","Reused high feature count","Reused media allocation","Platform support level","Product.Structure","Operation count","Use case count","Model collection complexity","New media count","Productivity level","Storage constraint","In-house experience","Diffusion cut count","Collection center slot count","Object-Oriented Function Points","Number of projects in parallel","Model slot size","Communication level","Information slot count","Reliability level","Domain experience level","Link count","High feature count","Project management level","Reused comment count","Maintainability level","Concern coupling","Tool experience level","Program count","Media duration","Client script count","Control flow complexity","Team capability","Server script count","Low feature count","Readability level","Node slot size","Concurrency level","Data Web points","Data usage complexity","Novelty level","Software reuse","Collection slot size","Requirements novelty level","Requirements clarity level","Trainability level","Process efficiency level","New complexity","Robustness level","Concern operation count","Class coupling","Module count","Team size","Reused component count","Focus factor","Module point cut count","Project.Infrastructure","Feature count","Product.Experience level","Scalability level","Cohesion complexity","Design volatility","Availability level","Output complexity","Web objects","Product.Type","Project.Type","Total complexity","Inner\u002fsub concern count","Platform volatility level","Component slot count","Page complexity","IT literacy","Stability level","Database size","Concern module count","Input complexity","Layout complexity","Anchor count","Installability level","Usability level","Technical factors","Processing requirements","Component complexity","Connectivity density","Resource level","Cohesion","Lines of code","Modularity level","Comment count","Time efficiency level","SPI program","Reusability level","Reused low feature count","Difficulty level","Cluster node size","Architecture","Lessons learned repository","Reused program count","Publishing model unit count","Common Software Measurement International Consortium","Slot granularity level","OO experience level","Memory efficiency level","Segment count","Reused media count","Programming language experience level","Compactness","New Web page count","Semantic association count","Portability level","Number of programming languages","Quality level","Adaptation complexity","Mapped workflows","Documentation level","Media count","Class complexity","Stratum","Indifferent concern count","Deployment platform experience level"],"x":{"dtype":"f4","bdata":"VinCQCCHMEFRHeZAoO3wQNGt+kDwBh9BuoI+QWEuFkHnlsdAt9rMQBEx+0CT9cRAtMb7QFRbDkE37Q9B+f3mQCnN4UCYjARByygKQccZKEFPP\u002f1A+oQMQRugQEGYLPhACig\u002fQWic50AzZv9Av7HPQOwo8kB13+NAVVIlQQbRHkEEmkBB+CQGQciDKUH4vwNBZ47hQO5d+0AkXDlBlNsZQcWpQEFSkSNBD8UpQREc\u002f0BtuwZBQVMFQQuNOEH0SCpBMqMGQemTCkFZLMlAvSoDQcVCKkEtyglBZBUeQbST9kDap+JAjMEKQcfcGEFNM+JAylI5QZhr5UCMKUVBfOgqQVWO9EApRgRBadwhQQw9BEHYjURByVUEQTF7KUE8bgxBlv0GQZO9B0F6F8NAD3ciQRgUB0G1GARB7Fc+Qf9w4EDb\u002fDZBfn8LQQz8w0D6tjFBn64UQXPZ4UBC2zJBUgkzQRXSQEHN7ClBlrTLQIPiQkF0KAdBlD\u002fYQIuI+kBnliBB2w0DQbfDEEEgFvhAyiYhQeW7A0EdbitBj306Qe4w00AmKDlB+Ns\u002fQZpAw0D6lwpB74MqQbIKI0GALshAvLQDQVu5PEHbaedALSDJQGL4NkF2F0RBl9vxQLImAEG77cJA4IDBQD599kDxfjxBLYI7QTsbLEHL\u002fSpB94nMQHiY4kCniDxB4VzkQNZ1D0ETpvdA3cEBQYrtJ0HA2BVBokc4QeWzBkG55DxBmITjQLbJKUFm2RxB4vQLQW5j+UCW6g5BGcjeQJO+KUHv2SlBAev1QGWyA0GZPSdBLtLXQPFe+kAknexAAME7QSnuDUHP6z5BNCvKQNe4GkES\u002fDFBpM0BQa\u002f0zUA26B5BPYwFQQn5KkE="},"y":{"dtype":"f4","bdata":"vhVGvwbxCb9svdQ+8MmFPrpZDD08kL+\u002fP6I0v0P0Wj6lLme\u002fDIq6vvffs74WUTO\u002fVXT0Ps8sP7+w3rm\u002ftVKJvgqn+T7yAky\u002fxtDav7aapr\u002fkIDQ+bA\u002fhv9WkVL8EWq4++z4lv47iDD8PdIw+xEDAvlc+QT6\u002faBQ\u002fx9wIv0Pfpb+NvU6\u002fQCm2PkL5lj29iuA9qHiqPk6JGb\u002fnvpq\u002fhnBdv7hFb7+L5Ns+Aj6Nv7BHjT3fzqO+lDmjPsGpbL\u002f9akG\u002fxToqPrZEO79M4+2+YqCtPp7yNz6kImg+4Eq3v2kK1T71Pvo+Flzev9YRZL5ZS\u002fo+Zg7ivsY3Dj+Kbiq\u002fkz2CvzvV\u002fL2ibES+\u002fCanvvQiK74FsTG\u002fKygYPyaNjL9wSai+Kt7lPttapL5fSku\u002fG+8rP3tLpr7KERq+OQtOv8vPBD8wa0i+\u002fFvVvzi1PL9WdkO\u002fOFV2v2XeBD+xcZW\u002fR6Kjv+cRUL8ZUYE+1fxUv0EbSr\u002fKz9s+OlCpvs0IFj\u002fZdzc\u002ftd1nvly537\u002fUARo\u002fUrc3vzi3zL0R74G\u002f2mHRvjclL7+bnaC\u002fCI\u002f2vlg5U79cE7i\u002f2vg9v7XTCL85RE+\u002fTCEAP511hr\u002fnzPk+ZoGEvwaegr+lzSi\u002fFh9JvvBmMj8umkq\u002f4xKCv9IGC7xC70q\u002fDZdUvwgCs79ii62\u002f9LJHvx4w5L5zzvu+2W44vyB3O79dRDA\u002fcF9NPZVTtT4Pml2\u002fV0Yiv9b8lb4DlU6\u002fHKjOPtrvY788ssS\u002fVEXnvh+5mz5Wn2q\u002fY2SIPkk1h78eL6A+zZFqPqhjqz4e5Ye\u002fn9REv4kZmr57hZK79Nsvv49Xob7kjwy\u002fxNVDv9gIl7+C\u002f5G\u002fj6r4PtfwP78WVrK+9bUlPw9ReL8="},"z":{"dtype":"f4","bdata":"B47ZQFwxukDwSfdAkKHvQG9750ASDqxA+tvSQHUShECrzd9AYwveQI+U0EA4h9lAxkfmQI4I0EBlyr9A+aPvQAIp+UD2O6lAnpqpQAnJrUAmANRAyAaoQEZPokCgrOFApmDJQFem+EDdoMVAz7rZQILw4kC+CvZAAW2FQA0lykAn\u002fqhAo6WrQFRrpEAoT9RArbDrQB76ykCgkbtATFWdQAVqz0CTJbJAZ6q0QGBE5ED2IuZAavCuQLDNyEDX2ahAjD\u002fPQFoq0EA5BNxAQ\u002fO0QHxVp0DwJKdAm1vFQPlM5EBiT\u002ftAXXepQPT6wkC2ovZA41GrQENf\u002fECsxb9A\u002fRnMQHvD2kAwzuxA1d63QKKD2UCPZMtAztXRQIM2ykC9YstAmzGpQCBRzkBW7NtAnZG2QN9kzkCWne5AyJjVQALE9EA5qshA1Gu2QFA43EAPHr5ABaPPQOps\u002fEBwp71APtbAQDcd0kBlradAmojbQPApvUByAtFATfXPQAe25UBm8bNAAKzhQLLOnkDDNuZA3CWxQBqB60C1V8dASb3NQPbT2UBELLVAjMPIQPHe3kAfrr9AkEStQOZ2sEAeBt1AIhvWQFm1vUBpMfxACR7dQPdQlkD6ZLpAwRa6QKhG30Bhmt9AOkPdQFt030Cuf9JAvwXZQMpCqECXsa5Aej\u002fiQOgk20AmjcdAxnbRQNQTxECZOetAp8\u002fVQItGokCnGatA4\u002frVQC5350D1e9tATr3vQMG2pEC0y71Aie7SQAnoykDv+LJA5Hb2QJklzECwOKtAoaffQB1stkDFtM1ALMLFQLIQ0ECcyeFAhcDQQE9uzkDuebdAu1\u002fZQOFyqEBD5shAuF6xQOI320ByBoVA13bTQKjAzkA="},"type":"scatter3d"},{"hovertemplate":"color=Britto_2016\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2016","marker":{"color":"#00cc96","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2016","scene":"scene","showlegend":true,"text":["Relationship.Temporal Distance","Centralized","Estimator & Provider","Site.Legal Entity","Early","Relationship.Geographic Distance","Site.Location","Site.Temporal Distance","Provider","Relationship.Location","Late","Early & Late","Distributed","Estimator","Relationship.Legal Entity","Site.Geographic Distance","Semi-distributed"],"x":{"dtype":"f4","bdata":"yNoJQXaIGEGNuShBc+gQQc0oHEFhGAlBefAMQXRcCkE5oydBhdUNQd2BG0Ff6hpBqv0bQWrzKkGXexFBdXoKQXg8GkE="},"y":{"dtype":"f4","bdata":"fh4GP7iHO72etqA\u002fvH6tPt8PEb92yRU\u002f6emFPt2T9j40Z0g+epqnPjjfF7+JsP6+1Z+6vMSDqz94e8c+0UDsPlml+Lw="},"z":{"dtype":"f4","bdata":"kP8zQEx9L0ClP4BA1sBSQN6WakCIQzBA5\u002f4+QE7lN0D4Ol5AXfM2QIvZZEBXoWlA4eksQMmIekBf21FAOzE2QPemIEA="},"type":"scatter3d"},{"hovertemplate":"color=Dasthi\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Dasthi","marker":{"color":"#ab63fa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Dasthi","scene":"scene","showlegend":true,"text":["AI-Combined hybrid","Basic-Combination","Software Evaluation and Estimation for Risk","Computational Intelligence.swarm","Constructive Cost Model","Computational Intelligence.evolutionary","Fuzzy Logic","Analogy-Based","Software Life Cycle Management","Artificial Neural Networks","Expert Judgment"],"x":{"dtype":"f4","bdata":"CSIVQXn+HEH\u002f2j5B\u002f8QTQYDrI0G\u002fVxRBSScfQX1iH0EjxRpBkfEVQWOMJUE="},"y":{"dtype":"f4","bdata":"Fgn4v9Kfjr8SgTS\u002fscD5vy9\u002fej845gLAsxf6v7YZz786MZG\u002fxZ0BwNAS8L8="},"z":{"dtype":"f4","bdata":"4wxyQIwGbkCXQKRAnJByQNOZlUDlkHlAuZ1\u002fQDdGikAAyclAibV3QCnpYUA="},"type":"scatter3d"},{"hovertemplate":"color=Mendes\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Mendes","marker":{"color":"#FFA15A","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Mendes","scene":"scene","showlegend":true,"text":["Nominal","Interval","Validation.None","Web hypermedia application","Program\u002fScript","Functionality","Web software application","Computation.Direct","Model dependency.Nonspecific","Model dependency.Specific","Validated Theoretically","Validation.Both","Complexity","Late size metric","Problem-oriented metric","Early size metric","Ratio","Validated Empirically","Ordinal","Solution-oriented metric","Absolute","Motivation","Class.Length","Media","Computation.Indirect","Web application"],"x":{"dtype":"f4","bdata":"978oQbZeGkFpaytB\u002fMILQVTeDEGwVBxB6P4MQUXqFUEm7dNAJYvUQCecJUF8\u002fSlB3DbMQPc4+kCN0gBB6g77QOzjHkFZ+CZB2XAmQbaQAUGWrihB3g4nQf9N3kDLYwlBB64WQW6HDEE="},"y":{"dtype":"f4","bdata":"wcRzv8+3p75OdNm\u002fnCizv2A\u002fEr9Jwaq\u002fe0G2v10Ior9YJsq7aMcNPcNO9r\u002fG8+S\u002fJMhwv042Kr8sEWm\u002fbRkyvxypMr7LGeO\u002fpZpHv3xBab\u002fKWam\u002fFeOxvY5eoL7c38Q+VPSnvyAXuL8="},"z":{"dtype":"f4","bdata":"qTRQQORoXkBe7mNAE8nBQF7owkA8SptArBXCQETolEChTNVAK7DWQJBMekCas2lA4BrbQNlIpUD3ZaRAy0+mQHJrTkD5IYBAtQNZQKC7pEAIIF5AcxKZQPBE0kBdLaVAJ\u002fqSQKibwUA="},"type":"scatter3d"},{"hovertemplate":"color=Usman\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Usman","marker":{"color":"#19d3f3","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Usman","scene":"scene","showlegend":true,"text":["Project domain.Other","User case points","Considered","Pair days","Not Considered","Considered without any metric","Median Magnitude of Relative Error","Customized Extreme Programming","Health","Mean Magnitude of Relative Error","Maintenance","Daily","Release","Extreme Programming","No. of team members","Estimation Techniques.Other","Implementation","Hours\u002fdays","Government\u002fMilitary","Expert Judgement","Use case","Retail\u002fWholesale","Crystal","Accuracy measure.Other","Communications industry","Distributed: Close Onshore","Security","Availability","Distributed: Far Offshore","Financial","Three point","Analogy","Design","User story","Use case points method","Distributed: Near Offshore","Estimate value(s)","Accuracy Level.Value","Dynamic Systems Development Method","Transportation","Ideal hours","Not used","Distributed: Distant Onshore","Task","Estimated activities.All","Kanban","Analysis","Customized Scrum","Distribution","Non functional requirements.Other","Feature-Driven Development","Estimation entity.Other","Manufacturing","Point","Bias of Relative Error","Education","Story points","Bidding","Reliability","Performance","Actual effort.Value","Not considered","Single","Group","Sprint","Not applicable","Number of entities estimated.Value","Co-located","Size.Other","Testing","Scrum","Planning Poker","Function points","Unit.Other","Maintainability","Effort estimate.Type.Other"],"x":{"dtype":"f4","bdata":"gD8hQW3SEkFabilBPNAZQRzAKkGH4vxADjAxQV56F0F26TVBUvcwQVrZS0G7hxtBzm4fQQ8vF0HiDSFBaBQqQX\u002feG0GzHBpBUOE5QeRFJkE++xRBSeMuQRyPIkHsZTBBp7czQZGNF0FG7D1BRY1EQbaSFkHWby1BIxENQZ+SH0FcXypBTFgXQVeHEEFv7RZBSkwrQVSGMUEoNx5BOQw0QQ4AG0HXPyxB8rMWQXo6IEHjLSZBF7obQczbJEG9qBpBMsEdQZ7dL0EfSQlBhPUmQbLvLUGw4w1BzoYxQWXUM0GiJg5BG7orQelzSEGVwiNBPXAkQWvYKkG9sSRBMvIiQfvsGkHbRytB8GYmQSPxE0HkffVA7TUsQfhCG0GxoypBbvILQZNpJ0Ee3EpB\u002fdAjQQ=="},"y":{"dtype":"f4","bdata":"r9sbv8Yd7b9eb7y\u002f5Pp6ParFuL\u002fSSEi\u002foDODP+gT7L57Fz2\u002fyo6CP3GxGr\u002f\u002fPz6974KYvmGoyb7gxj4\u002fpwOlP6JBmr9bvFU+\u002fitNv17D879w8du\u002f+wo1v+XeAL9B2YE\u002f\u002frEkv\u002fc0GD5YtGC\u002fqnUhv\u002fn4XT5zSTm\u002fu0Pqv3uXy78FxGm\u002fkhbav5d47r9XUUk+KLOXPx3BbT+Gs9S\u002frfcsv\u002fCUdD5dw4m\u002fCZhTPk4WJr+PLYw\u002fFQB\u002fvuzp379f9oO+dsmUvSxMtb87\u002fOm+3XahP8VhQb8Dg+O\u002fmG59P9ZYVb\u002f+2Oa\u002fyaMfv2D9Ib+T6wC\u002fyHo3P79wsb92l6i+vaFCP3FkYL4tH6a\u002f\u002fEuuP\u002fIU5T1ga\u002fe+mHPmv7WYgL4CYFq\u002fy7ruv+rOnb7UzSK\u002fPMNsPw=="},"z":{"dtype":"f4","bdata":"Q3+tQI+\u002ftkAIcWpAhsZwQOk7UECxY6NA2M5uQOZXoEDqb4VAADxwQNrRy0BqYnFAxwtWQB65n0BXErNAF76AQHJ1pUAQ53lAP12MQFecXUCfI7hAiUOjQGRqe0B1v3RA23CVQKnR8D\u002f3oJNAcGbJQI058z8tZ4VAZeinQF8VikCMHaRAq8a9QJvdsUDsJ+g\u002fNuh\u002fQEjFdkB8V61A0XSIQMgUe0BRC1xACuTpP3GEk0DCD4ZANSeDQA1yjECdOo9A7YY9QFr3t0CfTuhAFI+BQKeGo0DixKRArmZuQMQig0BJ261AAp2MQN3uxEDCE5dAED+PQFA\u002fVEDgUmVAJJatQDLXiEDBv1ZAfrCAQPVrLEAPgKpAD7ZtQLmhjUDNiZJAAn2oQLojX0DR6MpA8H+NQA=="},"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"UMAP Dimension 1"}},"yaxis":{"title":{"text":"UMAP Dimension 2"}},"zaxis":{"title":{"text":"UMAP Dimension 3"}}},"legend":{"title":{"text":"color"},"tracegroupgap":0},"title":{"text":"3D UMAP Visualization of Word Embeddings"}},                        {"responsive": true}                    )                };            </script>        </div>
<pre class="python"><code>plt.savefig(&#39;3d_word_embedding.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)

# Show the interactive plot
fig.show()</code></pre>
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="8190b8d4-cdf7-4e95-8caf-d8bf6d328bca" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("8190b8d4-cdf7-4e95-8caf-d8bf6d328bca")) {                    Plotly.newPlot(                        "8190b8d4-cdf7-4e95-8caf-d8bf6d328bca",                        [{"hovertemplate":"color=Bajta\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Bajta","marker":{"color":"#636efa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Bajta","scene":"scene","showlegend":true,"text":["Baseline comparison","System investigation","Considered","Not Considered","Size report","Temporal distance","Delphi","Analysis","Maintenance","Statistics analysis","Near offshore","Fuzzy similar","Agile","Group-based estimation","Execution","GA","Variation reduction","Conceptualization","CBR","Finance","Commissioning","Effort hours","Performance","Machine learning","Sensitivity analysis","Estimate value","Preliminary planning","Feasibility study","Implementation","Capability Maturity Model Integration","Close onshore","No of team members","Expert judgment","Non-machine learning","SE","Portfolio","Not considered","Telecommunication","Hardware","Staff\u002fcost","Other","Socio-cultural distance","Security","Individual","Availability","Detail Planning","Testing","Healthcare","Value","Geographical distance","Design","Reliability","Constructive Cost Model","Far offshore","Distant onshore","Risk","Maintainability"],"x":{"dtype":"f4","bdata":"yFIgQVgPIEG+FSlBvsEqQcF09EA+bghBH\u002fIgQbLQJEH9AExB7q0mQdk+FkEj0hxBfn0bQS+CKUH4PBtBtjUrQeLDK0FdkCZB58kmQRGcLkGJlTNBB8sgQZs1I0EzIhZBQ\u002fYtQTrIKkFsBShB2qoeQTHDG0GnuDRB8ZQWQZdPIEEyBCdBKqwWQZtrLEFxHSdBgd0qQaMPNUHqcixBLS4jQdacJ0FfaAhBbJY8QVQ5JUG17kNBB1QoQaHaLEE2XjZBoSsqQQ7aCEHPRytBbb5HQRUUJEETZRVBmacVQdTTPUG9u0lB"},"y":{"dtype":"f4","bdata":"5HKgvwAT2r+QlbK\u002fcTO2v08kpL7z9hU\u002f2KZ\u002fvzWO0r+81Ba\u002frzbYv6H4az7dHeW\u002fLTORvn4Foz+fFVG\u002f8jxZv1aN5L+gh8C\u002fmXNOv1lkLb\u002fuExe\u002fW3kUP0Gc7758Nf+\u002fzSXav8NDkD8AJoe\u002fJ0npv7+Sk7\u002fThcm+UgtcPhVlKz9+6uW\u002flbL5v1XxQb\u002fzEBa\u002fWmy9v1jSIb9b25+\u002fZ6FqPxuLEb9ljRk\u002f\u002fopYv6dhxb5ziQy\u002fPF2Cv0gT6r+p8zu\u002fw6wlP+CHDj+NmmO\u002fqoAhv3iPZj+3vZI+pPmCPvqIUL+Czju\u002f"},"z":{"dtype":"f4","bdata":"FDV3QBNUnkDDtmZAjZxPQCKxskBqxDdAaM+gQElojUBmQMxAjtONQBWa7j86EIJAQjSOQDiMhkAtPZtAvKdrQFT1lEDaVoBA3ChoQHdTh0AaUpBAoDmNQG29l0DXv3VAdOSWQAcpgECSRZdAXvOiQEFtpUBEF85AB6rpPxdxtEB7yl1ARxxvQOAvcEB9GpBAtq9bQPc1kkDxA6lArdWWQLfKcEBb6C5AmnaRQJYCcEDHQshAZMmbQBmYdkDCOoRA3CqDQMJ8LkCEZaFAhzPFQL5qlEDwDe8\u002fEf\u002fsP74xnEBxs8pA"},"type":"scatter3d"},{"hovertemplate":"color=Britto_2017\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2017","marker":{"color":"#EF553B","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2017","scene":"scene","showlegend":true,"text":["Interface complexity","Innovation level","Cluster slot count","Cluster count","Component count","Integration with legacy systems","Accessibility level","Time restriction","Cyclomatic complexity","Model association complexity","Web page count","Data flow complexity","Module attribute count","Reused lines of code","Rapid app development","Component granularity level","Association center slot count","Metrics program","Object-Oriented Heuristic Function Points","Development restriction","Entity count","International Function Point Users Group","Security level","Section count","Flexibility level","Slot count","Publishing unit count","Model link complexity","Node count","Association slot size","Personality","Software development experience","Risk level","Media allocation","Motivation level","Statement count","Model node size","Web page allocation","Requirements volatility level","Operational mode","Testability level","Work Team level","Authoring tool type","Attribute count","Reused high feature count","Reused media allocation","Platform support level","Product.Structure","Operation count","Use case count","Model collection complexity","New media count","Productivity level","Storage constraint","In-house experience","Diffusion cut count","Collection center slot count","Object-Oriented Function Points","Number of projects in parallel","Model slot size","Communication level","Information slot count","Reliability level","Domain experience level","Link count","High feature count","Project management level","Reused comment count","Maintainability level","Concern coupling","Tool experience level","Program count","Media duration","Client script count","Control flow complexity","Team capability","Server script count","Low feature count","Readability level","Node slot size","Concurrency level","Data Web points","Data usage complexity","Novelty level","Software reuse","Collection slot size","Requirements novelty level","Requirements clarity level","Trainability level","Process efficiency level","New complexity","Robustness level","Concern operation count","Class coupling","Module count","Team size","Reused component count","Focus factor","Module point cut count","Project.Infrastructure","Feature count","Product.Experience level","Scalability level","Cohesion complexity","Design volatility","Availability level","Output complexity","Web objects","Product.Type","Project.Type","Total complexity","Inner\u002fsub concern count","Platform volatility level","Component slot count","Page complexity","IT literacy","Stability level","Database size","Concern module count","Input complexity","Layout complexity","Anchor count","Installability level","Usability level","Technical factors","Processing requirements","Component complexity","Connectivity density","Resource level","Cohesion","Lines of code","Modularity level","Comment count","Time efficiency level","SPI program","Reusability level","Reused low feature count","Difficulty level","Cluster node size","Architecture","Lessons learned repository","Reused program count","Publishing model unit count","Common Software Measurement International Consortium","Slot granularity level","OO experience level","Memory efficiency level","Segment count","Reused media count","Programming language experience level","Compactness","New Web page count","Semantic association count","Portability level","Number of programming languages","Quality level","Adaptation complexity","Mapped workflows","Documentation level","Media count","Class complexity","Stratum","Indifferent concern count","Deployment platform experience level"],"x":{"dtype":"f4","bdata":"VinCQCCHMEFRHeZAoO3wQNGt+kDwBh9BuoI+QWEuFkHnlsdAt9rMQBEx+0CT9cRAtMb7QFRbDkE37Q9B+f3mQCnN4UCYjARByygKQccZKEFPP\u002f1A+oQMQRugQEGYLPhACig\u002fQWic50AzZv9Av7HPQOwo8kB13+NAVVIlQQbRHkEEmkBB+CQGQciDKUH4vwNBZ47hQO5d+0AkXDlBlNsZQcWpQEFSkSNBD8UpQREc\u002f0BtuwZBQVMFQQuNOEH0SCpBMqMGQemTCkFZLMlAvSoDQcVCKkEtyglBZBUeQbST9kDap+JAjMEKQcfcGEFNM+JAylI5QZhr5UCMKUVBfOgqQVWO9EApRgRBadwhQQw9BEHYjURByVUEQTF7KUE8bgxBlv0GQZO9B0F6F8NAD3ciQRgUB0G1GARB7Fc+Qf9w4EDb\u002fDZBfn8LQQz8w0D6tjFBn64UQXPZ4UBC2zJBUgkzQRXSQEHN7ClBlrTLQIPiQkF0KAdBlD\u002fYQIuI+kBnliBB2w0DQbfDEEEgFvhAyiYhQeW7A0EdbitBj306Qe4w00AmKDlB+Ns\u002fQZpAw0D6lwpB74MqQbIKI0GALshAvLQDQVu5PEHbaedALSDJQGL4NkF2F0RBl9vxQLImAEG77cJA4IDBQD599kDxfjxBLYI7QTsbLEHL\u002fSpB94nMQHiY4kCniDxB4VzkQNZ1D0ETpvdA3cEBQYrtJ0HA2BVBokc4QeWzBkG55DxBmITjQLbJKUFm2RxB4vQLQW5j+UCW6g5BGcjeQJO+KUHv2SlBAev1QGWyA0GZPSdBLtLXQPFe+kAknexAAME7QSnuDUHP6z5BNCvKQNe4GkES\u002fDFBpM0BQa\u002f0zUA26B5BPYwFQQn5KkE="},"y":{"dtype":"f4","bdata":"vhVGvwbxCb9svdQ+8MmFPrpZDD08kL+\u002fP6I0v0P0Wj6lLme\u002fDIq6vvffs74WUTO\u002fVXT0Ps8sP7+w3rm\u002ftVKJvgqn+T7yAky\u002fxtDav7aapr\u002fkIDQ+bA\u002fhv9WkVL8EWq4++z4lv47iDD8PdIw+xEDAvlc+QT6\u002faBQ\u002fx9wIv0Pfpb+NvU6\u002fQCm2PkL5lj29iuA9qHiqPk6JGb\u002fnvpq\u002fhnBdv7hFb7+L5Ns+Aj6Nv7BHjT3fzqO+lDmjPsGpbL\u002f9akG\u002fxToqPrZEO79M4+2+YqCtPp7yNz6kImg+4Eq3v2kK1T71Pvo+Flzev9YRZL5ZS\u002fo+Zg7ivsY3Dj+Kbiq\u002fkz2CvzvV\u002fL2ibES+\u002fCanvvQiK74FsTG\u002fKygYPyaNjL9wSai+Kt7lPttapL5fSku\u002fG+8rP3tLpr7KERq+OQtOv8vPBD8wa0i+\u002fFvVvzi1PL9WdkO\u002fOFV2v2XeBD+xcZW\u002fR6Kjv+cRUL8ZUYE+1fxUv0EbSr\u002fKz9s+OlCpvs0IFj\u002fZdzc\u002ftd1nvly537\u002fUARo\u002fUrc3vzi3zL0R74G\u002f2mHRvjclL7+bnaC\u002fCI\u002f2vlg5U79cE7i\u002f2vg9v7XTCL85RE+\u002fTCEAP511hr\u002fnzPk+ZoGEvwaegr+lzSi\u002fFh9JvvBmMj8umkq\u002f4xKCv9IGC7xC70q\u002fDZdUvwgCs79ii62\u002f9LJHvx4w5L5zzvu+2W44vyB3O79dRDA\u002fcF9NPZVTtT4Pml2\u002fV0Yiv9b8lb4DlU6\u002fHKjOPtrvY788ssS\u002fVEXnvh+5mz5Wn2q\u002fY2SIPkk1h78eL6A+zZFqPqhjqz4e5Ye\u002fn9REv4kZmr57hZK79Nsvv49Xob7kjwy\u002fxNVDv9gIl7+C\u002f5G\u002fj6r4PtfwP78WVrK+9bUlPw9ReL8="},"z":{"dtype":"f4","bdata":"B47ZQFwxukDwSfdAkKHvQG9750ASDqxA+tvSQHUShECrzd9AYwveQI+U0EA4h9lAxkfmQI4I0EBlyr9A+aPvQAIp+UD2O6lAnpqpQAnJrUAmANRAyAaoQEZPokCgrOFApmDJQFem+EDdoMVAz7rZQILw4kC+CvZAAW2FQA0lykAn\u002fqhAo6WrQFRrpEAoT9RArbDrQB76ykCgkbtATFWdQAVqz0CTJbJAZ6q0QGBE5ED2IuZAavCuQLDNyEDX2ahAjD\u002fPQFoq0EA5BNxAQ\u002fO0QHxVp0DwJKdAm1vFQPlM5EBiT\u002ftAXXepQPT6wkC2ovZA41GrQENf\u002fECsxb9A\u002fRnMQHvD2kAwzuxA1d63QKKD2UCPZMtAztXRQIM2ykC9YstAmzGpQCBRzkBW7NtAnZG2QN9kzkCWne5AyJjVQALE9EA5qshA1Gu2QFA43EAPHr5ABaPPQOps\u002fEBwp71APtbAQDcd0kBlradAmojbQPApvUByAtFATfXPQAe25UBm8bNAAKzhQLLOnkDDNuZA3CWxQBqB60C1V8dASb3NQPbT2UBELLVAjMPIQPHe3kAfrr9AkEStQOZ2sEAeBt1AIhvWQFm1vUBpMfxACR7dQPdQlkD6ZLpAwRa6QKhG30Bhmt9AOkPdQFt030Cuf9JAvwXZQMpCqECXsa5Aej\u002fiQOgk20AmjcdAxnbRQNQTxECZOetAp8\u002fVQItGokCnGatA4\u002frVQC5350D1e9tATr3vQMG2pEC0y71Aie7SQAnoykDv+LJA5Hb2QJklzECwOKtAoaffQB1stkDFtM1ALMLFQLIQ0ECcyeFAhcDQQE9uzkDuebdAu1\u002fZQOFyqEBD5shAuF6xQOI320ByBoVA13bTQKjAzkA="},"type":"scatter3d"},{"hovertemplate":"color=Britto_2016\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Britto_2016","marker":{"color":"#00cc96","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Britto_2016","scene":"scene","showlegend":true,"text":["Relationship.Temporal Distance","Centralized","Estimator & Provider","Site.Legal Entity","Early","Relationship.Geographic Distance","Site.Location","Site.Temporal Distance","Provider","Relationship.Location","Late","Early & Late","Distributed","Estimator","Relationship.Legal Entity","Site.Geographic Distance","Semi-distributed"],"x":{"dtype":"f4","bdata":"yNoJQXaIGEGNuShBc+gQQc0oHEFhGAlBefAMQXRcCkE5oydBhdUNQd2BG0Ff6hpBqv0bQWrzKkGXexFBdXoKQXg8GkE="},"y":{"dtype":"f4","bdata":"fh4GP7iHO72etqA\u002fvH6tPt8PEb92yRU\u002f6emFPt2T9j40Z0g+epqnPjjfF7+JsP6+1Z+6vMSDqz94e8c+0UDsPlml+Lw="},"z":{"dtype":"f4","bdata":"kP8zQEx9L0ClP4BA1sBSQN6WakCIQzBA5\u002f4+QE7lN0D4Ol5AXfM2QIvZZEBXoWlA4eksQMmIekBf21FAOzE2QPemIEA="},"type":"scatter3d"},{"hovertemplate":"color=Dasthi\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Dasthi","marker":{"color":"#ab63fa","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Dasthi","scene":"scene","showlegend":true,"text":["AI-Combined hybrid","Basic-Combination","Software Evaluation and Estimation for Risk","Computational Intelligence.swarm","Constructive Cost Model","Computational Intelligence.evolutionary","Fuzzy Logic","Analogy-Based","Software Life Cycle Management","Artificial Neural Networks","Expert Judgment"],"x":{"dtype":"f4","bdata":"CSIVQXn+HEH\u002f2j5B\u002f8QTQYDrI0G\u002fVxRBSScfQX1iH0EjxRpBkfEVQWOMJUE="},"y":{"dtype":"f4","bdata":"Fgn4v9Kfjr8SgTS\u002fscD5vy9\u002fej845gLAsxf6v7YZz786MZG\u002fxZ0BwNAS8L8="},"z":{"dtype":"f4","bdata":"4wxyQIwGbkCXQKRAnJByQNOZlUDlkHlAuZ1\u002fQDdGikAAyclAibV3QCnpYUA="},"type":"scatter3d"},{"hovertemplate":"color=Mendes\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Mendes","marker":{"color":"#FFA15A","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Mendes","scene":"scene","showlegend":true,"text":["Nominal","Interval","Validation.None","Web hypermedia application","Program\u002fScript","Functionality","Web software application","Computation.Direct","Model dependency.Nonspecific","Model dependency.Specific","Validated Theoretically","Validation.Both","Complexity","Late size metric","Problem-oriented metric","Early size metric","Ratio","Validated Empirically","Ordinal","Solution-oriented metric","Absolute","Motivation","Class.Length","Media","Computation.Indirect","Web application"],"x":{"dtype":"f4","bdata":"978oQbZeGkFpaytB\u002fMILQVTeDEGwVBxB6P4MQUXqFUEm7dNAJYvUQCecJUF8\u002fSlB3DbMQPc4+kCN0gBB6g77QOzjHkFZ+CZB2XAmQbaQAUGWrihB3g4nQf9N3kDLYwlBB64WQW6HDEE="},"y":{"dtype":"f4","bdata":"wcRzv8+3p75OdNm\u002fnCizv2A\u002fEr9Jwaq\u002fe0G2v10Ior9YJsq7aMcNPcNO9r\u002fG8+S\u002fJMhwv042Kr8sEWm\u002fbRkyvxypMr7LGeO\u002fpZpHv3xBab\u002fKWam\u002fFeOxvY5eoL7c38Q+VPSnvyAXuL8="},"z":{"dtype":"f4","bdata":"qTRQQORoXkBe7mNAE8nBQF7owkA8SptArBXCQETolEChTNVAK7DWQJBMekCas2lA4BrbQNlIpUD3ZaRAy0+mQHJrTkD5IYBAtQNZQKC7pEAIIF5AcxKZQPBE0kBdLaVAJ\u002fqSQKibwUA="},"type":"scatter3d"},{"hovertemplate":"color=Usman\u003cbr\u003eUMAP Dimension 1=%{x}\u003cbr\u003eUMAP Dimension 2=%{y}\u003cbr\u003eUMAP Dimension 3=%{z}\u003cbr\u003etext=%{text}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Usman","marker":{"color":"#19d3f3","symbol":"circle","opacity":0.8,"size":5},"mode":"markers+text","name":"Usman","scene":"scene","showlegend":true,"text":["Project domain.Other","User case points","Considered","Pair days","Not Considered","Considered without any metric","Median Magnitude of Relative Error","Customized Extreme Programming","Health","Mean Magnitude of Relative Error","Maintenance","Daily","Release","Extreme Programming","No. of team members","Estimation Techniques.Other","Implementation","Hours\u002fdays","Government\u002fMilitary","Expert Judgement","Use case","Retail\u002fWholesale","Crystal","Accuracy measure.Other","Communications industry","Distributed: Close Onshore","Security","Availability","Distributed: Far Offshore","Financial","Three point","Analogy","Design","User story","Use case points method","Distributed: Near Offshore","Estimate value(s)","Accuracy Level.Value","Dynamic Systems Development Method","Transportation","Ideal hours","Not used","Distributed: Distant Onshore","Task","Estimated activities.All","Kanban","Analysis","Customized Scrum","Distribution","Non functional requirements.Other","Feature-Driven Development","Estimation entity.Other","Manufacturing","Point","Bias of Relative Error","Education","Story points","Bidding","Reliability","Performance","Actual effort.Value","Not considered","Single","Group","Sprint","Not applicable","Number of entities estimated.Value","Co-located","Size.Other","Testing","Scrum","Planning Poker","Function points","Unit.Other","Maintainability","Effort estimate.Type.Other"],"x":{"dtype":"f4","bdata":"gD8hQW3SEkFabilBPNAZQRzAKkGH4vxADjAxQV56F0F26TVBUvcwQVrZS0G7hxtBzm4fQQ8vF0HiDSFBaBQqQX\u002feG0GzHBpBUOE5QeRFJkE++xRBSeMuQRyPIkHsZTBBp7czQZGNF0FG7D1BRY1EQbaSFkHWby1BIxENQZ+SH0FcXypBTFgXQVeHEEFv7RZBSkwrQVSGMUEoNx5BOQw0QQ4AG0HXPyxB8rMWQXo6IEHjLSZBF7obQczbJEG9qBpBMsEdQZ7dL0EfSQlBhPUmQbLvLUGw4w1BzoYxQWXUM0GiJg5BG7orQelzSEGVwiNBPXAkQWvYKkG9sSRBMvIiQfvsGkHbRytB8GYmQSPxE0HkffVA7TUsQfhCG0GxoypBbvILQZNpJ0Ee3EpB\u002fdAjQQ=="},"y":{"dtype":"f4","bdata":"r9sbv8Yd7b9eb7y\u002f5Pp6ParFuL\u002fSSEi\u002foDODP+gT7L57Fz2\u002fyo6CP3GxGr\u002f\u002fPz6974KYvmGoyb7gxj4\u002fpwOlP6JBmr9bvFU+\u002fitNv17D879w8du\u002f+wo1v+XeAL9B2YE\u002f\u002frEkv\u002fc0GD5YtGC\u002fqnUhv\u002fn4XT5zSTm\u002fu0Pqv3uXy78FxGm\u002fkhbav5d47r9XUUk+KLOXPx3BbT+Gs9S\u002frfcsv\u002fCUdD5dw4m\u002fCZhTPk4WJr+PLYw\u002fFQB\u002fvuzp379f9oO+dsmUvSxMtb87\u002fOm+3XahP8VhQb8Dg+O\u002fmG59P9ZYVb\u002f+2Oa\u002fyaMfv2D9Ib+T6wC\u002fyHo3P79wsb92l6i+vaFCP3FkYL4tH6a\u002f\u002fEuuP\u002fIU5T1ga\u002fe+mHPmv7WYgL4CYFq\u002fy7ruv+rOnb7UzSK\u002fPMNsPw=="},"z":{"dtype":"f4","bdata":"Q3+tQI+\u002ftkAIcWpAhsZwQOk7UECxY6NA2M5uQOZXoEDqb4VAADxwQNrRy0BqYnFAxwtWQB65n0BXErNAF76AQHJ1pUAQ53lAP12MQFecXUCfI7hAiUOjQGRqe0B1v3RA23CVQKnR8D\u002f3oJNAcGbJQI058z8tZ4VAZeinQF8VikCMHaRAq8a9QJvdsUDsJ+g\u002fNuh\u002fQEjFdkB8V61A0XSIQMgUe0BRC1xACuTpP3GEk0DCD4ZANSeDQA1yjECdOo9A7YY9QFr3t0CfTuhAFI+BQKeGo0DixKRArmZuQMQig0BJ261AAp2MQN3uxEDCE5dAED+PQFA\u002fVEDgUmVAJJatQDLXiEDBv1ZAfrCAQPVrLEAPgKpAD7ZtQLmhjUDNiZJAAn2oQLojX0DR6MpA8H+NQA=="},"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"scene":{"domain":{"x":[0.0,1.0],"y":[0.0,1.0]},"xaxis":{"title":{"text":"UMAP Dimension 1"}},"yaxis":{"title":{"text":"UMAP Dimension 2"}},"zaxis":{"title":{"text":"UMAP Dimension 3"}}},"legend":{"title":{"text":"color"},"tracegroupgap":0},"title":{"text":"3D UMAP Visualization of Word Embeddings"}},                        {"responsive": true}                    )                };            </script>        </div>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-9-5.png" style="display: block; margin: auto;" /></p>
</div>
<div id="k-means-plot" class="section level1">
<h1>K-means Plot</h1>
<pre class="python"><code>import random
import umap.umap_ as umap
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
from matplotlib.lines import Line2D  # Add this import at the top of your code
colorstyle = &quot;Set2&quot;
seed=5
marker_styles = [&#39;o&#39;, &#39;^&#39;, &#39;s&#39;, &#39;p&#39;, &#39;*&#39;, &#39;D&#39;]
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)</code></pre>
<pre><code>&lt;torch._C.Generator object at 0x000000015ABAEFB0&gt;</code></pre>
<pre class="python"><code>plt.clf()

plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)  # You can change this to any available style

plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;
# Step 1: Define the sets
Bajta = {&#39;Conceptualization&#39;, &#39;Feasibility study&#39;, &#39;Preliminary planning&#39;, &#39;Detail Planning&#39;, &#39;Execution&#39;, &#39;Commissioning&#39;, &#39;System investigation&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Other&#39;, &#39;SE&#39;, &#39;Research &amp; Dev&#39;, &#39;Telecommunication&#39;, &#39;Finance&#39;, &#39;Healthcare&#39;, &#39;Other&#39;, &#39;Close onshore&#39;, &#39;Distant onshore&#39;, &#39;Near offshore&#39;, &#39;Far offshore&#39;, &#39;Constructive Cost Model&#39;, &#39;Capability Maturity Model Integration&#39;, &#39;Agile&#39;, &#39;Delphi&#39;, &#39;GA&#39;, &#39;CBR&#39;, &#39;Fuzzy similar&#39;, &#39;Other&#39;, &#39;Value&#39;, &#39;No of team members&#39;, &#39;Expert judgment&#39;, &#39;Machine learning&#39;, &#39;Non-machine learning&#39;, &#39;Individual&#39;, &#39;Group-based estimation&#39;, &#39;Estimate value&#39;, &#39;Value&#39;, &#39;Effort hours&#39;, &#39;Staff/cost&#39;, &#39;Hardware&#39;, &#39;Risk&#39;, &#39;Portfolio&#39;, &#39;Baseline comparison&#39;, &#39;Variation reduction&#39;, &#39;Sensitivity analysis&#39;, &#39;Size report&#39;, &#39;Statistics analysis&#39;, &#39;Considered&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Other&#39;, &#39;Geographical distance&#39;, &#39;Temporal distance&#39;, &#39;Socio-cultural distance&#39;}
Britto_2017 = {&#39;Web page count&#39;, &#39;Media count&#39;, &#39;New media count&#39;, &#39;New Web page count&#39;, &#39;Link count&#39;, &#39;Program count&#39;, &#39;Reused component count&#39;, &#39;Lines of code&#39;, &#39;Reused program count&#39;, &#39;Reused media count&#39;, &#39;Web page allocation&#39;, &#39;Reused lines of code&#39;, &#39;Media allocation&#39;, &#39;Reused media allocation&#39;, &#39;Entity count&#39;, &#39;Attribute count&#39;, &#39;Component count&#39;, &#39;Statement count&#39;, &#39;Node count&#39;, &#39;Collection slot size&#39;, &#39;Component granularity level&#39;, &#39;Slot granularity level&#39;, &#39;Model node size&#39;, &#39;Cluster node size&#39;, &#39;Node slot size&#39;, &#39;Publishing model unit count&#39;, &#39;Model slot size&#39;, &#39;Association slot size&#39;, &#39;Client script count&#39;, &#39;Server script count&#39;, &#39;Information slot count&#39;, &#39;Association center slot count&#39;, &#39;Collection center slot count&#39;, &#39;Component slot count&#39;, &#39;Semantic association count&#39;, &#39;Segment count&#39;, &#39;Slot count&#39;, &#39;Cluster slot count&#39;, &#39;Cluster count&#39;, &#39;Publishing unit count&#39;, &#39;Section count&#39;, &#39;Inner/sub concern count&#39;, &#39;Indifferent concern count&#39;, &#39;Module point cut count&#39;, &#39;Module count&#39;, &#39;Module attribute count&#39;, &#39;Operation count&#39;, &#39;Comment count&#39;, &#39;Reused comment count&#39;, &#39;Media duration&#39;, &#39;Diffusion cut count&#39;, &#39;Concern module count&#39;, &#39;Concern operation count&#39;, &#39;Anchor count&#39;, &#39;High feature count&#39;, &#39;Low feature count&#39;, &#39;Reused high feature count&#39;, &#39;Reused low feature count&#39;, &#39;Web objects&#39;, &#39;Common Software Measurement International Consortium&#39;, &#39;International Function Point Users Group&#39;, &#39;Object-Oriented Heuristic Function Points&#39;, &#39;Object-Oriented Function Points&#39;, &#39;Use case count&#39;, &#39;Feature count&#39;, &#39;Data Web points&#39;, &#39;Cohesion&#39;, &#39;Class coupling&#39;, &#39;Concern coupling&#39;, &#39;Connectivity density&#39;, &#39;Cyclomatic complexity&#39;, &#39;Model collection complexity&#39;, &#39;Model association complexity&#39;, &#39;Model link complexity&#39;, &#39;Page complexity&#39;, &#39;Component complexity&#39;, &#39;Total complexity&#39;, &#39;Adaptation complexity&#39;, &#39;New complexity&#39;, &#39;Data usage complexity&#39;, &#39;Data flow complexity&#39;, &#39;Cohesion complexity&#39;, &#39;Interface complexity&#39;, &#39;Control flow complexity&#39;, &#39;Class complexity&#39;, &#39;Layout complexity&#39;, &#39;Input complexity&#39;, &#39;Output complexity&#39;, &#39;Product.Type&#39;, &#39;Stratum&#39;, &#39;Compactness&#39;, &#39;Product.Structure&#39;, &#39;Architecture&#39;, &#39;Integration with legacy systems&#39;, &#39;Concurrency level&#39;, &#39;Processing requirements&#39;, &#39;Database size&#39;, &#39;Requirements volatility level&#39;, &#39;Requirements novelty level&#39;, &#39;Reliability level&#39;, &#39;Maintainability level&#39;, &#39;Time efficiency level&#39;, &#39;Memory efficiency level&#39;, &#39;Portability level&#39;, &#39;Scalability level&#39;, &#39;Quality level&#39;, &#39;Usability level&#39;, &#39;Readability level&#39;, &#39;Security level&#39;, &#39;Installability level&#39;, &#39;Modularity level&#39;, &#39;Flexibility level&#39;, &#39;Testability level&#39;, &#39;Accessibility level&#39;, &#39;Trainability level&#39;, &#39;Innovation level&#39;, &#39;Technical factors&#39;, &#39;Storage constraint&#39;, &#39;Reusability level&#39;, &#39;Robustness level&#39;, &#39;Design volatility&#39;, &#39;Product.Experience level&#39;, &#39;Requirements clarity level&#39;, &#39;Availability level&#39;, &#39;IT literacy&#39;, &#39;Mapped workflows&#39;, &#39;Personality&#39;, &#39;SPI program&#39;, &#39;Metrics program&#39;, &#39;Number of projects in parallel&#39;, &#39;Software reuse&#39;, &#39;Documentation level&#39;, &#39;Number of programming languages&#39;, &#39;Project.Type&#39;, &#39;Process efficiency level&#39;, &#39;Project management level&#39;, &#39;Project.Infrastructure&#39;, &#39;Development restriction&#39;, &#39;Time restriction&#39;, &#39;Risk level&#39;, &#39;Rapid app development&#39;, &#39;Operational mode&#39;, &#39;Resource level&#39;, &#39;Lessons learned repository&#39;, &#39;Domain experience level&#39;, &#39;Team size&#39;, &#39;Deployment platform experience level&#39;, &#39;Team capability&#39;, &#39;Programming language experience level&#39;, &#39;Tool experience level&#39;, &#39;Communication level&#39;, &#39;Software development experience&#39;, &#39;Work Team level&#39;, &#39;Stability level&#39;, &#39;Motivation level&#39;, &#39;Focus factor&#39;, &#39;OO experience level&#39;, &#39;In-house experience&#39;, &#39;Authoring tool type&#39;, &#39;Productivity level&#39;, &#39;Novelty level&#39;, &#39;Platform volatility level&#39;, &#39;Difficulty level&#39;, &#39;Platform support level&#39;}
Britto_2016 = {&#39;Site.Location&#39;, &#39;Site.Legal Entity&#39;, &#39;Site.Geographic Distance&#39;, &#39;Site.Temporal Distance&#39;, &#39;Early&#39;, &#39;Early &amp; Late&#39;, &#39;Late&#39;, &#39;Estimator&#39;, &#39;Estimator &amp; Provider&#39;, &#39;Provider&#39;, &#39;Relationship.Location&#39;, &#39;Relationship.Legal Entity&#39;, &#39;Relationship.Geographic Distance&#39;, &#39;Relationship.Temporal Distance&#39;, &#39;Centralized&#39;, &#39;Distributed&#39;, &#39;Semi-distributed&#39;}
Dasthi = {&#39;Constructive Cost Model&#39;, &#39;Software Life Cycle Management&#39;, &#39;Software Evaluation and Estimation for Risk&#39;, &#39;Expert Judgment&#39;, &#39;Analogy-Based&#39;, &#39;Basic-Combination&#39;, &#39;Fuzzy Logic&#39;, &#39;Artificial Neural Networks&#39;, &#39;Computational Intelligence.swarm&#39;, &#39;Computational Intelligence.evolutionary&#39;, &#39;AI-Combined hybrid&#39;}
Mendes = {&#39;Motivation&#39;, &#39;Early size metric&#39;, &#39;Late size metric&#39;, &#39;Problem-oriented metric&#39;, &#39;Solution-oriented metric&#39;, &#39;Class.Length&#39;, &#39;Functionality&#39;, &#39;Complexity&#39;, &#39;Web hypermedia application&#39;, &#39;Web software application&#39;, &#39;Web application&#39;, &#39;Media&#39;, &#39;Program/Script&#39;, &#39;Nominal&#39;, &#39;Ordinal&#39;, &#39;Interval&#39;, &#39;Ratio&#39;, &#39;Absolute&#39;, &#39;Computation.Direct&#39;, &#39;Computation.Indirect&#39;, &#39;Validated Empirically&#39;, &#39;Validated Theoretically&#39;, &#39;Validation.Both&#39;, &#39;Validation.None&#39;, &#39;Model dependency.Specific&#39;, &#39;Model dependency.Nonspecific&#39;}
Usman = {&#39;Release&#39;, &#39;Sprint&#39;, &#39;Daily&#39;, &#39;Bidding&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Estimated activities.All&#39;, &#39;Extreme Programming&#39;, &#39;Scrum&#39;, &#39;Customized Extreme Programming&#39;, &#39;Customized Scrum&#39;, &#39;Dynamic Systems Development Method&#39;, &#39;Crystal&#39;, &#39;Feature-Driven Development&#39;, &#39;Kanban&#39;, &#39;Communications industry&#39;, &#39;Transportation&#39;, &#39;Financial&#39;, &#39;Education&#39;, &#39;Health&#39;, &#39;Retail/Wholesale&#39;, &#39;Manufacturing&#39;, &#39;Government/Military&#39;, &#39;Project domain.Other&#39;, &#39;Co-located&#39;, &#39;Distributed: Close Onshore&#39;, &#39;Distributed: Distant Onshore&#39;, &#39;Distributed: Near Offshore&#39;, &#39;Distributed: Far Offshore&#39;, &#39;User story&#39;, &#39;Task&#39;, &#39;Use case&#39;, &#39;Estimation entity.Other&#39;, &#39;Number of entities estimated.Value&#39;, &#39;No. of team members&#39;, &#39;Planning Poker&#39;, &#39;Expert Judgement&#39;, &#39;Analogy&#39;, &#39;Use case points method&#39;, &#39;Estimation Techniques.Other&#39;, &#39;Single&#39;, &#39;Group&#39;, &#39;Story points&#39;, &#39;User case points&#39;, &#39;Function points&#39;, &#39;Size.Other&#39;, &#39;Not used&#39;, &#39;Considered without any metric&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Non functional requirements.Other&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Not applicable&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Estimate value(s)&#39;, &#39;Actual effort.Value&#39;, &#39;Point&#39;, &#39;Three point&#39;, &#39;Distribution&#39;, &#39;Effort estimate.Type.Other&#39;, &#39;Hours/days&#39;, &#39;Pair days&#39;, &#39;Ideal hours&#39;, &#39;Unit.Other&#39;, &#39;Accuracy Level.Value&#39;, &#39;Mean Magnitude of Relative Error&#39;, &#39;Median Magnitude of Relative Error&#39;, &#39;Bias of Relative Error&#39;, &#39;Accuracy measure.Other&#39;, &#39;Not used&#39;}

# Combine the sets into a dictionary
sets = {
    &#39;Bajta&#39;: Bajta,
    &#39;Britto_2017&#39;: Britto_2017,
    &#39;Britto_2016&#39;: Britto_2016,
    &#39;Dasthi&#39;: Dasthi,
    &#39;Mendes&#39;: Mendes,
    &#39;Usman&#39;: Usman
}

# Step 2: Flatten the sets into a dataframe
words = []
labels = []
for label, words_set in sets.items():
    for word in words_set:
        words.append(word.lower())
        labels.append(label)

# Create a dataframe
df = pd.DataFrame({&#39;Word&#39;: words, &#39;Set&#39;: labels})

# Step 3: Load the pre-trained model and tokenizer
model_name = &quot;jinaai/jina-embeddings-v3&quot;
if &#39;model&#39; not in locals() or &#39;tokenizer&#39; not in locals():
    print(&quot;Loading model and tokenizer...&quot;)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
else:
    print(&quot;Model and tokenizer are already loaded.&quot;)</code></pre>
<pre><code>Model and tokenizer are already loaded.</code></pre>
<pre class="python"><code># Step 4: Get the embeddings for each word
def get_embeddings(word):
    inputs = tokenizer(word, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

embeddings = np.array([get_embeddings(word) for word in df[&#39;Word&#39;]])

# Step 5: Perform 2D UMAP
umap_model = umap.UMAP(n_components=2, random_state=5)
embeddings_2d = umap_model.fit_transform(embeddings)</code></pre>
<pre><code>C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\sklearn\utils\deprecation.py:151: FutureWarning:

&#39;force_all_finite&#39; was renamed to &#39;ensure_all_finite&#39; in 1.6 and will be removed in 1.8.

C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\umap\umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.</code></pre>
<pre class="python"><code># Step 6: Create a color map that reflects the set labels
unique_labels = list(df[&#39;Set&#39;].unique())  # Get the unique set labels
cmap = plt.cm.get_cmap(colorstyle, len(unique_labels))  # Create a colormap with enough colors</code></pre>
<pre><code>&lt;string&gt;:1: MatplotlibDeprecationWarning:

The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.</code></pre>
<pre class="python"><code># Step 7: Run K-means on UMAP embeddings
num_clusters = len(unique_labels)  # Set number of clusters to match unique labels
kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=5)
kmeans_labels = kmeans.fit_predict(embeddings_2d)

# Step 8: Generate top 4 names for each cluster
top_n = 3  # Set how many top words to display for each cluster
cluster_names = []

for i in range(num_clusters):
    # Get the embeddings for words in the current cluster
    cluster_indices = np.where(kmeans_labels == i)[0]
    cluster_embeddings = embeddings[cluster_indices]
    
    # Calculate the centroid of the cluster
    cluster_centroid = np.mean(cluster_embeddings, axis=0).reshape(1, -1)
    
    # Calculate cosine similarity of centroid to all words&#39; embeddings to find closest words
    similarities = cosine_similarity(cluster_centroid, embeddings).flatten()
    
    # Get the indices of the top 4 closest words
    closest_word_indices = np.argsort(similarities)[-top_n:][::-1]  # Get indices of top 4 closest words
    
    # Get the words corresponding to these indices
    closest_words = df[&#39;Word&#39;].iloc[closest_word_indices].tolist()
    
    # Store the top 4 closest words as the cluster name
    cluster_names.append(closest_words)

# Step 9: Plot with translucent shapes for each K-means cluster and annotate with top 4 names
plt.figure(figsize=(10, 7))
color_map = {label: cmap(i) for i, label in enumerate(unique_labels)}

# Create a list of marker styles to use for each label
marker_styles = [&#39;o&#39;, &#39;^&#39;, &#39;s&#39;, &#39;p&#39;, &#39;*&#39;, &#39;D&#39;]  # Add more marker styles if needed

# Loop through each label and plot with the corresponding marker style
plt.figure(figsize=(10, 7))

for i, label in enumerate(unique_labels):
    # Get the data for the current label
    label_data = df[df[&#39;Set&#39;] == label]
    
    # Plot with a different marker for each label
    plt.scatter(embeddings_2d[df[&#39;Set&#39;] == label, 0], 
                embeddings_2d[df[&#39;Set&#39;] == label, 1],
                c=[color_map[label]] * len(label_data), 
                s=80, 
                label=label,
                marker=marker_styles[i % len(marker_styles)], alpha=0.6)  # Use modulo to cycle through marker styles


# Draw convex hulls around each cluster and annotate with cluster names
for i in range(num_clusters):
    cluster_points = embeddings_2d[kmeans_labels == i]
    
    if len(cluster_points) &gt;= 3:  # ConvexHull requires at least 3 points
        hull = ConvexHull(cluster_points)
        hull_points = cluster_points[hull.vertices]
        plt.fill(hull_points[:, 0], hull_points[:, 1], alpha=0.2, 
                 color=cmap(i), label=f&#39;Cluster {i+1}&#39;)
    
    # Annotate with the top 4 cluster names at the centroid location
    cluster_centroid_2d = np.mean(cluster_points, axis=0)
    # Join the top 4 words into a string with commas for cleaner display
    cluster_name_text = &#39;\n&#39;.join(cluster_names[i]).upper() 
    
    # Annotate with the top words at the centroid, with slightly smaller font size
    plt.text(cluster_centroid_2d[0], cluster_centroid_2d[1], cluster_name_text, 
             fontsize=8, ha=&#39;center&#39;, color=&#39;black&#39;)

# Step 10: Custom legend to show colors and shapes for each label
plt.title(&quot;2D UMAP Visualization of Word Embeddings with K-means Clusters&quot;)
plt.xlabel(&quot;UMAP Dimension 1&quot;)
plt.ylabel(&quot;UMAP Dimension 2&quot;)

legend_elements = [Line2D([0], [0], marker=marker_styles[i % len(marker_styles)], color=&#39;w&#39;, 
                          markerfacecolor=color_map[label], markersize=10, label=label)
                   for i, label in enumerate(unique_labels)]
plt.legend(
    handles=legend_elements,
    title=&quot;Literature&quot;,
    loc=&quot;lower center&quot;,
    bbox_to_anchor=(0.5, -0.2),  # Position it just below the plot
    ncol=len(unique_labels),      # Arrange legend items in a single row
    frameon=False                 # Optional: Remove legend box frame
)
# Adjust layout to ensure the legend is not clipped
plt.tight_layout()

# Step 11: Save the plot in high resolution
plt.savefig(&#39;word_embeddings_kmeans.png&#39;, dpi=600, bbox_inches=&#39;tight&#39;)

# Show the plot
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-10-7.png" style="display: block; margin: auto;" /><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-10-8.png" style="display: block; margin: auto;" /></p>
<pre class="python"><code>import torch
from transformers import AutoModel, AutoTokenizer
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

plt.clf()

plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)  # You can change this to any available style

plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;

# Define the sets of words
Bajta = {&quot;Agile&quot;, &quot;Analysis&quot;, &quot;Availability&quot;, &quot;Baseline comparison&quot;, &quot;Bidding&quot;, &quot;CBR&quot;, &quot;CMMI&quot;, &quot;COCOMO&quot;, &quot;Commissioning&quot;, &quot;Conceptualization&quot;, &quot;Delphi&quot;, &quot;Detail planning&quot;, &quot;Design&quot;, &quot;Distant onshore&quot;, &quot;Expert judgment&quot;, &quot;Estimated value&quot;, &quot;Execution&quot;, &quot;Effort hours&quot;, &quot;Feasibility study&quot;, &quot;Finance&quot;, &quot;Fuzzy similarity&quot;, &quot;GA&quot;, &quot;Group-based estimation&quot;, &quot;Healthcare&quot;, &quot;Hardware&quot;, &quot;Implementation&quot;, &quot;Individual&quot;, &quot;Machine learning&quot;, &quot;Maintainability&quot;, &quot;Maintenance&quot;, &quot;Near offshore&quot;, &quot;Non-machine learning&quot;, &quot;Not considered&quot;, &quot;Number of team members&quot;, &quot;Performance&quot;, &quot;Portfolio&quot;, &quot;Preliminary planning&quot;, &quot;Reliability&quot;, &quot;Research &amp; development&quot;, &quot;Risk&quot;, &quot;Security&quot;, &quot;Sensitivity analysis&quot;, &quot;Size report&quot;, &quot;Socio-cultural distance&quot;, &quot;Statistical analysis&quot;, &quot;Staff/cost&quot;, &quot;System investigation&quot;, &quot;Temporal distance&quot;, &quot;Testing&quot;, &quot;Value&quot;, &quot;Variation reduction&quot;}
Britto_2017 = {&quot;Accessibility level&quot;, &quot;Adaptation complexity&quot;, &quot;Anchor count&quot;, &quot;Architecture&quot;, &quot;Association center slot count&quot;, &quot;Association slot size&quot;, &quot;Attribute count&quot;, &quot;Authoring tool type&quot;, &quot;Availability level&quot;, &quot;Class complexity&quot;, &quot;Class coupling&quot;, &quot;Client script count&quot;, &quot;Cluster count&quot;, &quot;Cluster node size&quot;, &quot;Cluster slot count&quot;, &quot;Cohesion&quot;, &quot;Cohesion complexity&quot;, &quot;Collection center slot count&quot;, &quot;Collection slot size&quot;, &quot;Comment count&quot;, &quot;Communication level&quot;, &quot;Compactness&quot;, &quot;Component complexity&quot;, &quot;Component count&quot;, &quot;Component granularity level&quot;, &quot;Component slot count&quot;, &quot;Concern coupling&quot;, &quot;Concern module count&quot;, &quot;Concern operation count&quot;, &quot;Concurrency level&quot;, &quot;Connectivity density&quot;, &quot;Control flow complexity&quot;, &quot;Cyclomatic complexity&quot;, &quot;Data Web points&quot;, &quot;Data flow complexity&quot;, &quot;Data usage complexity&quot;, &quot;Database size&quot;, &quot;Deployment platform experience level&quot;, &quot;Design volatility&quot;, &quot;Development restriction&quot;, &quot;Difficulty level&quot;, &quot;Diffusion cut count&quot;, &quot;Documentation level&quot;, &quot;Domain experience level&quot;, &quot;Entity count&quot;, &quot;Experience level&quot;, &quot;Feature count&quot;, &quot;Flexibility level&quot;, &quot;Focus factor&quot;, &quot;High feature count&quot;, &quot;IT literacy&quot;, &quot;In-house experience&quot;, &quot;Indifferent concern count&quot;, &quot;Information slot count&quot;, &quot;Infrastructure&quot;, &quot;Inner/sub concern count&quot;, &quot;Innovation level&quot;, &quot;Input complexity&quot;, &quot;Installability level&quot;, &quot;Integration with legacy systems&quot;, &quot;Interface complexity&quot;, &quot;International Function Point Users Group&quot;, &quot;Layout complexity&quot;, &quot;Lessons learned repository&quot;, &quot;Lines of code&quot;, &quot;Link count&quot;, &quot;Low feature count&quot;, &quot;Maintainability level&quot;, &quot;Mapped workflows&quot;, &quot;Media allocation&quot;, &quot;Media count&quot;, &quot;Media duration&quot;, &quot;Memory efficiency level&quot;, &quot;Metrics program&quot;, &quot;Model association complexity&quot;, &quot;Model collection complexity&quot;, &quot;Model link complexity&quot;, &quot;Model node size&quot;, &quot;Model slot size&quot;, &quot;Modularity level&quot;, &quot;Module attribute count&quot;, &quot;Module count&quot;, &quot;Module point cut count&quot;, &quot;Motivation level&quot;, &quot;New Web page count&quot;, &quot;New complexity&quot;, &quot;New media count&quot;, &quot;Node count&quot;, &quot;Node slot size&quot;, &quot;Novelty level&quot;, &quot;Number of programming languages&quot;, &quot;Number of projects in parallel&quot;, &quot;OO experience level&quot;, &quot;Object-Oriented Function Points&quot;, &quot;Operation count&quot;, &quot;Operational mode&quot;, &quot;Output complexity&quot;, &quot;Page complexity&quot;, &quot;Personality&quot;, &quot;Platform support level&quot;, &quot;Platform volatility level&quot;, &quot;Portability level&quot;, &quot;Process efficiency level&quot;, &quot;Processing requirements&quot;, &quot;Productivity level&quot;, &quot;Program count&quot;, &quot;Programming language experience level&quot;, &quot;Project management level&quot;, &quot;Publishing model unit count&quot;, &quot;Publishing unit count&quot;, &quot;Quality level&quot;, &quot;Rapid app development&quot;, &quot;Readability level&quot;, &quot;Reliability level&quot;, &quot;Requirements clarity level&quot;, &quot;Requirements novelty level&quot;, &quot;Requirements volatility level&quot;, &quot;Resource level&quot;, &quot;Reusability level&quot;, &quot;Reused comment count&quot;, &quot;Reused component count&quot;, &quot;Reused high feature count&quot;, &quot;Reused lines of code&quot;, &quot;Reused low feature count&quot;, &quot;Reused media allocation&quot;, &quot;Reused media count&quot;, &quot;Reused program count&quot;, &quot;Risk level&quot;, &quot;Robustness level&quot;, &quot;SPI program&quot;, &quot;Scalability level&quot;, &quot;Section count&quot;, &quot;Security level&quot;, &quot;Segment count&quot;, &quot;Semantic association count&quot;, &quot;Server script count&quot;, &quot;Slot count&quot;, &quot;Slot granularity level&quot;, &quot;Software development experience&quot;, &quot;Software reuse&quot;, &quot;Stability level&quot;, &quot;Statement count&quot;, &quot;Storage constraint&quot;, &quot;Structure&quot;, &quot;Team capability&quot;, &quot;Team size&quot;, &quot;Technical factors&quot;, &quot;Testability level&quot;, &quot;Time efficiency level&quot;, &quot;Time restriction&quot;, &quot;Tool experience level&quot;, &quot;Total complexity&quot;, &quot;Trainability level&quot;, &quot;Type&quot;, &quot;Usability level&quot;, &quot;Use case count&quot;, &quot;Web objects&quot;, &quot;Web page allocation&quot;, &quot;Web page count&quot;, &quot;Work Team level&quot;}
Britto_2016 = {&quot;Centralized&quot;, &quot;distributed&quot;, &quot;Early&quot;, &quot;Estimator&quot;, &quot;Early &amp; Late&quot;, &quot;Estimator &amp; Provider&quot;, &quot;geographic distance&quot;, &quot;geographic distance&quot;, &quot;late&quot;, &quot;legal entity&quot;, &quot;location&quot;, &quot;provider&quot;, &quot;semi-distributed&quot;, &quot;temporal distance&quot;, &quot;temporal distance&quot;}
Dasthi = {&quot;ANN&quot;, &quot;Analogy Base&quot;, &quot;COCOMO&quot;, &quot;Evolutionary&quot;, &quot;Expert Judgment&quot;, &quot;FUZZY&quot;, &quot;SEER-SEM&quot;, &quot;SLIM&quot;, &quot;Swarm&quot;}
Mendes = {&quot;Absolute&quot;, &quot;both&quot;, &quot;complexity&quot;, &quot;functionality&quot;, &quot;Directly&quot;, &quot;Early size metric&quot;, &quot;Empirically&quot;, &quot;indirectly&quot;, &quot;interval&quot;, &quot;Length&quot;, &quot;late size metric&quot;, &quot;media&quot;, &quot;none&quot;, &quot;Nominal&quot;, &quot;nonspecific&quot;, &quot;ordinal&quot;, &quot;other&quot;, &quot;Problem oriented metric&quot;, &quot;program/script&quot;, &quot;ratio&quot;, &quot;solution oriented metric&quot;, &quot;Specific&quot;, &quot;theoretically&quot;, &quot;Web application&quot;, &quot;Web hypermedia application&quot;, &quot;Web software application&quot;}
Usman = {&quot;Analysis&quot;, &quot;all&quot;, &quot;analogy&quot;, &quot;availability&quot;, &quot;bidding&quot;, &quot;Close Onshore&quot;, &quot;Co-located&quot;, &quot;Communications industry&quot;, &quot;Considered&quot;, &quot;crystal&quot;, &quot;customized XP&quot;, &quot;customized scrum&quot;, &quot;daily&quot;, &quot;design&quot;, &quot;distribution&quot;, &quot;education&quot;, &quot;expert judgement&quot;, &quot;DSDM&quot;, &quot;Distant Onshore&quot;, &quot;Estimate value(s)&quot;, &quot;FDD&quot;, &quot;Far Offshore&quot;, &quot;financial&quot;, &quot;function points&quot;, &quot;Hours/days&quot;, &quot;health&quot;, &quot;ideal hours&quot;, &quot;implementation&quot;, &quot;kanban&quot;, &quot;maintainability&quot;, &quot;maintenance&quot;, &quot;manufacturing&quot;, &quot;MMRE&quot;, &quot;MdMRE&quot;, &quot;Near Offshore&quot;, &quot;No. of team members&quot;, &quot;not applicable&quot;, &quot;not considered&quot;, &quot;not used&quot;, &quot;Other&quot;, &quot;Performance&quot;, &quot;Planning poker&quot;, &quot;Point&quot;, &quot;pair days&quot;, &quot;Release&quot;, &quot;reliability&quot;, &quot;retail/wholesale&quot;, &quot;Single&quot;, &quot;scrum&quot;, &quot;security&quot;, &quot;sprint&quot;, &quot;Story points&quot;, &quot;testing&quot;, &quot;three point&quot;, &quot;task&quot;, &quot;transportation&quot;, &quot;UC points&quot;, &quot;User story&quot;, &quot;Value&quot;, &quot;XP&quot;}

# Combine all sets into a single list with labels
word_sets = {
    &quot;Bajta&quot;: Bajta,
    &quot;Britto_2016&quot;: Britto_2016,
    &quot;Britto_2017&quot;: Britto_2017,
    &quot;Dasthi&quot;: Dasthi,
    &quot;Mendes&quot;: Mendes,
    &quot;Usman&quot;: Usman
}

word_sets = {label: {word.lower() for word in words} for label, words in word_sets.items()}

# Load model and tokenizer
model_name = &quot;jinaai/jina-embeddings-v3&quot;
if &#39;model&#39; not in locals() or &#39;tokenizer&#39; not in locals():
    print(&quot;Loading model and tokenizer...&quot;)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
else:
    print(&quot;Model and tokenizer are already loaded.&quot;)</code></pre>
<pre><code>Model and tokenizer are already loaded.</code></pre>
<pre class="python"><code># Function to get embedding for a word
def get_embedding(word):
    inputs = tokenizer(word, return_tensors=&quot;pt&quot;)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Collect embeddings
embeddings = []
words = []
labels = []

for label, words_set in word_sets.items():
    for word in words_set:
        embedding = get_embedding(word)
        embeddings.append(embedding)
        words.append(word)
        labels.append(label)

# Create a DataFrame with words, labels, and embeddings
embedding_df = pd.DataFrame({
    &quot;Word&quot;: words,
    &quot;Label&quot;: labels,
    &quot;Embedding&quot;: [emb[0] for emb in embeddings]
})

# Pivot the DataFrame to have the set labels as columns
pivoted_df = embedding_df.pivot(index=&quot;Word&quot;, columns=&quot;Label&quot;, values=&quot;Embedding&quot;)

# Flatten the embeddings (if you want to display them properly as vectors, you might want to separate them)
# Convert the embedding vectors to string for display purposes (or keep them as arrays if you&#39;re working with them in computations)
pivoted_df = pivoted_df.applymap(lambda x: str(x.tolist()) if isinstance(x, np.ndarray) else x)</code></pre>
<pre><code>&lt;string&gt;:4: FutureWarning:

DataFrame.applymap has been deprecated. Use DataFrame.map instead.</code></pre>
<pre class="python"><code># Display the pivoted DataFrame
print(pivoted_df)</code></pre>
<pre><code>Label                                                                 Bajta  ...                                              Usman
Word                                                                         ...                                                   
absolute                                                                NaN  ...                                                NaN
accessibility level                                                     NaN  ...                                                NaN
adaptation complexity                                                   NaN  ...                                                NaN
agile                     [2.776266098022461, -2.1827030181884766, 1.469...  ...                                                NaN
all                                                                     NaN  ...  [1.5750207901000977, -2.3228142261505127, 1.04...
...                                                                     ...  ...                                                ...
web page allocation                                                     NaN  ...                                                NaN
web page count                                                          NaN  ...                                                NaN
web software application                                                NaN  ...                                                NaN
work team level                                                         NaN  ...                                                NaN
xp                                                                      NaN  ...  [2.5570878982543945, -1.4092556238174438, -0.1...

[300 rows x 6 columns]</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-11-11.png" style="display: block; margin: auto;" /></p>
</div>
<div
id="another-table-showing-the-common-words-between-papers-a-bit-harder-to-read"
class="section level1">
<h1>Another table showing the common words between papers, a bit harder
to read</h1>
<pre class="python"><code>import torch
from transformers import AutoModel, AutoTokenizer
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

plt.clf()

plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)  # You can change this to any available style

plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;

# Define the sets of words (characteristics)
Bajta = {&#39;Conceptualization&#39;, &#39;Feasibility study&#39;, &#39;Preliminary planning&#39;, &#39;Detail Planning&#39;, &#39;Execution&#39;, &#39;Commissioning&#39;, &#39;System investigation&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Other&#39;, &#39;SE&#39;, &#39;Telecommunication&#39;, &#39;Finance&#39;, &#39;Healthcare&#39;, &#39;Other&#39;, &#39;Close onshore&#39;, &#39;Distant onshore&#39;, &#39;Near offshore&#39;, &#39;Far offshore&#39;, &#39;Constructive Cost Model&#39;, &#39;Capability Maturity Model Integration&#39;, &#39;Agile&#39;, &#39;Delphi&#39;, &#39;GA&#39;, &#39;CBR&#39;, &#39;Fuzzy similar&#39;, &#39;Other&#39;, &#39;Value&#39;, &#39;No of team members&#39;, &#39;Expert judgment&#39;, &#39;Machine learning&#39;, &#39;Non-machine learning&#39;, &#39;Individual&#39;, &#39;Group-based estimation&#39;, &#39;Estimate value&#39;, &#39;Value&#39;, &#39;Effort hours&#39;, &#39;Staff/cost&#39;, &#39;Hardware&#39;, &#39;Risk&#39;, &#39;Portfolio&#39;, &#39;Baseline comparison&#39;, &#39;Variation reduction&#39;, &#39;Sensitivity analysis&#39;, &#39;Size report&#39;, &#39;Statistics analysis&#39;, &#39;Considered&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Other&#39;, &#39;Geographical distance&#39;, &#39;Temporal distance&#39;, &#39;Socio-cultural distance&#39;}
Britto_2017 = {&#39;Web page count&#39;, &#39;Media count&#39;, &#39;New media count&#39;, &#39;New Web page count&#39;, &#39;Link count&#39;, &#39;Program count&#39;, &#39;Reused component count&#39;, &#39;Lines of code&#39;, &#39;Reused program count&#39;, &#39;Reused media count&#39;, &#39;Web page allocation&#39;, &#39;Reused lines of code&#39;, &#39;Media allocation&#39;, &#39;Reused media allocation&#39;, &#39;Entity count&#39;, &#39;Attribute count&#39;, &#39;Component count&#39;, &#39;Statement count&#39;, &#39;Node count&#39;, &#39;Collection slot size&#39;, &#39;Component granularity level&#39;, &#39;Slot granularity level&#39;, &#39;Model node size&#39;, &#39;Cluster node size&#39;, &#39;Node slot size&#39;, &#39;Publishing model unit count&#39;, &#39;Model slot size&#39;, &#39;Association slot size&#39;, &#39;Client script count&#39;, &#39;Server script count&#39;, &#39;Information slot count&#39;, &#39;Association center slot count&#39;, &#39;Collection center slot count&#39;, &#39;Component slot count&#39;, &#39;Semantic association count&#39;, &#39;Segment count&#39;, &#39;Slot count&#39;, &#39;Cluster slot count&#39;, &#39;Cluster count&#39;, &#39;Publishing unit count&#39;, &#39;Section count&#39;, &#39;Inner/sub concern count&#39;, &#39;Indifferent concern count&#39;, &#39;Module point cut count&#39;, &#39;Module count&#39;, &#39;Module attribute count&#39;, &#39;Operation count&#39;, &#39;Comment count&#39;, &#39;Reused comment count&#39;, &#39;Media duration&#39;, &#39;Diffusion cut count&#39;, &#39;Concern module count&#39;, &#39;Concern operation count&#39;, &#39;Anchor count&#39;, &#39;High feature count&#39;, &#39;Low feature count&#39;, &#39;Reused high feature count&#39;, &#39;Reused low feature count&#39;, &#39;Web objects&#39;, &#39;Common Software Measurement International Consortium&#39;, &#39;International Function Point Users Group&#39;, &#39;Object-Oriented Heuristic Function Points&#39;, &#39;Object-Oriented Function Points&#39;, &#39;Use case count&#39;, &#39;Feature count&#39;, &#39;Data Web points&#39;, &#39;Cohesion&#39;, &#39;Class coupling&#39;, &#39;Concern coupling&#39;, &#39;Connectivity density&#39;, &#39;Cyclomatic complexity&#39;, &#39;Model collection complexity&#39;, &#39;Model association complexity&#39;, &#39;Model link complexity&#39;, &#39;Page complexity&#39;, &#39;Component complexity&#39;, &#39;Total complexity&#39;, &#39;Adaptation complexity&#39;, &#39;New complexity&#39;, &#39;Data usage complexity&#39;, &#39;Data flow complexity&#39;, &#39;Cohesion complexity&#39;, &#39;Interface complexity&#39;, &#39;Control flow complexity&#39;, &#39;Class complexity&#39;, &#39;Layout complexity&#39;, &#39;Input complexity&#39;, &#39;Output complexity&#39;, &#39;Product.Type&#39;, &#39;Stratum&#39;, &#39;Compactness&#39;, &#39;Product.Structure&#39;, &#39;Architecture&#39;, &#39;Integration with legacy systems&#39;, &#39;Concurrency level&#39;, &#39;Processing requirements&#39;, &#39;Database size&#39;, &#39;Requirements volatility level&#39;, &#39;Requirements novelty level&#39;, &#39;Reliability level&#39;, &#39;Maintainability level&#39;, &#39;Time efficiency level&#39;, &#39;Memory efficiency level&#39;, &#39;Portability level&#39;, &#39;Scalability level&#39;, &#39;Quality level&#39;, &#39;Usability level&#39;, &#39;Readability level&#39;, &#39;Security level&#39;, &#39;Installability level&#39;, &#39;Modularity level&#39;, &#39;Flexibility level&#39;, &#39;Testability level&#39;, &#39;Accessibility level&#39;, &#39;Trainability level&#39;, &#39;Innovation level&#39;, &#39;Technical factors&#39;, &#39;Storage constraint&#39;, &#39;Reusability level&#39;, &#39;Robustness level&#39;, &#39;Design volatility&#39;, &#39;Product.Experience level&#39;, &#39;Requirements clarity level&#39;, &#39;Availability level&#39;, &#39;IT literacy&#39;, &#39;Mapped workflows&#39;, &#39;Personality&#39;, &#39;SPI program&#39;, &#39;Metrics program&#39;, &#39;Number of projects in parallel&#39;, &#39;Software reuse&#39;, &#39;Documentation level&#39;, &#39;Number of programming languages&#39;, &#39;Project.Type&#39;, &#39;Process efficiency level&#39;, &#39;Project management level&#39;, &#39;Project.Infrastructure&#39;, &#39;Development restriction&#39;, &#39;Time restriction&#39;, &#39;Risk level&#39;, &#39;Rapid app development&#39;, &#39;Operational mode&#39;, &#39;Resource level&#39;, &#39;Lessons learned repository&#39;, &#39;Domain experience level&#39;, &#39;Team size&#39;, &#39;Deployment platform experience level&#39;, &#39;Team capability&#39;, &#39;Programming language experience level&#39;, &#39;Tool experience level&#39;, &#39;Communication level&#39;, &#39;Software development experience&#39;, &#39;Work Team level&#39;, &#39;Stability level&#39;, &#39;Motivation level&#39;, &#39;Focus factor&#39;, &#39;OO experience level&#39;, &#39;In-house experience&#39;, &#39;Authoring tool type&#39;, &#39;Productivity level&#39;, &#39;Novelty level&#39;, &#39;Platform volatility level&#39;, &#39;Difficulty level&#39;, &#39;Platform support level&#39;}
Britto_2016 = {&#39;Site.Location&#39;, &#39;Site.Legal Entity&#39;, &#39;Site.Geographic Distance&#39;, &#39;Site.Temporal Distance&#39;, &#39;Early&#39;, &#39;Early &amp; Late&#39;, &#39;Late&#39;, &#39;Estimator&#39;, &#39;Estimator &amp; Provider&#39;, &#39;Provider&#39;, &#39;Relationship.Location&#39;, &#39;Relationship.Legal Entity&#39;, &#39;Relationship.Geographic Distance&#39;, &#39;Relationship.Temporal Distance&#39;, &#39;Centralized&#39;, &#39;Distributed&#39;, &#39;Semi-distributed&#39;}
Dasthi = {&#39;Constructive Cost Model&#39;, &#39;Software Life Cycle Management&#39;, &#39;Software Evaluation and Estimation for Risk&#39;, &#39;Expert Judgment&#39;, &#39;Analogy-Based&#39;, &#39;Basic-Combination&#39;, &#39;Fuzzy Logic&#39;, &#39;Artificial Neural Networks&#39;, &#39;Computational Intelligence.swarm&#39;, &#39;Computational Intelligence.evolutionary&#39;, &#39;AI-Combined hybrid&#39;}
Mendes = {&#39;Motivation&#39;, &#39;Early size metric&#39;, &#39;Late size metric&#39;, &#39;Problem-oriented metric&#39;, &#39;Solution-oriented metric&#39;, &#39;Class.Length&#39;, &#39;Functionality&#39;, &#39;Complexity&#39;, &#39;Web hypermedia application&#39;, &#39;Web software application&#39;, &#39;Web application&#39;, &#39;Media&#39;, &#39;Program/Script&#39;, &#39;Nominal&#39;, &#39;Ordinal&#39;, &#39;Interval&#39;, &#39;Ratio&#39;, &#39;Absolute&#39;, &#39;Computation.Direct&#39;, &#39;Computation.Indirect&#39;, &#39;Validated Empirically&#39;, &#39;Validated Theoretically&#39;, &#39;Validation.Both&#39;, &#39;Validation.None&#39;, &#39;Model dependency.Specific&#39;, &#39;Model dependency.Nonspecific&#39;}
Usman = {&#39;Release&#39;, &#39;Sprint&#39;, &#39;Daily&#39;, &#39;Bidding&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Estimated activities.All&#39;, &#39;Extreme Programming&#39;, &#39;Scrum&#39;, &#39;Customized Extreme Programming&#39;, &#39;Customized Scrum&#39;, &#39;Dynamic Systems Development Method&#39;, &#39;Crystal&#39;, &#39;Feature-Driven Development&#39;, &#39;Kanban&#39;, &#39;Communications industry&#39;, &#39;Transportation&#39;, &#39;Financial&#39;, &#39;Education&#39;, &#39;Health&#39;, &#39;Retail/Wholesale&#39;, &#39;Manufacturing&#39;, &#39;Government/Military&#39;, &#39;Project domain.Other&#39;, &#39;Co-located&#39;, &#39;Distributed: Close Onshore&#39;, &#39;Distributed: Distant Onshore&#39;, &#39;Distributed: Near Offshore&#39;, &#39;Distributed: Far Offshore&#39;, &#39;User story&#39;, &#39;Task&#39;, &#39;Use case&#39;, &#39;Estimation entity.Other&#39;, &#39;Number of entities estimated.Value&#39;, &#39;No. of team members&#39;, &#39;Planning Poker&#39;, &#39;Expert Judgement&#39;, &#39;Analogy&#39;, &#39;Use case points method&#39;, &#39;Estimation Techniques.Other&#39;, &#39;Single&#39;, &#39;Group&#39;, &#39;Story points&#39;, &#39;User case points&#39;, &#39;Function points&#39;, &#39;Size.Other&#39;, &#39;Not used&#39;, &#39;Considered without any metric&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Non functional requirements.Other&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Not applicable&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Estimate value(s)&#39;, &#39;Actual effort.Value&#39;, &#39;Point&#39;, &#39;Three point&#39;, &#39;Distribution&#39;, &#39;Effort estimate.Type.Other&#39;, &#39;Hours/days&#39;, &#39;Pair days&#39;, &#39;Ideal hours&#39;, &#39;Unit.Other&#39;, &#39;Accuracy Level.Value&#39;, &#39;Mean Magnitude of Relative Error&#39;, &#39;Median Magnitude of Relative Error&#39;, &#39;Bias of Relative Error&#39;, &#39;Accuracy measure.Other&#39;, &#39;Not used&#39;}


# # Define the sets of words (categories)
# Bajta = {&quot;Cost estimation context&quot;, &quot;Estimation technique&quot;, &quot;Cost estimate&quot;, &quot;Cost estimators&quot;, &quot;Planning&quot;, &quot;Project activities&quot;, &quot;Project domain&quot;, &quot;Project setting&quot;, &quot;Planning approaches&quot;, &quot;Number of sites&quot;, &quot;Team size&quot;, &quot;Estimation technique&quot;, &quot;Use technique&quot;, &quot;Estimated cost&quot;, &quot;Actual cost&quot;, &quot;Estimation dimension&quot;, &quot;Accuracy measure&quot;, &quot;Product size&quot;, &quot;Team experience&quot;, &quot;Team structure&quot;, &quot;Product requirement&quot;, &quot;Distributed teams distances&quot;}
# Britto_2017 = {&quot;Size Metric&quot;, &quot;Cost Driver&quot;, &quot;Length&quot;, &quot;Functionality&quot;, &quot;Object-oriented&quot;, &quot;Complexity&quot;, &quot;Product&quot;, &quot;Client&quot;, &quot;Development Company&quot;, &quot;Project&quot;, &quot;Team&quot;, &quot;Technology&quot;}
# Britto_2016 = {&quot;Project&quot;, &quot;Setting site&quot;, &quot;Setting relationship&quot;, &quot;Estimation stage&quot;, &quot;Estimation process architectural model&quot;}
# Dasthi = {&quot;Basic Estimating Methods&quot;, &quot;Combined Estimating Methods&quot;, &quot;Algorithmic&quot;, &quot;Non-Algorithmic&quot;, &quot;AI-Combination&quot;}
# Mendes = {&quot;Motivation&quot;, &quot;Harvesting time&quot;, &quot;Metric foundation&quot;, &quot;Class&quot;, &quot;Entity&quot;, &quot;Measurement Scale&quot;, &quot;Computation&quot;, &quot;Validation&quot;, &quot;Model dependency&quot;}
# Usman = {&quot;Estimation context&quot;, &quot;Estimation technique&quot;, &quot;Effort predictors&quot;, &quot;Effort estimate&quot;, &quot;Planning level&quot;, &quot;Estimated activities&quot;, &quot;Agile methods&quot;, &quot;Project domain&quot;, &quot;Project setting&quot;, &quot;Estimation entity&quot;, &quot;Number of entities estimated&quot;, &quot;Team size&quot;, &quot;Type&quot;, &quot;Size&quot;, &quot;Team&#39;s prior experience&quot;, &quot;Team&#39;s skill level&quot;, &quot;Non-functional requirements&quot;, &quot;Distributed teams&#39; issues&quot;, &quot;Customer Communication&quot;, &quot;Estimated effort&quot;, &quot;Actual effort&quot;, &quot;Type&quot;, &quot;Unit&quot;, &quot;Accuracy Level&quot;, &quot;Accuracy measure&quot;}
# 
# # Create a dictionary to store the sets
# sets = {
#     &quot;Bajta&quot;: Bajta,
#     &quot;Britto_2016&quot;: Britto_2016,
#     &quot;Britto_2017&quot;: Britto_2017,
#     &quot;Dasthi&quot;: Dasthi,
#     &quot;Mendes&quot;: Mendes,
#     &quot;Usman&quot;: Usman
# }

# Load the pre-trained model and tokenizer
model_name = &quot;jinaai/jina-embeddings-v3&quot;
if &#39;model&#39; not in locals() or &#39;tokenizer&#39; not in locals():
    print(&quot;Loading model and tokenizer...&quot;)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
else:
    print(&quot;Model and tokenizer are already loaded.&quot;)</code></pre>
<pre><code>Model and tokenizer are already loaded.</code></pre>
<pre class="python"><code># Function to normalize text to lowercase
def normalize_words(words):
    return {word.lower() for word in words}

# Normalize all words in the sets to lowercase
normalized_sets = {set_name: normalize_words(word_set) for set_name, word_set in sets.items()}

# Function to get embeddings for a list of words
def get_embeddings(words):
    inputs = tokenizer(list(words), padding=True, truncation=True, return_tensors=&#39;pt&#39;)
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1)  # Mean pooling
    return embeddings

# Create a dictionary to store the embeddings of each set
embeddings = {}
for set_name, word_set in normalized_sets.items():
    embeddings[set_name] = get_embeddings(word_set)

# Create a function to calculate the semantic similarity between sets
def compute_similarity(set1, set2):
    # Get the embeddings for both sets
    embeddings1 = embeddings[set1]
    embeddings2 = embeddings[set2]
    
    # Calculate cosine similarity between all pairs of words in set1 and set2
    sim_matrix = cosine_similarity(embeddings1, embeddings2)
    
    return sim_matrix

# Create a similarity matrix for each pair of sets
similarity_results = {}
for set1 in normalized_sets.keys():
    for set2 in normalized_sets.keys():
        if set1 != set2:
            sim_matrix = compute_similarity(set1, set2)
            similarity_results[(set1, set2)] = sim_matrix

# Create a simple table to store the similarity values
similarity_table = []

# Populate the table with word pairs and their cosine similarity values
for (set1, set2), sim_matrix in similarity_results.items():
    for i, word1 in enumerate(normalized_sets[set1]):
        for j, word2 in enumerate(normalized_sets[set2]):
            similarity_table.append({
                &quot;Set 1&quot;: set1,
                &quot;Word 1&quot;: word1,
                &quot;Set 2&quot;: set2,
                &quot;Word 2&quot;: word2,
                &quot;Cosine Similarity&quot;: sim_matrix[i, j]
            })

# Convert the table to a DataFrame for better display
similarity_df = pd.DataFrame(similarity_table)

# Filter the DataFrame to keep only cosine similarities above 0.7
similarity_df_filtered = similarity_df[similarity_df[&#39;Cosine Similarity&#39;] &gt; 0.7]

# Create an empty table to store the words that are similar
common_words_table = pd.DataFrame(index=sets.keys(), columns=sets.keys(), dtype=object)

# Populate the table with word pairs that have similarity above 0.7
for index, row in similarity_df_filtered.iterrows():
    set1 = row[&#39;Set 1&#39;]
    word1 = row[&#39;Word 1&#39;]
    set2 = row[&#39;Set 2&#39;]
    word2 = row[&#39;Word 2&#39;]
    
    # Check if the cell is empty or needs to be updated with word pairs
    if pd.isna(common_words_table.at[set1, set2]):
        common_words_table.at[set1, set2] = f&quot;{word1} - {word2}&quot;
    else:
        common_words_table.at[set1, set2] += f&quot;, {word1} - {word2}&quot;

# Display the table showing the common word pairs
print(common_words_table)</code></pre>
<pre><code>                                                         Bajta  ...                                              Usman
Bajta                                                      NaN  ...  estimate value - estimation entity.other, esti...
Britto_2017  technical factors - hardware, portability leve...  ...  entity count - estimation entity.other, entity...
Britto_2016  estimator &amp; provider - estimate value, relatio...  ...  semi-distributed - distributed: far offshore, ...
Dasthi       expert judgment - expert judgment, constructiv...  ...  expert judgment - expert judgement, analogy-ba...
Mendes       validated theoretically - not considered, vali...  ...  validated theoretically - not considered, vali...
Usman        ideal hours - effort hours, size.other - size ...  ...                                                NaN

[6 rows x 6 columns]</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-12-13.png" style="display: block; margin: auto;" /></p>
</div>
<div id="t-sne" class="section level1">
<h1>t sne</h1>
<pre class="python"><code>from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from adjustText import adjust_text
import numpy as np
import seaborn as sns


# Create a dictionary to store the embeddings of each set
embeddings = {}
all_words = []
word_to_set = {}
for set_name, word_set in normalized_sets.items():
    embeddings[set_name] = get_embeddings(word_set)
    all_words.extend(list(word_set))
    for word in word_set:
        word_to_set[word] = set_name

# Create an array of all embeddings
all_embeddings = torch.cat([embeddings[set_name] for set_name in normalized_sets], dim=0)

color_palette = sns.color_palette(&quot;Set2&quot;, n_colors=10)  # or however many colors you need
# Map sets to colors
set_colors = {set_name: sns.color_palette(&quot;Set2&quot;)[i] for i, set_name in enumerate(sets.keys())}
word_colors = [set_colors[word_to_set[word]] for word in all_words]

# Apply t-SNE to reduce the dimensionality of the embeddings to 2D
tsne = TSNE(n_components=2, random_state=5)
reduced_embeddings = tsne.fit_transform(all_embeddings)

# Initialize figure
plt.figure(figsize=(16, 12))

# Track words already labeled
labeled_words = {}

# Scatter plot with words colored by their set and label duplicates only once
for i, word in enumerate(all_words):
    # Color and position each word&#39;s dot
    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], 
                c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)
    
    # Check if the word has appeared before
    if word not in labeled_words:
        # If first occurrence, label it and choose red if shared
        color = &#39;red&#39; if all_words.count(word) &gt; 1 else &#39;black&#39;
        text = plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word.upper(), 
                        fontsize=5, color=color)
        labeled_words[word] = text  # Track labeled words for adjustText
    
# Adjust the positions of labels to avoid overlap
adjust_text(list(labeled_words.values()), only_move={&#39;points&#39;: &#39;xy&#39;}, force_text=0.75, expand_text=(1.5, 1.5))</code></pre>
<pre><code>([Text(26.23777825570876, -9.803137615748824, &#39;VARIATION REDUCTION&#39;), Text(5.668986464508116, -0.9618818010602794, &#39;CONCEPTUALIZATION&#39;), Text(10.670517436496674, -17.422911596298228, &#39;RISK&#39;), Text(7.859450822184158, -2.948335627147138, &#39;BASELINE COMPARISON&#39;), Text(-0.6487837475345941, 9.686803259168343, &#39;ESTIMATE VALUE&#39;), Text(-16.08302453679423, -18.811130966459007, &#39;TEMPORAL DISTANCE&#39;), Text(6.083562344697214, 4.731164155049008, &#39;NOT CONSIDERED&#39;), Text(-1.9643791397540795, -13.559389904567183, &#39;AGILE&#39;), Text(-3.5252142245923253, 7.871147087642122, &#39;GROUP-BASED ESTIMATION&#39;), Text(27.199368668448543, 12.960812405177514, &#39;CONSTRUCTIVE COST MODEL&#39;), Text(11.00232501995179, 22.831079598835522, &#39;CAPABILITY MATURITY MODEL INTEGRATION&#39;), Text(16.5532261012062, 2.0064638205936873, &#39;TELECOMMUNICATION&#39;), Text(10.851535155811618, -2.2264003651482795, &#39;HARDWARE&#39;), Text(19.102837498803297, 19.132841341836105, &#39;NEAR OFFSHORE&#39;), Text(10.75460337592709, 5.64644325120107, &#39;CBR&#39;), Text(3.036094795850019, -25.437125022070756, &#39;NON-MACHINE LEARNING&#39;), Text(19.9688637938807, 3.5485238347734622, &#39;DESIGN&#39;), Text(10.80347059599815, -5.790582752227792, &#39;MAINTAINABILITY&#39;), Text(13.913723474433354, 8.946959168570373, &#39;PORTFOLIO&#39;), Text(2.024887848438759, 19.651259585789255, &#39;STAFF/COST&#39;), Text(11.814826088028568, 0.3413833822522747, &#39;HEALTHCARE&#39;), Text(-5.6970720453416135, 18.1642674922943, &#39;SIZE REPORT&#39;), Text(3.0745494784078318, -0.7918192676135618, &#39;IMPLEMENTATION&#39;), Text(7.361674830105997, -0.044649064966620244, &#39;PERFORMANCE&#39;), Text(2.7325861341722586, 0.6708980492183017, &#39;EXECUTION&#39;), Text(0.23195610461696248, 0.9825280155454266, &#39;DELPHI&#39;), Text(16.625433644017868, 8.414392866407113, &#39;FINANCE&#39;), Text(9.8748745500503, -15.909456089564738, &#39;SENSITIVITY ANALYSIS&#39;), Text(-14.35012410936817, -17.068144845962532, &#39;GEOGRAPHICAL DISTANCE&#39;), Text(2.2006214666366617, -3.822023384911681, &#39;ANALYSIS&#39;), Text(7.396590021348771, 6.304213884898587, &#39;GA&#39;), Text(22.162817760475228, 9.342850194658538, &#39;PRELIMINARY PLANNING&#39;), Text(4.7885578142731475, -5.639115497044166, &#39;FEASIBILITY STUDY&#39;), Text(13.480318811055156, -4.702021878106265, &#39;AVAILABILITY&#39;), Text(3.1628186769254754, -3.4222940581185597, &#39;STATISTICS ANALYSIS&#39;), Text(4.501225507836189, 10.445148651940471, &#39;INDIVIDUAL&#39;), Text(13.857173449147133, -1.4623547009059479, &#39;SECURITY&#39;), Text(-15.01615921397363, -16.295290994644173, &#39;SOCIO-CULTURAL DISTANCE&#39;), Text(12.290995599146818, -7.096230309350158, &#39;RELIABILITY&#39;), Text(16.678051030597373, 5.444397878646839, &#39;COMMISSIONING&#39;), Text(-3.723073158341073, 16.32892985343932, &#39;OTHER&#39;), Text(6.86346847934108, 3.114164277485429, &#39;CONSIDERED&#39;), Text(7.0381142568203785, 5.074019105093811, &#39;SE&#39;), Text(-0.8257983468040351, 8.641533620016908, &#39;VALUE&#39;), Text(3.5649949885568333, -5.1106715679168815, &#39;SYSTEM INVESTIGATION&#39;), Text(-3.0453689905135874, -23.104136862073634, &#39;FUZZY SIMILAR&#39;), Text(22.103124007217346, 8.311317838941292, &#39;DETAIL PLANNING&#39;), Text(17.133474233611935, 19.28739324297223, &#39;FAR OFFSHORE&#39;), Text(8.605137316949914, -25.03319347245353, &#39;EXPERT JUDGMENT&#39;), Text(19.32960096693808, 17.338281631469712, &#39;CLOSE ONSHORE&#39;), Text(14.322714744306381, -14.551815148762302, &#39;TESTING&#39;), Text(21.08195763630252, 3.236464173453186, &#39;RESEARCH &amp; DEV&#39;), Text(1.3457922206771045, 15.264565093176692, &#39;EFFORT HOURS&#39;), Text(18.644894356535325, 18.599185173852092, &#39;DISTANT ONSHORE&#39;), Text(10.669496885499647, -3.1966028009142278, &#39;MAINTENANCE&#39;), Text(2.3741123500947054, -26.71874427795411, &#39;MACHINE LEARNING&#39;), Text(4.107782431533259, 22.003977380480073, &#39;NO OF TEAM MEMBERS&#39;), Text(-12.820596125433518, -4.24950866018024, &#39;CONNECTIVITY DENSITY&#39;), Text(6.923801015423194, -0.9101625476564763, &#39;IN-HOUSE EXPERIENCE&#39;), Text(-7.819336019446766, 11.762549863542816, &#39;ENTITY COUNT&#39;), Text(-17.85465584647271, 7.087687608173908, &#39;COMPONENT COUNT&#39;), Text(-23.79105632570482, 6.965702486038197, &#39;NODE SLOT SIZE&#39;), Text(-0.5853178882598868, 0.1669751857008137, &#39;SPI PROGRAM&#39;), Text(9.150556000548022, -11.739705644335075, &#39;USABILITY LEVEL&#39;), Text(10.817524171644642, -2.2461750405175422, &#39;TECHNICAL FACTORS&#39;), Text(-9.035860669189887, 6.216085931232989, &#39;CONCERN OPERATION COUNT&#39;), Text(-19.026058704430056, -5.621379886354724, &#39;INTERFACE COMPLEXITY&#39;), Text(-22.39143486338277, 7.546352202551695, &#39;CLUSTER NODE SIZE&#39;), Text(-18.306190983710746, 3.204059239796223, &#39;SEMANTIC ASSOCIATION COUNT&#39;), Text(9.787364626853702, -10.03722230366299, &#39;PORTABILITY LEVEL&#39;), Text(-14.681823239787931, 10.350007077625811, &#39;MODULE ATTRIBUTE COUNT&#39;), Text(-11.659847990966615, 9.15920173781258, &#39;MODULARITY LEVEL&#39;), Text(10.885734228357194, -16.645201962334777, &#39;RISK LEVEL&#39;), Text(-12.290100408561766, 8.536635283061429, &#39;CONCERN MODULE COUNT&#39;), Text(-6.202852824811011, 24.49768673351832, &#39;METRICS PROGRAM&#39;), Text(5.3871624320553195, -14.972527572086886, &#39;PROJECT.INFRASTRUCTURE&#39;), Text(-21.73909503198439, -5.996755783898493, &#39;CONTROL FLOW COMPLEXITY&#39;), Text(9.106304721332364, -8.940881633758544, &#39;SCALABILITY LEVEL&#39;), Text(-24.555031743818713, 10.400845548084789, &#39;INFORMATION SLOT COUNT&#39;), Text(24.563175424298933, -7.475522020884931, &#39;PLATFORM VOLATILITY LEVEL&#39;), Text(-16.315809123362268, 7.346554756164544, &#39;REUSED COMPONENT COUNT&#39;), Text(6.554368308282669, 22.6282763821738, &#39;TEAM CAPABILITY&#39;), Text(-12.202497629311779, 4.9080634798322365, &#39;STATEMENT COUNT&#39;), Text(18.682306752820168, -9.903073147365035, &#39;DOCUMENTATION LEVEL&#39;), Text(20.93073423024147, -5.521265343257369, &#39;OO EXPERIENCE LEVEL&#39;), Text(-21.99563079299465, 0.10261452219315004, &#39;MODEL LINK COMPLEXITY&#39;), Text(-14.551698610551895, 17.503090463365815, &#39;MEDIA COUNT&#39;), Text(-13.05260788540686, 11.045213651657093, &#39;MODULE POINT CUT COUNT&#39;), Text(17.630540661542653, -4.315690667288649, &#39;AUTHORING TOOL TYPE&#39;), Text(-13.605318504918007, -9.305899456569133, &#39;WEB PAGE ALLOCATION&#39;), Text(4.37485942821349, -13.443258333206181, &#39;PROJECT.TYPE&#39;), Text(-14.695792916513255, -8.22002863202777, &#39;WEB PAGE COUNT&#39;), Text(0.04438974734275547, -6.541668299266277, &#39;LESSONS LEARNED REPOSITORY&#39;), Text(9.311672049914641, -13.444960662296847, &#39;READABILITY LEVEL&#39;), Text(-5.356844094722497, 4.7474145889282084, &#39;CLASS COUPLING&#39;), Text(-17.377232614678718, -4.900584534236373, &#39;ADAPTATION COMPLEXITY&#39;), Text(0.6577502424101738, -2.4207793031420124, &#39;MAPPED WORKFLOWS&#39;), Text(-20.69601157949817, 8.742047030585148, &#39;CLUSTER COUNT&#39;), Text(-12.8202534628299, 3.826209940229134, &#39;COMMENT COUNT&#39;), Text(-14.271143292457829, 16.545285620008237, &#39;REUSED MEDIA COUNT&#39;), Text(-23.510822321714894, 11.065748725618626, &#39;ASSOCIATION SLOT SIZE&#39;), Text(24.597407249712177, -9.11453819274903, &#39;DESIGN VOLATILITY&#39;), Text(-14.812227957479415, -3.045790672302253, &#39;COHESION&#39;), Text(-10.132183948486084, 1.1549533230917675, &#39;PROGRAM COUNT&#39;), Text(-1.9177024632884638, 3.752986342566345, &#39;TIME EFFICIENCY LEVEL&#39;), Text(-18.609067894258807, 13.812320729664386, &#39;REUSED HIGH FEATURE COUNT&#39;), Text(-8.667889844083021, 12.02959665570939, &#39;PUBLISHING UNIT COUNT&#39;), Text(-13.243004359506788, 12.850810119083938, &#39;DIFFUSION CUT COUNT&#39;), Text(8.744099970594533, 1.5144412074770202, &#39;FOCUS FACTOR&#39;), Text(-17.903279622947014, -0.508371881076279, &#39;CLASS COMPLEXITY&#39;), Text(5.747424169548097, 20.860971103395727, &#39;WORK TEAM LEVEL&#39;), Text(7.342324671860666, 14.079799209322239, &#39;TIME RESTRICTION&#39;), Text(-16.096459534437425, -1.4171016011919377, &#39;CYCLOMATIC COMPLEXITY&#39;), Text(-16.97969738825675, 9.14267852646961, &#39;ATTRIBUTE COUNT&#39;), Text(-8.632316389083861, -10.229566226686757, &#39;WEB OBJECTS&#39;), Text(-2.3553371328692236, -0.6473725880895387, &#39;COMMUNICATION LEVEL&#39;), Text(-4.415422814392272, -3.4608144078936007, &#39;USE CASE COUNT&#39;), Text(21.477031441273233, -11.377008880887718, &#39;REQUIREMENTS NOVELTY LEVEL&#39;), Text(-10.985156935914869, -0.504913939748505, &#39;SERVER SCRIPT COUNT&#39;), Text(-9.094688352269511, -9.643656778335579, &#39;DATA WEB POINTS&#39;), Text(-1.085298111361837, 2.0678772794348745, &#39;PRODUCTIVITY LEVEL&#39;), Text(-4.382123289954279, 1.3872711334909624, &#39;NUMBER OF PROJECTS IN PARALLEL&#39;), Text(13.173216924975002, -12.457532324109767, &#39;TRAINABILITY LEVEL&#39;), Text(-19.186759561300278, 13.10692289216177, &#39;REUSED LOW FEATURE COUNT&#39;), Text(-13.303614087527798, -8.227354873929713, &#39;NEW WEB PAGE COUNT&#39;), Text(-21.26538530472786, -0.32749690839223433, &#39;MODEL COLLECTION COMPLEXITY&#39;), Text(-26.435489687650435, 8.54680044991629, &#39;COLLECTION SLOT SIZE&#39;), Text(3.2885670562328855, -11.492037957055235, &#39;IT LITERACY&#39;), Text(-23.989659028322468, 3.345436259678422, &#39;COMPONENT GRANULARITY LEVEL&#39;), Text(-16.302506834114748, -4.496706846782146, &#39;DIFFICULTY LEVEL&#39;), Text(26.33626501225656, 0.3911565847694831, &#39;INTERNATIONAL FUNCTION POINT USERS GROUP&#39;), Text(-8.285009857800702, -4.354197025299079, &#39;LINES OF CODE&#39;), Text(-22.322643540290095, -4.123978614807136, &#39;DATA FLOW COMPLEXITY&#39;), Text(-25.352338976090955, 8.330713830675386, &#39;COMPONENT SLOT COUNT&#39;), Text(-14.94879075854055, 6.082502412796011, &#39;SEGMENT COUNT&#39;), Text(22.920666940635257, 4.975251034327911, &#39;ARCHITECTURE&#39;), Text(20.023298919431625, -5.863184847150535, &#39;DOMAIN EXPERIENCE LEVEL&#39;), Text(4.840875406880535, -7.504638310841159, &#39;INTEGRATION WITH LEGACY SYSTEMS&#39;), Text(-8.938862671621386, 0.1858319589069879, &#39;REUSED PROGRAM COUNT&#39;), Text(14.37060828666533, -8.962880202702124, &#39;STABILITY LEVEL&#39;), Text(14.993064403610845, -11.127385323388246, &#39;FLEXIBILITY LEVEL&#39;), Text(27.885680816365834, 7.1279666355677875, &#39;MOTIVATION LEVEL&#39;), Text(-25.327671786892797, 6.79999275888715, &#39;MODEL SLOT SIZE&#39;), Text(-8.909356702100844, 7.668269109725941, &#39;INNER/SUB CONCERN COUNT&#39;), Text(20.613260162476585, -11.122147396632613, &#39;REQUIREMENTS CLARITY LEVEL&#39;), Text(-10.158013600957005, 5.0319533688681375, &#39;OPERATION COUNT&#39;), Text(-22.29622125306437, -3.70703338895526, &#39;DATA USAGE COMPLEXITY&#39;), Text(-18.32348666475665, 4.08293036052158, &#39;LINK COUNT&#39;), Text(-15.150417911660284, -0.21757234632970324, &#39;COMPACTNESS&#39;), Text(-23.542410796611538, 11.27014278684343, &#39;ASSOCIATION CENTER SLOT COUNT&#39;), Text(-12.421096207826366, 16.384705707005082, &#39;NEW MEDIA COUNT&#39;), Text(-12.157068781275903, 18.774203252792347, &#39;MEDIA ALLOCATION&#39;), Text(-13.742985182769836, -3.540137243270884, &#39;COHESION COMPLEXITY&#39;), Text(20.800330022573476, -1.7683681964874296, &#39;PRODUCT.TYPE&#39;), Text(-21.328281710378583, 9.704920952660686, &#39;CLUSTER SLOT COUNT&#39;), Text(-0.589068349957472, 2.831806618826718, &#39;PROCESS EFFICIENCY LEVEL&#39;), Text(-3.621142412962456, 0.9990387086357586, &#39;CONCURRENCY LEVEL&#39;), Text(-5.487120862314775, 5.477396835599613, &#39;CONCERN COUPLING&#39;), Text(14.46802194479973, 11.150155694144104, &#39;STRATUM&#39;), Text(-3.153220496369947, -11.100614779336123, &#39;RAPID APP DEVELOPMENT&#39;), Text(3.648639709372681, 2.395187064579545, &#39;OPERATIONAL MODE&#39;), Text(14.212495233820327, -5.236560392379765, &#39;AVAILABILITY LEVEL&#39;), Text(14.40941275769665, -13.595345285960615, &#39;TESTABILITY LEVEL&#39;), Text(-17.572964746452143, -6.827917739323219, &#39;PAGE COMPLEXITY&#39;), Text(2.9940514089215213, 9.010731908253248, &#39;PERSONALITY&#39;), Text(-17.52568916128528, 11.362199619838158, &#39;FEATURE COUNT&#39;), Text(-13.47339189264082, 3.3548599992479495, &#39;REUSED COMMENT COUNT&#39;), Text(-20.628448051368036, -2.591148580823635, &#39;OUTPUT COMPLEXITY&#39;), Text(9.871289955992857, -2.467460897990648, &#39;PROCESSING REQUIREMENTS&#39;), Text(-9.06935895385281, 1.9102842330932575, &#39;NUMBER OF PROGRAMMING LANGUAGES&#39;), Text(-14.949343455991432, 5.611765398297983, &#39;SECTION COUNT&#39;), Text(-19.18889774795501, 11.863909278597141, &#39;LOW FEATURE COUNT&#39;), Text(-18.912723670736437, -3.4017595427377003, &#39;NEW COMPLEXITY&#39;), Text(-19.667480136386807, -4.555494533266348, &#39;INPUT COMPLEXITY&#39;), Text(-19.491798096049216, -1.0842363085065614, &#39;COMPONENT COMPLEXITY&#39;), Text(27.485657475071577, -2.0142428874969553, &#39;OBJECT-ORIENTED FUNCTION POINTS&#39;), Text(20.658862723612017, -6.756678942271648, &#39;DEPLOYMENT PLATFORM EXPERIENCE LEVEL&#39;), Text(-10.19247690012378, 18.24444077355519, &#39;REUSED MEDIA ALLOCATION&#39;), Text(11.406028632502405, -12.07602675301689, &#39;ACCESSIBILITY LEVEL&#39;), Text(24.364022446524714, -9.11139202117921, &#39;REQUIREMENTS VOLATILITY LEVEL&#39;), Text(7.267123894537647, -9.913972084862852, &#39;PROJECT MANAGEMENT LEVEL&#39;), Text(-9.414748220520636, 12.041543333871012, &#39;PUBLISHING MODEL UNIT COUNT&#39;), Text(-8.197401441566406, 16.13471715109688, &#39;DATABASE SIZE&#39;), Text(13.852979834079747, -9.359414148330696, &#39;ROBUSTNESS LEVEL&#39;), Text(11.97493802978147, -9.898379605157043, &#39;INSTALLABILITY LEVEL&#39;), Text(4.098182899307822, -10.09139416558402, &#39;PROGRAMMING LANGUAGE EXPERIENCE LEVEL&#39;), Text(-9.048290422808734, 17.102288968222467, &#39;STORAGE CONSTRAINT&#39;), Text(9.147013845828276, 13.629891511372143, &#39;DEVELOPMENT RESTRICTION&#39;), Text(2.7576149339445237, 23.25478262901305, &#39;TEAM SIZE&#39;), Text(-21.351305480926268, 1.4115358420780666, &#39;MODEL ASSOCIATION COMPLEXITY&#39;), Text(-20.91889901103512, 5.296316358021322, &#39;MODEL NODE SIZE&#39;), Text(21.981741105587247, -3.4264798573085358, &#39;PRODUCT.EXPERIENCE LEVEL&#39;), Text(22.307816889093772, -13.047053173610152, &#39;NOVELTY LEVEL&#39;), Text(-2.657488893578133, 3.9995649201529275, &#39;MEMORY EFFICIENCY LEVEL&#39;), Text(-21.44704967341115, 7.6438614913395355, &#39;SLOT COUNT&#39;), Text(-14.320475684019826, 10.264226933888011, &#39;MODULE COUNT&#39;), Text(14.45536901074071, -2.785510376521529, &#39;SECURITY LEVEL&#39;), Text(18.7533696599545, -4.580140474864422, &#39;TOOL EXPERIENCE LEVEL&#39;), Text(2.7922008017186286, -10.164565086364753, &#39;SOFTWARE DEVELOPMENT EXPERIENCE&#39;), Text(-8.242080093891389, 6.710250541142052, &#39;INDIFFERENT CONCERN COUNT&#39;), Text(-19.435956354794964, -1.7648773329598697, &#39;TOTAL COMPLEXITY&#39;), Text(8.551261691662575, -6.93708058084761, &#39;QUALITY LEVEL&#39;), Text(26.36445174840189, -2.58837579999652, &#39;OBJECT-ORIENTED HEURISTIC FUNCTION POINTS&#39;), Text(-20.226274550999364, 5.363964966365259, &#39;NODE COUNT&#39;), Text(-13.830283366403272, 19.384441491535718, &#39;MEDIA DURATION&#39;), Text(10.703538448541394, -7.372291101728177, &#39;MAINTAINABILITY LEVEL&#39;), Text(-7.435568280643032, -5.110370029721949, &#39;REUSED LINES OF CODE&#39;), Text(21.613652385819343, -12.986526673180723, &#39;INNOVATION LEVEL&#39;), Text(16.467414585724967, -6.969902202061252, &#39;PLATFORM SUPPORT LEVEL&#39;), Text(-24.646719183421904, 5.0913640703473675, &#39;SLOT GRANULARITY LEVEL&#39;), Text(12.388366672992703, -7.9109436375754285, &#39;RELIABILITY LEVEL&#39;), Text(7.269491021133241, -10.992752238682346, &#39;REUSABILITY LEVEL&#39;), Text(20.062206703270633, -0.8790441887719354, &#39;PRODUCT.STRUCTURE&#39;), Text(-1.2572647653087472, 25.519257729394084, &#39;COMMON SOFTWARE MEASUREMENT INTERNATIONAL CONSORTIUM&#39;), Text(-25.24289970313349, 9.664206293651027, &#39;COLLECTION CENTER SLOT COUNT&#39;), Text(-10.853850418444605, -1.630284064156676, &#39;CLIENT SCRIPT COUNT&#39;), Text(-1.8459315599933745, -9.031904384068088, &#39;SOFTWARE REUSE&#39;), Text(-17.900534267463993, 4.944961091450267, &#39;ANCHOR COUNT&#39;), Text(-18.387228038311004, 12.418434374673012, &#39;HIGH FEATURE COUNT&#39;), Text(-18.56570147272079, -6.709517792293013, &#39;LAYOUT COMPLEXITY&#39;), Text(6.8370428749053715, -8.123972940444954, &#39;RESOURCE LEVEL&#39;), Text(10.871809729799153, 11.469566249847404, &#39;SEMI-DISTRIBUTED&#39;), Text(-2.139812655256641, 10.399422809055864, &#39;ESTIMATOR &amp; PROVIDER&#39;), Text(12.852450974064496, 4.8887695789337045, &#39;PROVIDER&#39;), Text(-12.458826644420625, -18.383421060017184, &#39;RELATIONSHIP.LOCATION&#39;), Text(-11.462614718944796, -19.888920736312873, &#39;SITE.LOCATION&#39;), Text(11.504658542025481, 9.38052998270306, &#39;CENTRALIZED&#39;), Text(8.37820924174401, 16.446402386256615, &#39;LATE&#39;), Text(-11.91641769189988, -18.273137930461345, &#39;RELATIONSHIP.GEOGRAPHIC DISTANCE&#39;), Text(9.849848090871696, 16.94888368334088, &#39;EARLY &amp; LATE&#39;), Text(-8.044017177166474, -16.802095508575448, &#39;RELATIONSHIP.LEGAL ENTITY&#39;), Text(-11.927537633257522, -20.21397760254997, &#39;SITE.GEOGRAPHIC DISTANCE&#39;), Text(-4.124444313856863, 10.33715652738298, &#39;ESTIMATOR&#39;), Text(9.2779059328956, 17.533532094955433, &#39;EARLY&#39;), Text(-13.976897145125168, -20.717192833764223, &#39;SITE.TEMPORAL DISTANCE&#39;), Text(-12.842006136294334, -18.5626497745514, &#39;RELATIONSHIP.TEMPORAL DISTANCE&#39;), Text(10.854759909722112, 10.120047964368538, &#39;DISTRIBUTED&#39;), Text(-7.226047107135095, -16.87633782114302, &#39;SITE.LEGAL ENTITY&#39;), Text(2.1676830831650804, 4.498645816530491, &#39;BASIC-COMBINATION&#39;), Text(0.14150682714677743, 5.398465013504001, &#39;AI-COMBINED HYBRID&#39;), Text(2.1587102999610295, -9.743046365465446, &#39;SOFTWARE LIFE CYCLE MANAGEMENT&#39;), Text(15.691850053533436, -18.85783802441189, &#39;SOFTWARE EVALUATION AND ESTIMATION FOR RISK&#39;), Text(3.6718150115397634, -27.37692944662912, &#39;ARTIFICIAL NEURAL NETWORKS&#39;), Text(-2.936909967160993, -23.398206268038074, &#39;FUZZY LOGIC&#39;), Text(-4.211215385275501, -18.8950260434832, &#39;ANALOGY-BASED&#39;), Text(1.7640690210173204, -27.84824248041426, &#39;COMPUTATIONAL INTELLIGENCE.SWARM&#39;), Text(3.5857707047847, -28.702150133677897, &#39;COMPUTATIONAL INTELLIGENCE.EVOLUTIONARY&#39;), Text(6.555332023712893, -4.99678575651987, &#39;VALIDATED THEORETICALLY&#39;), Text(6.1868909751215355, 6.288159138815736, &#39;ABSOLUTE&#39;), Text(-5.483474123747115, -10.520995862143387, &#39;WEB SOFTWARE APPLICATION&#39;), Text(27.30453713753147, 7.2703777858189085, &#39;MOTIVATION&#39;), Text(7.091565655931355, 9.455204916000362, &#39;RATIO&#39;), Text(6.528944502415193, -4.54156160354615, &#39;VALIDATED EMPIRICALLY&#39;), Text(17.014378779242115, -17.416837113244203, &#39;VALIDATION.BOTH&#39;), Text(4.162870948960702, -0.008177802392424383, &#39;FUNCTIONALITY&#39;), Text(-7.781135547122645, -1.3298755543572653, &#39;PROGRAM/SCRIPT&#39;), Text(-1.8314046007587095, 24.154380355562473, &#39;SOLUTION-ORIENTED METRIC&#39;), Text(-5.744519217975679, -11.83674391337804, &#39;WEB HYPERMEDIA APPLICATION&#39;), Text(-4.725949671576096, 20.091074105671467, &#39;EARLY SIZE METRIC&#39;), Text(-17.80918247861247, -2.4464256150382013, &#39;COMPLEXITY&#39;), Text(23.74556432193326, -18.7176594393594, &#39;MODEL DEPENDENCY.SPECIFIC&#39;), Text(-17.958553381735275, 1.4314874018941524, &#39;CLASS.LENGTH&#39;), Text(-6.6177883379305555, -11.101704992566798, &#39;WEB APPLICATION&#39;), Text(16.1899988420548, -24.68976637295315, &#39;COMPUTATION.DIRECT&#39;), Text(15.972306001917005, -24.694200958524437, &#39;COMPUTATION.INDIRECT&#39;), Text(24.203359427528994, -18.730653251920437, &#39;MODEL DEPENDENCY.NONSPECIFIC&#39;), Text(-2.026838542799794, 24.234844439370285, &#39;PROBLEM-ORIENTED METRIC&#39;), Text(2.2976816601907117, 7.275219392776478, &#39;ORDINAL&#39;), Text(4.488425211675711, 12.47677791459219, &#39;INTERVAL&#39;), Text(-4.642829850527548, 20.885988024302883, &#39;LATE SIZE METRIC&#39;), Text(3.4832635927200357, 6.855403481210992, &#39;NOMINAL&#39;), Text(17.079918063571384, -17.572916800635213, &#39;VALIDATION.NONE&#39;), Text(-14.801300454947256, 20.324015126909515, &#39;MEDIA&#39;), Text(16.981866809668077, 6.937427949905384, &#39;BIDDING&#39;), Text(3.290485611769462, 17.326931462969085, &#39;PAIR DAYS&#39;), Text(5.748727130236162, -19.553209284373697, &#39;MEAN MAGNITUDE OF RELATIVE ERROR&#39;), Text(27.275822883767475, 2.742544923509861, &#39;CUSTOMIZED EXTREME PROGRAMMING&#39;), Text(4.265947033474525, 15.320250531605303, &#39;IDEAL HOURS&#39;), Text(-4.347663212283962, 17.207993459701534, &#39;SIZE.OTHER&#39;), Text(0.5510592795956555, 13.54746806962148, &#39;EFFORT ESTIMATE.TYPE.OTHER&#39;), Text(5.195153525298643, -20.881417901175368, &#39;MEDIAN MAGNITUDE OF RELATIVE ERROR&#39;), Text(-1.2254105151853238, -4.05051336288453, &#39;STORY POINTS&#39;), Text(-4.070704097205599, -4.026751945699999, &#39;USE CASE&#39;), Text(3.9697532297718965, 21.437151956558218, &#39;NO. OF TEAM MEMBERS&#39;), Text(13.363793607796389, 5.794199419021595, &#39;SPRINT&#39;), Text(17.760351540926962, 16.505823693956636, &#39;DISTRIBUTED: FAR OFFSHORE&#39;), Text(-4.220448330563883, -6.002233818599166, &#39;USE CASE POINTS METHOD&#39;), Text(20.08162418888461, 15.997310965401773, &#39;DISTRIBUTED: DISTANT ONSHORE&#39;), Text(-17.008893392739758, 14.291151210239946, &#39;FEATURE-DRIVEN DEVELOPMENT&#39;), Text(-3.3528422401412783, -18.911207989283977, &#39;ANALOGY&#39;), Text(25.385015310010616, -1.326535538264693, &#39;FUNCTION POINTS&#39;), Text(-0.7798568344500723, -15.235455186026442, &#39;CUSTOMIZED SCRUM&#39;), Text(11.681437620693643, 2.0529753412519085, &#39;EDUCATION&#39;), Text(8.737499757555227, 3.273461096627365, &#39;POINT&#39;), Text(3.479564664286954, 15.562212896347035, &#39;HOURS/DAYS&#39;), Text(16.41627243757248, 0.05137479986462523, &#39;GOVERNMENT/MILITARY&#39;), Text(3.734505423115145, 1.5598209687641642, &#39;TASK&#39;), Text(10.113321167307511, 7.779847730909097, &#39;CO-LOCATED&#39;), Text(19.503634643093235, 17.434269857406605, &#39;DISTRIBUTED: NEAR OFFSHORE&#39;), Text(13.030265006519137, 12.742600393295284, &#39;CRYSTAL&#39;), Text(7.130999817790531, -19.164109856741778, &#39;ACCURACY LEVEL.VALUE&#39;), Text(16.94812073553763, 3.524660771233684, &#39;COMMUNICATIONS INDUSTRY&#39;), Text(-1.188287469571634, 13.116126966476429, &#39;ESTIMATED ACTIVITIES.ALL&#39;), Text(9.954699279800543, 3.3020765032086956, &#39;THREE POINT&#39;), Text(20.899327766356926, -11.924221713202346, &#39;NON FUNCTIONAL REQUIREMENTS.OTHER&#39;), Text(3.6949338419206654, -14.867075245721004, &#39;PROJECT DOMAIN.OTHER&#39;), Text(-2.6342676104268676, 12.307697248458851, &#39;ESTIMATION ENTITY.OTHER&#39;), Text(3.07544324905642, 24.345612246649594, &#39;GROUP&#39;), Text(13.864936833035564, 3.710522277014583, &#39;TRANSPORTATION&#39;), Text(8.647430448109105, 10.025895908900658, &#39;DISTRIBUTION&#39;), Text(-2.968068532597634, 10.780715315682542, &#39;NUMBER OF ENTITIES ESTIMATED.VALUE&#39;), Text(-1.2464644980815152, 8.439604296003054, &#39;ESTIMATION TECHNIQUES.OTHER&#39;), Text(19.12646699397795, 1.4617659194128763, &#39;RETAIL/WHOLESALE&#39;), Text(3.8733679247287043, 4.720818274361733, &#39;NOT USED&#39;), Text(5.301880004325227, -20.917663247244704, &#39;BIAS OF RELATIVE ERROR&#39;), Text(20.844777698709116, 15.664641312190462, &#39;DISTRIBUTED: CLOSE ONSHORE&#39;), Text(12.074107770035354, 7.212752505711137, &#39;KANBAN&#39;), Text(21.446172619173602, 9.993448209762562, &#39;PLANNING POKER&#39;), Text(7.436119127150512, -19.340243407658175, &#39;ACCURACY MEASURE.OTHER&#39;), Text(4.438142003705423, -5.292675951548993, &#39;DYNAMIC SYSTEMS DEVELOPMENT METHOD&#39;), Text(-3.6125941025826194, 15.626691096169594, &#39;UNIT.OTHER&#39;), Text(9.218025964767705, -23.986800241470345, &#39;EXPERT JUDGEMENT&#39;), Text(3.8518619456214296, 16.343848623548226, &#39;DAILY&#39;), Text(-1.5927427602967867, -5.120809800284256, &#39;USER STORY&#39;), Text(-0.893748441626947, 9.684635162353509, &#39;ESTIMATE VALUE(S)&#39;), Text(17.07300192794493, 1.748846731867097, &#39;MANUFACTURING&#39;), Text(7.671038034423702, 11.028064680099476, &#39;RELEASE&#39;), Text(5.079032187731038, 10.19367471422467, &#39;SINGLE&#39;), Text(12.061371759522352, 0.06774090869085114, &#39;HEALTH&#39;), Text(-4.717669993831265, -5.841846418380747, &#39;USER CASE POINTS&#39;), Text(-2.2301119474057245, 21.79779164450509, &#39;CONSIDERED WITHOUT ANY METRIC&#39;), Text(28.628786218743173, 2.779444370950962, &#39;EXTREME PROGRAMMING&#39;), Text(17.025626452315244, 9.315293775285987, &#39;FINANCIAL&#39;), Text(5.7936791579569515, 6.080031299591056, &#39;NOT APPLICABLE&#39;), Text(-1.8309785074187843, -14.253485284532829, &#39;SCRUM&#39;), Text(1.4226128461668566, 13.830653980800072, &#39;ACTUAL EFFORT.VALUE&#39;)], [])</code></pre>
<pre class="python"><code># First legend for word sets
handles = [plt.Line2D([0], [0], marker=&#39;o&#39;, color=&#39;w&#39;, markerfacecolor=color, markersize=10) for color in set_colors.values()]
labels = list(sets.keys())
legend1 = plt.legend(handles=handles, labels=labels, title=&quot;Literature&quot;, loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.05), ncol=6)

# Second legend for duplicate words
duplicate_legend = plt.Line2D([0], [1], color=&#39;red&#39;, lw=2)
legend2 = plt.legend([duplicate_legend], [&quot;In red: duplicate words&quot;], loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.12), frameon=False)

# Re-add the first legend
plt.gca().add_artist(legend1)

plt.title(&quot;t-SNE Visualization of Word Embeddings&quot;)
plt.xlabel(&quot;t-SNE Component 1&quot;)
plt.ylabel(&quot;t-SNE Component 2&quot;)

# Save the plot as an image
plt.savefig(&#39;tsne_word_embeddings.png&#39;, dpi=600, bbox_inches=&#39;tight&#39;)

# Display the plot
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-13-15.png" style="display: block; margin: auto;" /></p>
<pre class="python"><code>from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from adjustText import adjust_text
import numpy as np
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools

# Create a dictionary to store the embeddings of each set
embeddings = {}
all_words = []
word_to_set = {}
for set_name, word_set in normalized_sets.items():
    embeddings[set_name] = get_embeddings(word_set)
    all_words.extend(list(word_set))
    for word in word_set:
        word_to_set[word] = set_name

# Create an array of all embeddings
all_embeddings = torch.cat([embeddings[set_name] for set_name in normalized_sets], dim=0)

# Map sets to colors
set_colors = {set_name: sns.color_palette(&quot;Set2&quot;)[i] for i, set_name in enumerate(sets.keys())}
word_colors = [set_colors[word_to_set[word]] for word in all_words]

# Apply t-SNE to reduce the dimensionality of the embeddings to 3D
tsne = TSNE(n_components=3, random_state=5)
reduced_embeddings = tsne.fit_transform(all_embeddings)

# Initialize figure for 3D plot
fig = plt.figure(figsize=(16, 12))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)

# Track words already labeled
labeled_words = {}

# Scatter plot with words colored by their set and label duplicates only once
for i, word in enumerate(all_words):
    # Color and position each word&#39;s dot in 3D
    ax.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], 
               c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)
    
    # Check if the word has appeared before
    if word not in labeled_words:
        # If first occurrence, label it and choose red if shared
        color = &#39;red&#39; if all_words.count(word) &gt; 1 else &#39;black&#39;
        ax.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], 
                word.upper(), fontsize=5, color=color)
        labeled_words[word] = True  # Track labeled words for avoid overlap
    
# Adjust the positions of labels to avoid overlap (this part doesn&#39;t adjust in 3D directly, 
# but you could explore 3D label adjustments using other techniques like manually adjusting the positions)
# For now, we keep the label text without adjustment in 3D (more complex adjustments can be done with other libraries).

# First legend for word sets
handles = [plt.Line2D([0], [0], marker=&#39;o&#39;, color=&#39;w&#39;, markerfacecolor=color, markersize=10) for color in set_colors.values()]
labels = list(sets.keys())
legend1 = plt.legend(handles=handles, labels=labels, title=&quot;Literature&quot;, loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.05), ncol=6)

# Second legend for duplicate words
duplicate_legend = plt.Line2D([0], [1], color=&#39;red&#39;, lw=2)
legend2 = plt.legend([duplicate_legend], [&quot;In red: duplicate words&quot;], loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.12), frameon=False)

# Re-add the first legend
plt.gca().add_artist(legend1)

ax.set_title(&quot;t-SNE Visualization of Word Embeddings in 3D&quot;)
ax.set_xlabel(&quot;t-SNE Component 1&quot;)
ax.set_ylabel(&quot;t-SNE Component 2&quot;)
ax.set_zlabel(&quot;t-SNE Component 3&quot;)

# Save the plot as an image
plt.savefig(&#39;tsne_word_embeddings_3d.png&#39;, dpi=600, bbox_inches=&#39;tight&#39;)

# Display the plot
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-14-17.png" style="display: block; margin: auto;" /></p>
</div>
<div id="same-as-before-but-umap" class="section level1">
<h1>Same as before but UMAP</h1>
<pre class="python"><code>import matplotlib.pyplot as plt
from adjustText import adjust_text
import numpy as np
import umap.umap_ as umap

# Apply UMAP to reduce the dimensionality of the embeddings to 2D
umap_model = umap.UMAP(n_components=2, random_state=5)
reduced_embeddings = umap_model.fit_transform(all_embeddings)</code></pre>
<pre><code>C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\sklearn\utils\deprecation.py:151: FutureWarning:

&#39;force_all_finite&#39; was renamed to &#39;ensure_all_finite&#39; in 1.6 and will be removed in 1.8.

C:\Users\mysit\AppData\Local\Programs\Python\PYTHON~1\Lib\site-packages\umap\umap_.py:1952: UserWarning:

n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.</code></pre>
<pre class="python"><code># Initialize figure
plt.figure(figsize=(16, 12))

# Track words already labeled
labeled_words = {}

# Scatter plot with words colored by their set and label duplicates only once
for i, word in enumerate(all_words):
    # Color and position each word&#39;s dot
    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], 
                c=[set_colors[word_to_set[word]]], s=50, alpha=0.6)
    
    # Check if the word has appeared before
    if word not in labeled_words:
        # If first occurrence, label it and choose red if shared
        color = &#39;red&#39; if all_words.count(word) &gt; 1 else &#39;black&#39;
        text = plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word.upper(), 
                        fontsize=5, color=color)
        labeled_words[word] = text  # Track labeled words for adjustText
    
# Adjust the positions of labels to avoid overlap
adjust_text(list(labeled_words.values()), only_move={&#39;points&#39;: &#39;xy&#39;}, force_text=0.75, expand_text=(1.5, 1.5))</code></pre>
<pre><code>([Text(13.999819766936763, 18.336604021844412, &#39;VARIATION REDUCTION&#39;), Text(12.894391602794489, 16.259759709948586, &#39;CONCEPTUALIZATION&#39;), Text(14.5316600193588, 17.8551274696986, &#39;RISK&#39;), Text(13.334371254434508, 15.708178809710912, &#39;BASELINE COMPARISON&#39;), Text(12.531293733182453, 13.539640388034641, &#39;ESTIMATE VALUE&#39;), Text(15.186530368207922, 14.430005988620579, &#39;TEMPORAL DISTANCE&#39;), Text(13.630739966431452, 15.202204511279152, &#39;NOT CONSIDERED&#39;), Text(12.893789389799199, 16.6720113947278, &#39;AGILE&#39;), Text(12.43483752553261, 13.407723484720504, &#39;GROUP-BASED ESTIMATION&#39;), Text(13.579855651687229, 17.324942723910016, &#39;CONSTRUCTIVE COST MODEL&#39;), Text(14.339674828852855, 18.75104234672728, &#39;CAPABILITY MATURITY MODEL INTEGRATION&#39;), Text(14.372259715062476, 17.210296708061584, &#39;TELECOMMUNICATION&#39;), Text(13.908441679816573, 17.51816286245982, &#39;HARDWARE&#39;), Text(15.799295262187238, 16.014352172896977, &#39;NEAR OFFSHORE&#39;), Text(14.336361529221456, 16.536341802279157, &#39;CBR&#39;), Text(12.855883955393105, 15.545323120980038, &#39;NON-MACHINE LEARNING&#39;), Text(13.306409560081459, 17.139079074632555, &#39;DESIGN&#39;), Text(14.858363188555643, 18.67108263174693, &#39;MAINTAINABILITY&#39;), Text(13.643411638383423, 16.465111520176848, &#39;PORTFOLIO&#39;), Text(11.157118623646035, 14.250662726447699, &#39;STAFF/COST&#39;), Text(14.626872860188444, 17.291191023872017, &#39;HEALTHCARE&#39;), Text(11.720004774308011, 14.611892526490351, &#39;SIZE REPORT&#39;), Text(12.523970500235595, 16.800948278109235, &#39;IMPLEMENTATION&#39;), Text(13.259194963773892, 16.73840705780756, &#39;PERFORMANCE&#39;), Text(12.39718776369287, 16.711597519829162, &#39;EXECUTION&#39;), Text(12.282375764337278, 16.563529891059517, &#39;DELPHI&#39;), Text(13.88643091238314, 16.890422195479985, &#39;FINANCE&#39;), Text(14.330592984275953, 17.984452228319082, &#39;SENSITIVITY ANALYSIS&#39;), Text(15.46725687746319, 14.571671698206949, &#39;GEOGRAPHICAL DISTANCE&#39;), Text(13.177483143021739, 16.111358603977024, &#39;ANALYSIS&#39;), Text(13.699014885339043, 15.239764136359808, &#39;GA&#39;), Text(13.316255795977288, 17.048476161275595, &#39;PRELIMINARY PLANNING&#39;), Text(12.844158761482085, 16.86176019736699, &#39;FEASIBILITY STUDY&#39;), Text(14.824059189507555, 18.313916341463724, &#39;AVAILABILITY&#39;), Text(13.49528856268357, 16.108320274807163, &#39;STATISTICS ANALYSIS&#39;), Text(13.578016068836854, 15.980430738131208, &#39;INDIVIDUAL&#39;), Text(14.489664235143893, 17.56431890782855, &#39;SECURITY&#39;), Text(15.566084151272811, 14.650274604842778, &#39;SOCIO-CULTURAL DISTANCE&#39;), Text(14.73699002296213, 18.24070560591562, &#39;RELIABILITY&#39;), Text(13.856894044527603, 16.923548833529157, &#39;COMMISSIONING&#39;), Text(13.784974128704878, 14.89211454277947, &#39;OTHER&#39;), Text(13.424066989710253, 15.309978427205767, &#39;CONSIDERED&#39;), Text(13.56619351926037, 15.119748038337347, &#39;SE&#39;), Text(12.432408849111486, 13.83363260428111, &#39;VALUE&#39;), Text(12.804258030559266, 17.000468582198735, &#39;SYSTEM INVESTIGATION&#39;), Text(12.91690826253064, 15.515046996162052, &#39;FUZZY SIMILAR&#39;), Text(13.350658160532191, 17.034117447762263, &#39;DETAIL PLANNING&#39;), Text(15.88572205378644, 15.711316811470763, &#39;FAR OFFSHORE&#39;), Text(13.177880616582208, 15.322116813205538, &#39;EXPERT JUDGMENT&#39;), Text(15.969403134178249, 15.847661037672136, &#39;CLOSE ONSHORE&#39;), Text(13.634846342850114, 15.733372784796217, &#39;TESTING&#39;), Text(12.906343733372706, 17.20000023387729, &#39;RESEARCH &amp; DEV&#39;), Text(12.621349276965184, 14.225349349067326, &#39;EFFORT HOURS&#39;), Text(15.99688209811526, 15.812791727838066, &#39;DISTANT ONSHORE&#39;), Text(14.836798750868727, 17.732608717963814, &#39;MAINTENANCE&#39;), Text(13.13044522512584, 15.66728300026485, &#39;MACHINE LEARNING&#39;), Text(11.038990792634507, 14.393631877217976, &#39;NO OF TEAM MEMBERS&#39;), Text(10.343028291401362, 17.794839781806584, &#39;CONNECTIVITY DENSITY&#39;), Text(12.385135116654055, 17.709895153272722, &#39;IN-HOUSE EXPERIENCE&#39;), Text(9.635890491785059, 16.450242023240953, &#39;ENTITY COUNT&#39;), Text(9.319637117606979, 16.561630287624542, &#39;COMPONENT COUNT&#39;), Text(8.485909615699802, 17.43712814421881, &#39;NODE SLOT SIZE&#39;), Text(12.099679643511772, 16.641808413323908, &#39;SPI PROGRAM&#39;), Text(14.066332647081824, 19.11191959608169, &#39;USABILITY LEVEL&#39;), Text(13.730562014464407, 17.344123763129826, &#39;TECHNICAL FACTORS&#39;), Text(10.263056143000721, 16.312204206557503, &#39;CONCERN OPERATION COUNT&#39;), Text(10.212888116567365, 18.63582338492076, &#39;INTERFACE COMPLEXITY&#39;), Text(8.873429564369781, 17.196706887653896, &#39;CLUSTER NODE SIZE&#39;), Text(9.713230290441743, 17.29555912244888, &#39;SEMANTIC ASSOCIATION COUNT&#39;), Text(14.432109024678507, 18.89519722348168, &#39;PORTABILITY LEVEL&#39;), Text(9.31525793542785, 16.610061568305607, &#39;MODULE ATTRIBUTE COUNT&#39;), Text(9.332707342863563, 17.084839801561266, &#39;MODULARITY LEVEL&#39;), Text(14.308162304891212, 18.035188752128967, &#39;RISK LEVEL&#39;), Text(9.522602123995462, 16.63650602272579, &#39;CONCERN MODULE COUNT&#39;), Text(11.878416331030667, 15.196722320147925, &#39;METRICS PROGRAM&#39;), Text(14.689283230660422, 15.361356851032804, &#39;PROJECT.INFRASTRUCTURE&#39;), Text(10.23203701311542, 18.724905091240295, &#39;CONTROL FLOW COMPLEXITY&#39;), Text(13.709948404712542, 18.901302646455314, &#39;SCALABILITY LEVEL&#39;), Text(8.399217001661176, 17.343381920314975, &#39;INFORMATION SLOT COUNT&#39;), Text(14.137817605671382, 18.487572805086774, &#39;PLATFORM VOLATILITY LEVEL&#39;), Text(9.660688281919686, 16.37652299858275, &#39;REUSED COMPONENT COUNT&#39;), Text(10.84539570635365, 14.405783614658176, &#39;TEAM CAPABILITY&#39;), Text(10.137285479922685, 16.38283441520873, &#39;STATEMENT COUNT&#39;), Text(13.407480435073374, 18.41696170852298, &#39;DOCUMENTATION LEVEL&#39;), Text(12.667940666017032, 18.535937367166795, &#39;OO EXPERIENCE LEVEL&#39;), Text(10.078972891292745, 18.403916339647207, &#39;MODEL LINK COMPLEXITY&#39;), Text(9.399577667869385, 15.41879352387928, &#39;MEDIA COUNT&#39;), Text(9.211210336661146, 16.724459706033983, &#39;MODULE POINT CUT COUNT&#39;), Text(12.793482838609524, 18.079982564562847, &#39;AUTHORING TOOL TYPE&#39;), Text(10.368870352881569, 17.08747781912486, &#39;WEB PAGE ALLOCATION&#39;), Text(14.239261422226985, 15.378427659897579, &#39;PROJECT.TYPE&#39;), Text(10.153949915266805, 17.015180703571865, &#39;WEB PAGE COUNT&#39;), Text(12.438921653887917, 17.447174149467834, &#39;LESSONS LEARNED REPOSITORY&#39;), Text(14.266307867815899, 19.12322740895408, &#39;READABILITY LEVEL&#39;), Text(11.417678201424497, 17.751877919832868, &#39;CLASS COUPLING&#39;), Text(10.377793337282153, 18.891618921643214, &#39;ADAPTATION COMPLEXITY&#39;), Text(12.341880698292965, 17.079335232008074, &#39;MAPPED WORKFLOWS&#39;), Text(8.740923996620602, 16.856979640324916, &#39;CLUSTER COUNT&#39;), Text(9.758208117857574, 16.79541899022602, &#39;COMMENT COUNT&#39;), Text(9.64304519848237, 15.662863943690349, &#39;REUSED MEDIA COUNT&#39;), Text(8.44077010164578, 17.37549591064453, &#39;ASSOCIATION SLOT SIZE&#39;), Text(14.145147894995826, 18.43273996057965, &#39;DESIGN VOLATILITY&#39;), Text(11.089378616396457, 18.106034413973493, &#39;COHESION&#39;), Text(10.536079228606916, 16.55316921188718, &#39;PROGRAM COUNT&#39;), Text(13.381989516070291, 17.6413545415515, &#39;TIME EFFICIENCY LEVEL&#39;), Text(9.016387457934117, 16.15739328180041, &#39;REUSED HIGH FEATURE COUNT&#39;), Text(9.65408693081429, 15.983724632717315, &#39;PUBLISHING UNIT COUNT&#39;), Text(9.190019661221772, 16.896455062003366, &#39;DIFFUSION CUT COUNT&#39;), Text(12.083573034321589, 15.83091052827381, &#39;FOCUS FACTOR&#39;), Text(10.184355349831524, 18.540831546556383, &#39;CLASS COMPLEXITY&#39;), Text(10.945465712554512, 14.528865092141288, &#39;WORK TEAM LEVEL&#39;), Text(13.19012998202395, 14.509990788641431, &#39;TIME RESTRICTION&#39;), Text(10.293800702933822, 18.964707702682134, &#39;CYCLOMATIC COMPLEXITY&#39;), Text(9.184056730205494, 16.20253823938824, &#39;ATTRIBUTE COUNT&#39;), Text(11.452870680882086, 16.66573516754877, &#39;WEB OBJECTS&#39;), Text(13.896323820568862, 17.82081596283686, &#39;COMMUNICATION LEVEL&#39;), Text(10.900839954852575, 16.28652082284292, &#39;USE CASE COUNT&#39;), Text(13.610043233690721, 18.76192030339014, &#39;REQUIREMENTS NOVELTY LEVEL&#39;), Text(10.284110957876809, 16.76240181241717, &#39;SERVER SCRIPT COUNT&#39;), Text(11.538372664458809, 16.32002748648326, &#39;DATA WEB POINTS&#39;), Text(13.299299706576331, 17.79708679290045, &#39;PRODUCTIVITY LEVEL&#39;), Text(12.731554222102126, 17.870097998210365, &#39;NUMBER OF PROJECTS IN PARALLEL&#39;), Text(14.392425884008407, 18.960691606430785, &#39;TRAINABILITY LEVEL&#39;), Text(8.769550117701291, 16.20680307093121, &#39;REUSED LOW FEATURE COUNT&#39;), Text(10.098825793475392, 16.892509537651428, &#39;NEW WEB PAGE COUNT&#39;), Text(10.049345167759926, 18.314742887587776, &#39;MODEL COLLECTION COMPLEXITY&#39;), Text(8.290804593748142, 17.3364573490052, &#39;COLLECTION SLOT SIZE&#39;), Text(13.814700646388435, 17.590985433260602, &#39;IT LITERACY&#39;), Text(9.421987551457939, 17.77723876862299, &#39;COMPONENT GRANULARITY LEVEL&#39;), Text(10.422375539519614, 18.77900512786139, &#39;DIFFICULTY LEVEL&#39;), Text(12.134856747862312, 15.7787467786244, &#39;INTERNATIONAL FUNCTION POINT USERS GROUP&#39;), Text(10.864830293225184, 16.720351084073386, &#39;LINES OF CODE&#39;), Text(10.183427656530853, 18.589899958883016, &#39;DATA FLOW COMPLEXITY&#39;), Text(8.456030132313888, 17.064270799500605, &#39;COMPONENT SLOT COUNT&#39;), Text(9.12744401268661, 16.52261726288569, &#39;SEGMENT COUNT&#39;), Text(13.586170208730522, 17.073950786817644, &#39;ARCHITECTURE&#39;), Text(12.740091877752732, 18.579300081162227, &#39;DOMAIN EXPERIENCE LEVEL&#39;), Text(12.587266296105039, 17.285068782170615, &#39;INTEGRATION WITH LEGACY SYSTEMS&#39;), Text(10.65019670945502, 16.568097875231793, &#39;REUSED PROGRAM COUNT&#39;), Text(14.418441555694706, 18.34198842843374, &#39;STABILITY LEVEL&#39;), Text(14.194488509119518, 18.61036281358628, &#39;FLEXIBILITY LEVEL&#39;), Text(13.447032644597272, 17.88456329164051, &#39;MOTIVATION LEVEL&#39;), Text(8.351322588074591, 17.50528930936541, &#39;MODEL SLOT SIZE&#39;), Text(10.145985269032177, 16.405941009521484, &#39;INNER/SUB CONCERN COUNT&#39;), Text(13.594200821569368, 18.73623462631589, &#39;REQUIREMENTS CLARITY LEVEL&#39;), Text(10.212917765876458, 16.261323851630806, &#39;OPERATION COUNT&#39;), Text(9.902324442260207, 18.399177435466225, &#39;DATA USAGE COMPLEXITY&#39;), Text(9.439670319138994, 16.96280782903944, &#39;LINK COUNT&#39;), Text(9.61920431620892, 18.827069340433397, &#39;COMPACTNESS&#39;), Text(8.37945952079469, 17.335110992477055, &#39;ASSOCIATION CENTER SLOT COUNT&#39;), Text(9.455222885031855, 15.692217143376672, &#39;NEW MEDIA COUNT&#39;), Text(9.640420679442823, 15.359972934495836, &#39;MEDIA ALLOCATION&#39;), Text(10.123428130805973, 18.408310639290583, &#39;COHESION COMPLEXITY&#39;), Text(14.103680186889344, 16.278753203437443, &#39;PRODUCT.TYPE&#39;), Text(8.450889645555327, 17.140572451409845, &#39;CLUSTER SLOT COUNT&#39;), Text(13.291488405886675, 17.778808632351105, &#39;PROCESS EFFICIENCY LEVEL&#39;), Text(13.273498624623302, 18.005970078422912, &#39;CONCURRENCY LEVEL&#39;), Text(11.381423532933958, 17.8245173851649, &#39;CONCERN COUPLING&#39;), Text(13.554790203604966, 16.48796579610734, &#39;STRATUM&#39;), Text(11.909321533232447, 17.07219143140884, &#39;RAPID APP DEVELOPMENT&#39;), Text(12.558386533445407, 16.466400223686584, &#39;OPERATIONAL MODE&#39;), Text(14.295094960175694, 18.4813993817284, &#39;AVAILABILITY LEVEL&#39;), Text(14.506897906399061, 18.99080661819095, &#39;TESTABILITY LEVEL&#39;), Text(10.185966546191803, 18.155873124940058, &#39;PAGE COMPLEXITY&#39;), Text(13.524346135672062, 16.01992116769155, &#39;PERSONALITY&#39;), Text(8.855312054190904, 16.312540054321293, &#39;FEATURE COUNT&#39;), Text(9.925451321383157, 16.598098870686126, &#39;REUSED COMMENT COUNT&#39;), Text(10.211741300631434, 18.697702485039123, &#39;OUTPUT COMPLEXITY&#39;), Text(12.967063264089727, 17.371133727119087, &#39;PROCESSING REQUIREMENTS&#39;), Text(10.86728856507088, 16.821063060987566, &#39;NUMBER OF PROGRAMMING LANGUAGES&#39;), Text(9.294325373470784, 16.52577205612546, &#39;SECTION COUNT&#39;), Text(8.834029758099106, 16.39513887564342, &#39;LOW FEATURE COUNT&#39;), Text(10.190182092903601, 18.82866104216803, &#39;NEW COMPLEXITY&#39;), Text(10.248369358847697, 18.829240876152404, &#39;INPUT COMPLEXITY&#39;), Text(9.740042813579402, 18.517751616523384, &#39;COMPONENT COMPLEXITY&#39;), Text(12.04725865808706, 15.921647876784919, &#39;OBJECT-ORIENTED FUNCTION POINTS&#39;), Text(13.113913440901424, 18.664299320039298, &#39;DEPLOYMENT PLATFORM EXPERIENCE LEVEL&#39;), Text(9.977618865310664, 15.527477303005403, &#39;REUSED MEDIA ALLOCATION&#39;), Text(14.21884721336826, 18.988067665554233, &#39;ACCESSIBILITY LEVEL&#39;), Text(14.152947773957443, 18.608332460267206, &#39;REQUIREMENTS VOLATILITY LEVEL&#39;), Text(13.221024570288677, 17.987321930839904, &#39;PROJECT MANAGEMENT LEVEL&#39;), Text(9.701959011746991, 15.871189156032745, &#39;PUBLISHING MODEL UNIT COUNT&#39;), Text(9.46485009243892, 17.671805516878766, &#39;DATABASE SIZE&#39;), Text(14.575265375226735, 18.488430841763815, &#39;ROBUSTNESS LEVEL&#39;), Text(14.657357857217711, 18.94174396651132, &#39;INSTALLABILITY LEVEL&#39;), Text(12.790448149733486, 18.332203027180267, &#39;PROGRAMMING LANGUAGE EXPERIENCE LEVEL&#39;), Text(10.12971698662927, 15.52813905647823, &#39;STORAGE CONSTRAINT&#39;), Text(12.680914862574106, 17.059839210056126, &#39;DEVELOPMENT RESTRICTION&#39;), Text(10.93989378434756, 14.588571876571294, &#39;TEAM SIZE&#39;), Text(9.99491824249106, 18.27317623183841, &#39;MODEL ASSOCIATION COMPLEXITY&#39;), Text(9.004090331506823, 17.467690680140542, &#39;MODEL NODE SIZE&#39;), Text(13.059144706557834, 18.486208857808798, &#39;PRODUCT.EXPERIENCE LEVEL&#39;), Text(13.270269121032566, 18.49838369573866, &#39;NOVELTY LEVEL&#39;), Text(13.210287580098354, 17.80402074654897, &#39;MEMORY EFFICIENCY LEVEL&#39;), Text(8.704285237726666, 17.296978969801042, &#39;SLOT COUNT&#39;), Text(9.127305919424659, 16.801466219765803, &#39;MODULE COUNT&#39;), Text(14.468173613067595, 18.046731871650337, &#39;SECURITY LEVEL&#39;), Text(12.634814904541258, 18.45777807349251, &#39;TOOL EXPERIENCE LEVEL&#39;), Text(12.284341970735017, 17.81096483525776, &#39;SOFTWARE DEVELOPMENT EXPERIENCE&#39;), Text(10.029670265127093, 16.405377137093318, &#39;INDIFFERENT CONCERN COUNT&#39;), Text(10.175828200054745, 18.84116300968897, &#39;TOTAL COMPLEXITY&#39;), Text(13.748889854362893, 18.125773294766745, &#39;QUALITY LEVEL&#39;), Text(11.923828512564782, 15.963485092208503, &#39;OBJECT-ORIENTED HEURISTIC FUNCTION POINTS&#39;), Text(9.163258810871067, 17.041140691439313, &#39;NODE COUNT&#39;), Text(9.56450811276032, 15.309187773295815, &#39;MEDIA DURATION&#39;), Text(14.765253602614806, 18.81255352156503, &#39;MAINTAINABILITY LEVEL&#39;), Text(10.955451076316738, 17.01767458120982, &#39;REUSED LINES OF CODE&#39;), Text(13.263863762403208, 18.25173105398814, &#39;INNOVATION LEVEL&#39;), Text(13.968307365787124, 18.220119399116154, &#39;PLATFORM SUPPORT LEVEL&#39;), Text(8.505802831666603, 17.58716329960596, &#39;SLOT GRANULARITY LEVEL&#39;), Text(14.722837449336243, 18.629038300968347, &#39;RELIABILITY LEVEL&#39;), Text(11.234847862631561, 17.348420394034612, &#39;REUSABILITY LEVEL&#39;), Text(13.78857194741647, 16.737781524658207, &#39;PRODUCT.STRUCTURE&#39;), Text(12.483446435572638, 15.455671329725359, &#39;COMMON SOFTWARE MEASUREMENT INTERNATIONAL CONSORTIUM&#39;), Text(8.209824363608513, 17.1974695568993, &#39;COLLECTION CENTER SLOT COUNT&#39;), Text(10.329895239544491, 16.579462831360956, &#39;CLIENT SCRIPT COUNT&#39;), Text(11.41583601904492, 17.21345808051882, &#39;SOFTWARE REUSE&#39;), Text(9.566512771962632, 16.94956320013319, &#39;ANCHOR COUNT&#39;), Text(9.052236191392424, 16.196302220934918, &#39;HIGH FEATURE COUNT&#39;), Text(10.115446999633024, 18.518260878608345, &#39;LAYOUT COMPLEXITY&#39;), Text(13.62051299568626, 18.247966534750805, &#39;RESOURCE LEVEL&#39;), Text(15.387496076714127, 16.274511568886897, &#39;SEMI-DISTRIBUTED&#39;), Text(12.325919793457274, 13.265025841622128, &#39;ESTIMATOR &amp; PROVIDER&#39;), Text(14.395096408378691, 16.730534688631696, &#39;PROVIDER&#39;), Text(15.205430098327415, 14.695633907545183, &#39;RELATIONSHIP.LOCATION&#39;), Text(14.751911478953973, 14.828830969901313, &#39;SITE.LOCATION&#39;), Text(14.915119521702007, 16.332354680697126, &#39;CENTRALIZED&#39;), Text(12.822145225751784, 14.76375311102186, &#39;LATE&#39;), Text(15.524230077367635, 14.544582521347774, &#39;RELATIONSHIP.GEOGRAPHIC DISTANCE&#39;), Text(12.911138603680076, 14.72457227820442, &#39;EARLY &amp; LATE&#39;), Text(14.838381204823811, 14.924264069965911, &#39;RELATIONSHIP.LEGAL ENTITY&#39;), Text(15.385965611629429, 14.372935179301672, &#39;SITE.GEOGRAPHIC DISTANCE&#39;), Text(12.49839770229353, 13.426346547263012, &#39;ESTIMATOR&#39;), Text(12.727772786316851, 14.8072825057166, &#39;EARLY&#39;), Text(15.205390832974544, 14.344099824769158, &#39;SITE.TEMPORAL DISTANCE&#39;), Text(15.4038737713858, 14.439527550197791, &#39;RELATIONSHIP.TEMPORAL DISTANCE&#39;), Text(14.988323555723312, 16.173735676492967, &#39;DISTRIBUTED&#39;), Text(14.635916810348148, 14.945418512253537, &#39;SITE.LEGAL ENTITY&#39;), Text(12.933963009844863, 16.54924198105222, &#39;BASIC-COMBINATION&#39;), Text(12.805766797419036, 16.47288805076054, &#39;AI-COMBINED HYBRID&#39;), Text(12.083615303440919, 17.51315136182876, &#39;SOFTWARE LIFE CYCLE MANAGEMENT&#39;), Text(15.179824609076304, 17.91556821664175, &#39;SOFTWARE EVALUATION AND ESTIMATION FOR RISK&#39;), Text(13.028276108366347, 15.776078166280476, &#39;ARTIFICIAL NEURAL NETWORKS&#39;), Text(12.783462843659422, 15.520048064277287, &#39;FUZZY LOGIC&#39;), Text(12.862702361554387, 16.003240430922737, &#39;ANALOGY-BASED&#39;), Text(12.99700279885963, 15.8359244573684, &#39;COMPUTATIONAL INTELLIGENCE.SWARM&#39;), Text(12.721756937150513, 15.690487919534958, &#39;COMPUTATIONAL INTELLIGENCE.EVOLUTIONARY&#39;), Text(13.627013476111234, 15.760168249266492, &#39;VALIDATED THEORETICALLY&#39;), Text(13.24043991072524, 15.006986714544752, &#39;ABSOLUTE&#39;), Text(11.7058283010726, 16.928251382282802, &#39;WEB SOFTWARE APPLICATION&#39;), Text(13.337472680581193, 17.63188634713491, &#39;MOTIVATION&#39;), Text(12.52060192653008, 15.024567739168804, &#39;RATIO&#39;), Text(13.439819407102561, 15.61932954901741, &#39;VALIDATED EMPIRICALLY&#39;), Text(13.773250035686837, 15.483251861163549, &#39;VALIDATION.BOTH&#39;), Text(12.741904239611278, 16.536240145138354, &#39;FUNCTIONALITY&#39;), Text(10.844638301463377, 16.69344808601198, &#39;PROGRAM/SCRIPT&#39;), Text(12.20151762971955, 15.032646140598118, &#39;SOLUTION-ORIENTED METRIC&#39;), Text(11.73163768385447, 16.80811305954343, &#39;WEB HYPERMEDIA APPLICATION&#39;), Text(12.096693261799311, 14.735388794399446, &#39;EARLY SIZE METRIC&#39;), Text(9.759153833367652, 18.661737442016605, &#39;COMPLEXITY&#39;), Text(14.296797890648726, 15.164241887274246, &#39;MODEL DEPENDENCY.SPECIFIC&#39;), Text(9.736024582049538, 17.27169628370376, &#39;CLASS.LENGTH&#39;), Text(11.61755495823439, 17.026553404898873, &#39;WEB APPLICATION&#39;), Text(12.489099888097373, 16.37475395202637, &#39;COMPUTATION.DIRECT&#39;), Text(12.49201114589168, 16.282144392104378, &#39;COMPUTATION.INDIRECT&#39;), Text(14.539893565548041, 15.282516595295501, &#39;MODEL DEPENDENCY.NONSPECIFIC&#39;), Text(12.111057574301956, 15.128276025681272, &#39;PROBLEM-ORIENTED METRIC&#39;), Text(13.519689752296092, 14.917987726983574, &#39;ORDINAL&#39;), Text(12.972714419658145, 14.60616792837779, &#39;INTERVAL&#39;), Text(12.148597286196965, 14.769287340981627, &#39;LATE SIZE METRIC&#39;), Text(13.655537508899165, 14.875778024537226, &#39;NOMINAL&#39;), Text(13.703256954769454, 15.547764990443278, &#39;VALIDATION.NONE&#39;), Text(9.370674523309834, 15.251711768195744, &#39;MEDIA&#39;), Text(13.722749477459057, 16.68964094093868, &#39;BIDDING&#39;), Text(13.13555462924463, 14.335823935554142, &#39;PAIR DAYS&#39;), Text(13.229487228386343, 13.794390987214591, &#39;MEAN MAGNITUDE OF RELATIVE ERROR&#39;), Text(12.770150790178487, 16.825252822467274, &#39;CUSTOMIZED EXTREME PROGRAMMING&#39;), Text(13.043820689428717, 14.436518630527315, &#39;IDEAL HOURS&#39;), Text(12.244142077267169, 14.4441909018017, &#39;SIZE.OTHER&#39;), Text(12.542515413893327, 13.93794168631236, &#39;EFFORT ESTIMATE.TYPE.OTHER&#39;), Text(12.99421047259002, 13.663825988769535, &#39;MEDIAN MAGNITUDE OF RELATIVE ERROR&#39;), Text(11.619225109560835, 16.209363157408582, &#39;STORY POINTS&#39;), Text(11.747992592018939, 16.512261332784384, &#39;USE CASE&#39;), Text(10.869960596614305, 14.312114599772864, &#39;NO. OF TEAM MEMBERS&#39;), Text(14.16743457200306, 16.555464879671735, &#39;SPRINT&#39;), Text(15.8263380520935, 15.951494409924463, &#39;DISTRIBUTED: FAR OFFSHORE&#39;), Text(11.700371127337696, 16.10120309988658, &#39;USE CASE POINTS METHOD&#39;), Text(15.771816298250227, 15.869616508483888, &#39;DISTRIBUTED: DISTANT ONSHORE&#39;), Text(8.749755495390104, 16.391808644930524, &#39;FEATURE-DRIVEN DEVELOPMENT&#39;), Text(12.777123338173954, 16.159695837611245, &#39;ANALOGY&#39;), Text(11.59828058864801, 15.812967223212834, &#39;FUNCTION POINTS&#39;), Text(13.4519971668552, 16.621395304089503, &#39;CUSTOMIZED SCRUM&#39;), Text(14.168531106080255, 17.324675617899217, &#39;EDUCATION&#39;), Text(11.5551221741784, 15.709395273526512, &#39;POINT&#39;), Text(13.097060142140233, 14.300664708727883, &#39;HOURS/DAYS&#39;), Text(14.78401608253198, 17.46228899161021, &#39;GOVERNMENT/MILITARY&#39;), Text(12.929421888484109, 16.529648954527723, &#39;TASK&#39;), Text(15.147603691765857, 15.95108024506342, &#39;CO-LOCATED&#39;), Text(15.801912306371715, 15.833013096309848, &#39;DISTRIBUTED: NEAR OFFSHORE&#39;), Text(13.903568267007508, 16.156995696113228, &#39;CRYSTAL&#39;), Text(12.950129963640244, 13.635192813192097, &#39;ACCURACY LEVEL.VALUE&#39;), Text(14.568235615722594, 17.12949087165651, &#39;COMMUNICATIONS INDUSTRY&#39;), Text(12.33436284786751, 13.812675804183598, &#39;ESTIMATED ACTIVITIES.ALL&#39;), Text(11.501616558081679, 15.73322954064324, &#39;THREE POINT&#39;), Text(13.464992559383951, 18.741528800555642, &#39;NON FUNCTIONAL REQUIREMENTS.OTHER&#39;), Text(14.635965172004314, 15.209287817137584, &#39;PROJECT DOMAIN.OTHER&#39;), Text(12.422795135071201, 13.526213626634508, &#39;ESTIMATION ENTITY.OTHER&#39;), Text(10.892057185384534, 14.363851759547286, &#39;GROUP&#39;), Text(14.544783392945122, 17.042312506267002, &#39;TRANSPORTATION&#39;), Text(14.695700587695162, 15.939920348212834, &#39;DISTRIBUTION&#39;), Text(12.645130942365334, 13.631137905802047, &#39;NUMBER OF ENTITIES ESTIMATED.VALUE&#39;), Text(12.682655584170451, 13.264753129368739, &#39;ESTIMATION TECHNIQUES.OTHER&#39;), Text(14.37685284603988, 16.884363251640686, &#39;RETAIL/WHOLESALE&#39;), Text(13.768668563123192, 15.554392189071296, &#39;NOT USED&#39;), Text(13.288545290892642, 13.623411520322167, &#39;BIAS OF RELATIVE ERROR&#39;), Text(15.976899894922969, 16.01943809872582, &#39;DISTRIBUTED: CLOSE ONSHORE&#39;), Text(14.46476753617006, 16.327190611476, &#39;KANBAN&#39;), Text(13.294571170518473, 17.246412315822784, &#39;PLANNING POKER&#39;), Text(12.934779895947823, 13.809002270017356, &#39;ACCURACY MEASURE.OTHER&#39;), Text(12.607803228408578, 17.23920187382471, &#39;DYNAMIC SYSTEMS DEVELOPMENT METHOD&#39;), Text(13.882135379439397, 14.710601729438423, &#39;UNIT.OTHER&#39;), Text(13.174272423357731, 15.349347172464645, &#39;EXPERT JUDGEMENT&#39;), Text(13.027383128411827, 14.401572150275822, &#39;DAILY&#39;), Text(11.721347143094867, 16.845141237122675, &#39;USER STORY&#39;), Text(12.387783904164548, 13.465777532259626, &#39;ESTIMATE VALUE(S)&#39;), Text(13.959441419652393, 16.984849698202954, &#39;MANUFACTURING&#39;), Text(14.36531860009797, 16.182455958638876, &#39;RELEASE&#39;), Text(13.83895618462995, 15.852790013949079, &#39;SINGLE&#39;), Text(14.546435932002721, 17.41077329658327, &#39;HEALTH&#39;), Text(11.347910154702202, 16.168538190069658, &#39;USER CASE POINTS&#39;), Text(12.410640420887258, 14.897523138636636, &#39;CONSIDERED WITHOUT ANY METRIC&#39;), Text(12.602104314298877, 17.005586701347717, &#39;EXTREME PROGRAMMING&#39;), Text(13.736250077430759, 16.444497984931587, &#39;FINANCIAL&#39;), Text(13.855368397430546, 15.393225785664152, &#39;NOT APPLICABLE&#39;), Text(13.328025381822497, 16.646781102816266, &#39;SCRUM&#39;), Text(12.513908616410147, 14.150071279207868, &#39;ACTUAL EFFORT.VALUE&#39;)], [])</code></pre>
<pre class="python"><code># First legend for word sets
handles = [plt.Line2D([0], [0], marker=&#39;o&#39;, color=&#39;w&#39;, markerfacecolor=color, markersize=10) for color in set_colors.values()]
labels = list(sets.keys())
legend1 = plt.legend(handles=handles, labels=labels, title=&quot;Literature&quot;, loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.05), ncol=6)

# Second legend for duplicate words
duplicate_legend = plt.Line2D([0], [1], color=&#39;red&#39;, lw=2)
legend2 = plt.legend([duplicate_legend], [&quot;In red: duplicate words&quot;], loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.12), frameon=False)

# Re-add the first legend
plt.gca().add_artist(legend1)

plt.title(&quot;UMAP Visualization of Word Embeddings&quot;)
plt.xlabel(&quot;UMAP Component 1&quot;)
plt.ylabel(&quot;UMAP Component 2&quot;)

# Save the plot as an image
plt.savefig(&#39;umap_word_embeddings.png&#39;, dpi=600, bbox_inches=&#39;tight&#39;)

# Display the plot
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-15-19.png" style="display: block; margin: auto;" /></p>
</div>
<div id="similarity-counts-heatmap" class="section level1">
<h1>Similarity Counts Heatmap</h1>
<pre class="python"><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Clear any existing plots and set plot style
plt.clf()
plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)
plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;

df = similarity_df_filtered

# Group by Set 1 and Set 2 and count the number of shared words
count_table = df.groupby([&quot;Set 1&quot;, &quot;Set 2&quot;]).size().reset_index(name=&quot;Shared Word Count&quot;)

# Pivot the DataFrame to create a matrix of shared word counts
pivot_count = count_table.pivot(index=&quot;Set 1&quot;, columns=&quot;Set 2&quot;, values=&quot;Shared Word Count&quot;).fillna(0)

# Reindex the pivot table to ensure symmetry in rows and columns
pivot_count = pivot_count.reindex(index=pivot_count.columns, columns=pivot_count.columns, fill_value=0)

# Mask only the upper triangle, excluding the diagonal
mask = np.triu(np.ones_like(pivot_count, dtype=bool), k=1)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_count, annot=True, mask=mask, cmap=&quot;Blues&quot;, cbar_kws={&#39;label&#39;: &#39;Number of Shared Words&#39;})
plt.title(&quot;Heatmap of Similar Words between Literature&quot;)
plt.xlabel(&quot;Literature&quot;)
plt.ylabel(&quot;Literature&quot;)
plt.xticks(rotation=45, ha=&quot;right&quot;)</code></pre>
<pre><code>(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5]), [Text(0.5, 0, &#39;Bajta&#39;), Text(1.5, 0, &#39;Britto_2016&#39;), Text(2.5, 0, &#39;Britto_2017&#39;), Text(3.5, 0, &#39;Dasthi&#39;), Text(4.5, 0, &#39;Mendes&#39;), Text(5.5, 0, &#39;Usman&#39;)])</code></pre>
<pre class="python"><code>plt.yticks(rotation=0)</code></pre>
<pre><code>(array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5]), [Text(0, 0.5, &#39;Bajta&#39;), Text(0, 1.5, &#39;Britto_2016&#39;), Text(0, 2.5, &#39;Britto_2017&#39;), Text(0, 3.5, &#39;Dasthi&#39;), Text(0, 4.5, &#39;Mendes&#39;), Text(0, 5.5, &#39;Usman&#39;)])</code></pre>
<pre class="python"><code>plt.tight_layout()
plt.savefig(&#39;word_counts.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-16-21.png" style="display: block; margin: auto;" /><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-16-22.png" style="display: block; margin: auto;" /></p>
</div>
<div id="barplot" class="section level1">
<h1>Barplot</h1>
<pre class="python"><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Clear any existing plots and set plot style
plt.clf()
plt.style.use(&#39;seaborn-v0_8-whitegrid&#39;)
plt.rcParams[&#39;font.family&#39;] = &#39;serif&#39;

# Data preparation
df = similarity_df_filtered

# Group by Set 1 and Set 2 and count the number of shared words
count_table = df.groupby([&quot;Set 1&quot;, &quot;Set 2&quot;]).size().reset_index(name=&quot;Shared Word Count&quot;)

# Remove redundant comparisons (keeping only Set1 &lt; Set2)
count_table = count_table[count_table[&quot;Set 1&quot;] &lt; count_table[&quot;Set 2&quot;]]

# Create all possible combinations of Set 1 and Set 2
all_sets = pd.MultiIndex.from_product([count_table[&quot;Set 1&quot;].unique(), count_table[&quot;Set 2&quot;].unique()], names=[&quot;Set 1&quot;, &quot;Set 2&quot;])

# Create a DataFrame with all combinations and zero counts
count_table_full = pd.DataFrame(index=all_sets).reset_index()

# Merge count_table_full with the original count_table to get the actual shared word counts
count_table_full = pd.merge(count_table_full, count_table, on=[&quot;Set 1&quot;, &quot;Set 2&quot;], how=&quot;left&quot;).fillna(0)

# Create the bar plot (dodge=True)
plt.figure(figsize=(12, 8))
ax = sns.barplot(x=&quot;Set 1&quot;, y=&quot;Shared Word Count&quot;, hue=&quot;Set 2&quot;, data=count_table_full, palette=&quot;Set2&quot;, width=0.8, dodge=True)

# Title and labels
plt.title(&quot;Barplot of Similar Words Between Literature Sets (Upper 70% similarity threshold&quot;)
plt.xlabel(&quot;Number of Similar Words&quot;)
plt.ylabel(&quot;Literature&quot;)

# Add value labels inside each bar with the same color as the bars
for container in ax.containers:
    for bar in container:
        bar_color = bar.get_facecolor()  # Get the color of the bar
        ax.bar_label(container, label_type=&quot;edge&quot;, padding=3, fontsize=12, color=bar_color, fontweight=&#39;bold&#39;)  # Set the label color to the bar&#39;s color

# Change the legend title and position it at the bottom horizontally
plt.legend(title=&quot;Literature&quot;, loc=&#39;upper center&#39;, bbox_to_anchor=(0.5, -0.1), ncol=len(count_table[&#39;Set 2&#39;].unique()))

# Adjust layout to prevent clipping and save the plot
plt.tight_layout()
plt.savefig(&#39;shared_words_barplot_with_labels.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;)
plt.show()</code></pre>
<p><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-17-25.png" style="display: block; margin: auto;" /><img src="figure/robustnessandconciseness.Rmd/unnamed-chunk-17-26.png" style="display: block; margin: auto;" /></p>
</div>
<div id="table-showing-what-words-are-related-to-words-in-other-papers"
class="section level1">
<h1>Table showing what words are related to words in other papers</h1>
<pre class="python"><code># Group by Set 1 and Set 2
count_table = df.groupby([&quot;Set 1&quot;, &quot;Set 2&quot;]).size().reset_index(name=&quot;Shared Word Count&quot;)

# Create empty lists to store the shared words for Set 1 and Set 2
words_set1 = []
words_set2 = []

# For each pair of sets, collect the shared words
for _, row in count_table.iterrows():
    set1 = row[&#39;Set 1&#39;]
    set2 = row[&#39;Set 2&#39;]
    
    # Get the rows from the filtered DataFrame that match the current pair of sets
    shared_rows = df[(df[&#39;Set 1&#39;] == set1) &amp; (df[&#39;Set 2&#39;] == set2)]
    
    # Collect the shared words for Set 1 and Set 2
    set1_words = sorted(shared_rows[&#39;Word 1&#39;].unique())  # Unique words from Set 1
    set2_words = sorted(shared_rows[&#39;Word 2&#39;].unique())  # Unique words from Set 2
    
    # Join the words into a comma-separated string
    words_set1.append(&quot;, &quot;.join(set1_words))
    words_set2.append(&quot;, &quot;.join(set2_words))

# Add the new columns to the count_table
count_table[&#39;Words From Set 1&#39;] = words_set1
count_table[&#39;Words From Set 2&#39;] = words_set2



# Display the resulting DataFrame
print(count_table)</code></pre>
<pre><code>          Set 1  ...                                   Words From Set 2
0         Bajta  ...  estimator, estimator &amp; provider, relationship....
1         Bajta  ...  architecture, availability level, installabili...
2         Bajta  ...  artificial neural networks, constructive cost ...
3         Bajta  ...  early size metric, functionality, late size me...
4         Bajta  ...  actual effort.value, analysis, availability, c...
5   Britto_2016  ...  estimate value, geographical distance, tempora...
6   Britto_2016  ...  co-located, distributed: close onshore, distri...
7   Britto_2017  ...  availability, design, effort hours, hardware, ...
8   Britto_2017  ...                     software life cycle management
9   Britto_2017  ...  class.length, complexity, media, model depende...
10  Britto_2017  ...  accuracy level.value, availability, design, ef...
11       Dasthi  ...  constructive cost model, expert judgment, fuzz...
12       Dasthi  ...                    software development experience
13       Dasthi  ...                          analogy, expert judgement
14       Mendes  ...  considered, implementation, not considered, pe...
15       Mendes  ...  adaptation complexity, class complexity, cohes...
16       Mendes  ...  considered, considered without any metric, imp...
17        Usman  ...  agile, analysis, availability, close onshore, ...
18        Usman  ...  centralized, distributed, estimator, estimator...
19        Usman  ...  architecture, availability level, data web poi...
20        Usman  ...                     analogy-based, expert judgment
21        Usman  ...  early size metric, functionality, late size me...

[22 rows x 5 columns]</code></pre>
<pre class="python"><code># Make sure that Set 1 is always smaller than Set 2 (lexicographically)
count_table[&#39;Set Pair&#39;] = count_table.apply(lambda row: tuple(sorted([row[&#39;Set 1&#39;], row[&#39;Set 2&#39;]])), axis=1)

# Drop duplicates based on the &#39;Set Pair&#39; column
count_table_filtered = count_table.drop_duplicates(subset=[&#39;Set Pair&#39;])

# Drop the &#39;Set Pair&#39; column now that we don&#39;t need it anymore
count_table_filtered = count_table_filtered.drop(columns=[&#39;Set Pair&#39;])

# Display the filtered result
print(count_table_filtered)</code></pre>
<pre><code>          Set 1  ...                                   Words From Set 2
0         Bajta  ...  estimator, estimator &amp; provider, relationship....
1         Bajta  ...  architecture, availability level, installabili...
2         Bajta  ...  artificial neural networks, constructive cost ...
3         Bajta  ...  early size metric, functionality, late size me...
4         Bajta  ...  actual effort.value, analysis, availability, c...
6   Britto_2016  ...  co-located, distributed: close onshore, distri...
8   Britto_2017  ...                     software life cycle management
9   Britto_2017  ...  class.length, complexity, media, model depende...
10  Britto_2017  ...  accuracy level.value, availability, design, ef...
13       Dasthi  ...                          analogy, expert judgement
16       Mendes  ...  considered, considered without any metric, imp...

[11 rows x 5 columns]</code></pre>
<pre class="python"><code>##############################COLORIZE TABLE############################

import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font


# Initialize tracking variables
word1_to_color = {}
color_index = 0
previous_word = None
previous_color = None

# Create a new column &quot;Marker&quot; to hold &quot;*&quot; if &quot;Word 1&quot; equals &quot;Word 2&quot;
similarity_df_filtered[&#39;Marker&#39;] = similarity_df_filtered.apply(
    lambda row: &#39;*&#39; if row[&#39;Word 1&#39;] == row[&#39;Word 2&#39;] else &#39;&#39;, axis=1
)</code></pre>
<pre><code>&lt;string&gt;:3: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</code></pre>
<pre class="python"><code># Function to assign colors to rows, skipping color assignment if the &quot;Word 1&quot; is the same as the previous row&#39;s &quot;Word 1&quot;
def assign_colors(row):
    global previous_word, previous_color, color_index

    word1 = row[&#39;Word 1&#39;]
    
    # If the current &quot;Word 1&quot; is the same as the previous &quot;Word 1&quot;, reuse the color
    if word1 == previous_word:
        color = previous_color
    else:
        # Otherwise, assign the next color and update tracking variables
        color = color_palette[color_index % len(color_palette)]
        previous_color = color
        previous_word = word1
        color_index += 1
    
    # Return a list of styles for all columns except for &quot;Marker&quot; (bold)
    return [f&#39;background-color: {color}&#39;] * (len(row) - 1) + [&#39;font-weight: bold; color: black;&#39;]

# Apply the function to style the DataFrame
styled_similarity_df = similarity_df_filtered.style.apply(assign_colors, axis=1)

# Display the styled DataFrame
styled_similarity_df</code></pre>
<pre><code>&lt;pandas.io.formats.style.Styler object at 0x00000000CD847A10&gt;</code></pre>
<pre class="python"><code>styled_similarity_df.to_html(&#39;similarity_table.html&#39;)

######################## EXCEL ###################
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill


# Initialize the Excel workbook and sheet
wb = Workbook()
ws = wb.active
ws.title = &quot;Similarity Results&quot;

# Write the headers to the Excel sheet
headers = list(similarity_df_filtered.columns)
ws.append(headers)

# Define the color palette (matching Excel-friendly hex colors)
excel_palette = [
    &#39;FFFFFF&#39;,  # Soft Orange
    &#39;cfebff&#39;,  # Soft Blue
]

# Initialize tracking variables for coloring
word1_to_color = {}
color_index = 0
previous_word = None
previous_color = None

# Function to get the next color for a word pair, reusing previous color if the word matches
def get_color(word1):
    global previous_word, previous_color, color_index
    if word1 == previous_word:
        color = previous_color
    else:
        color = excel_palette[color_index % len(excel_palette)]
        previous_word = word1
        previous_color = color
        color_index += 1
    return color

# Populate the Excel sheet with data and apply conditional coloring
for _, row in similarity_df_filtered.iterrows():
    # Determine the color for this row
    color_hex = get_color(row[&#39;Word 1&#39;])
    fill = PatternFill(start_color=color_hex, end_color=color_hex, fill_type=&quot;solid&quot;)
    
    # Create a list to hold row values, including the new &quot;Marker&quot; column
    row_values = list(row)

    # Add an asterisk &quot;*&quot; in the &quot;Marker&quot; column if Word 1 == Word 2
    marker = &quot;*&quot; if row[&#39;Word 1&#39;] == row[&#39;Word 2&#39;] else &quot;&quot;
    row_values.append(marker)  # Append the marker to the row values

    # Write row to Excel and apply styling
    ws.append(row_values)
    for col_idx in range(1, len(row_values)):
        cell = ws.cell(row=ws.max_row, column=col_idx)
        cell.fill = fill  # Apply the color fill to each cell in the row

    # Apply bold font to the marker cell if it contains &quot;*&quot;
    marker_cell = ws.cell(row=ws.max_row, column=len(row_values))
    if marker_cell.value == &quot;*&quot;:
        marker_cell.font = Font(bold=True)

# Save the workbook
wb.save(&quot;similarity_results_colored.xlsx&quot;)</code></pre>
</div>
<div id="trying-to-collapse-the-excel" class="section level1">
<h1>Trying to collapse the excel</h1>
<pre class="python"><code>import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font
from openpyxl.utils import get_column_letter

# Initialize Excel workbook and worksheet
wb = Workbook()
ws = wb.active
ws.title = &quot;Collapsed Similarity Results&quot;

# Create a collapsed column header
headers = [&quot;Set Pair&quot;, &quot;Word Pair&quot;, &quot;Cosine Similarity&quot;, &quot;Marker&quot;]
ws.append(headers)

# Define color palette for coloring words
excel_palette = [
    &#39;ADD8E6&#39;,  # Light Blue
    &#39;FFFFFF&#39;,  # Sky Blue
]

# Initialize tracking for word color and color index
word1_to_color = {}
color_index = 0
previous_word = None
previous_color = None

# Function to get the next color for a word, preventing consecutive repetitions
def get_color(word1):
    global previous_word, previous_color, color_index
    if word1 == previous_word:
        color = previous_color
    else:
        color = excel_palette[color_index % len(excel_palette)]
        previous_word = word1
        previous_color = color
        color_index += 1
    return color

# Populate the sheet with merged &quot;Set Pair&quot; and &quot;Word Pair&quot; columns
for _, row in similarity_df_filtered.iterrows():
    set_pair = f&quot;{row[&#39;Set 1&#39;]} - {row[&#39;Set 2&#39;]}&quot;
    word_pair = f&quot;{row[&#39;Word 1&#39;]} - {row[&#39;Word 2&#39;]}&quot;
    cosine_similarity = row[&#39;Cosine Similarity&#39;]
    marker = &quot;*&quot; if row[&#39;Word 1&#39;] == row[&#39;Word 2&#39;] else &quot;&quot;

    # Append collapsed data to Excel sheet
    ws.append([set_pair, word_pair, cosine_similarity, marker])

    # Retrieve the current row in Excel (last row)
    current_row = ws.max_row

    # Apply color fill to &quot;Word Pair&quot; cell, splitting the color between the words
    word1_color = get_color(row[&#39;Word 1&#39;])
    word2_color = get_color(row[&#39;Word 2&#39;])

    # Use PatternFill to set color on individual words in the &quot;Word Pair&quot; cell
    cell = ws.cell(row=current_row, column=2)
    cell_value = cell.value

    # Apply color fills
    ws.cell(row=current_row, column=2).fill = PatternFill(
        start_color=word1_color, end_color=word1_color, fill_type=&quot;solid&quot;
    )
    # Format each word with its designated color
    # Since openpyxl doesn&#39;t support in-cell color splitting directly,
    # we’d visually inspect it or use external software for more advanced formatting.

    # Apply bold font to the &quot;Marker&quot; column if it has &quot;*&quot;
    marker_cell = ws.cell(row=current_row, column=4)
    if marker:
        marker_cell.font = Font(bold=True)

# Adjust column width for readability
for col in range(1, ws.max_column + 1):
    ws.column_dimensions[get_column_letter(col)].width = 20

# Save the workbook
wb.save(&quot;collapsed_similarity_results_colored.xlsx&quot;)</code></pre>
</div>
<div id="trying-to-collapse-the-excel-even-more" class="section level1">
<h1>Trying to collapse the excel even more</h1>
<pre class="python"><code>import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter

# Create a workbook and a worksheet
wb = Workbook()
ws = wb.active
ws.title = &quot;Collapsed Similarity Results&quot;

# Define the headers for the new table
headers = [&quot;Set Pair&quot;, &quot;Word Pair&quot;]
ws.append(headers)

# Define color palette for coloring words
excel_palette = [
    &#39;FF6384&#39;,  # Light Red
    &#39;36A2EB&#39;,  # Light Blue
    &#39;FFCE56&#39;,  # Light Yellow
    &#39;4BC0C0&#39;,  # Light Green
    &#39;9966FF&#39;,  # Light Purple
    &#39;FF9F40&#39;   # Light Orange
]

# Initialize the color index and word-to-color mapping
color_index = 0
word_colors = {}

# Function to assign a color to a word, and avoid repeating colors for the same word in consecutive rows
def get_word_color(word):
    global color_index
    if word not in word_colors:
        word_colors[word] = excel_palette[color_index % len(excel_palette)]
        color_index += 1
    return word_colors[word]

# Group the words by set pair and track the font color for each word
set_pairs_dict = {}

# Populate the set_pairs_dict with the relevant data
for _, row in similarity_df_filtered.iterrows():
    set_pair = f&quot;{row[&#39;Set 1&#39;]} - {row[&#39;Set 2&#39;]}&quot;
    word_pair = f&quot;{row[&#39;Word 1&#39;]} - {row[&#39;Word 2&#39;]}&quot;
    
    # Group words by set pairs
    if set_pair not in set_pairs_dict:
        set_pairs_dict[set_pair] = []
    
    set_pairs_dict[set_pair].append((row[&#39;Word 1&#39;], row[&#39;Word 2&#39;], row[&#39;Cosine Similarity&#39;]))

# Function to apply colors to words in the final merged table
def apply_word_colors(word_pair_str, word_colors):
    colored_str = []
    for word in word_pair_str.split(&quot; - &quot;):
        color = word_colors.get(word, &#39;000000&#39;)  # Default to black if no color is found
        word_font = Font(color=color)
        colored_str.append(f&quot;{word} ({color})&quot;)  # Track color alongside the word
    return &quot; - &quot;.join(colored_str)

# Now we will merge words and apply colors to the font
for set_pair, word_pairs in set_pairs_dict.items():
    combined_words = []
    for word1, word2, _ in word_pairs:
        color_word1 = get_word_color(word1)
        color_word2 = get_word_color(word2)

        # Append words to the combined list, ensuring no duplicates
        combined_words.extend([word1, word2])

    # Remove duplicates and join them into one string for the word pair column
    combined_words = sorted(set(combined_words), key=lambda x: combined_words.index(x))
    word_pair_str = &quot; - &quot;.join(combined_words)

    # Add the set pair and word pair to the Excel sheet
    current_row = ws.max_row + 1
    ws.append([set_pair, word_pair_str])

    # Apply font color for each word in the final word pair
    current_cell = ws.cell(row=current_row, column=2)
    current_cell.value = word_pair_str

    for word in word_pair_str.split(&quot; - &quot;):
        # Set font color for each word (as per the color assigned previously)
        color = word_colors.get(word, &#39;000000&#39;)

        # Check if the word is identical and bold it
        is_bold = False
        for word1, word2, _ in word_pairs:
            if word1 == word2:
                is_bold = True

        # Apply the font color and bold if necessary
        current_cell.font = Font(color=color, bold=is_bold)

# Save the workbook to a file
wb.save(&quot;colored_word_pairs.xlsx&quot;)

print(&quot;Excel file with colored words and bold identical words has been created!&quot;)</code></pre>
<pre><code>Excel file with colored words and bold identical words has been created!</code></pre>
<pre class="python"><code># Initialize tracking variables
word1_to_color = {}
color_index = 0
previous_word = None
previous_color = None

# Create a new column &quot;Marker&quot; to hold &quot;*&quot; if &quot;Word 1&quot; equals &quot;Word 2&quot;
similarity_df_filtered[&#39;Marker&#39;] = similarity_df_filtered.apply(
    lambda row: &#39;*&#39; if row[&#39;Word 1&#39;] == row[&#39;Word 2&#39;] else &#39;&#39;, axis=1
)</code></pre>
<pre><code>&lt;string&gt;:3: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</code></pre>
<pre class="python"><code># Function to assign colors to rows, skipping color assignment if the &quot;Word 1&quot; is the same as the previous row&#39;s &quot;Word 1&quot;
def assign_colors(row):
    global previous_word, previous_color, color_index

    word1 = row[&#39;Word 1&#39;]
    
    # If the current &quot;Word 1&quot; is the same as the previous &quot;Word 1&quot;, reuse the color
    if word1 == previous_word:
        color = previous_color
    else:
        # Otherwise, assign the next color and update tracking variables
        color = color_palette[color_index % len(color_palette)]
        previous_color = color
        previous_word = word1
        color_index += 1
    
    # Return a list of styles for all columns except for &quot;Marker&quot; (bold)
    return [f&#39;background-color: {color}&#39;] * (len(row) - 1) + [&#39;font-weight: bold; color: black;&#39;]

# Apply the function to style the DataFrame
styled_similarity_df = similarity_df_filtered.style.apply(assign_colors, axis=1)

# Display the styled DataFrame
styled_similarity_df</code></pre>
<pre><code>&lt;pandas.io.formats.style.Styler object at 0x00000000CB5260D0&gt;</code></pre>
<pre class="python"><code>styled_similarity_df.to_html(&#39;similarity_table.html&#39;)</code></pre>
<pre class="python"><code>import pandas as pd
from collections import defaultdict

# Define the color map for sets
set_colors = {
    &quot;Bajta&quot;: &quot;yellow&quot;,
    &quot;Britto_2016&quot;: &quot;blue&quot;,
    &quot;Britto_2017&quot;: &quot;green&quot;,
    &quot;Dasthi&quot;: &quot;red&quot;,
    &quot;Mendes&quot;: &quot;purple&quot;,
    &quot;Usman&quot;: &quot;orange&quot;
}

# Initialize a dictionary to store words grouped by similarity
grouped_words = defaultdict(set)

# Iterate over the similarity dataframe
for index, row in similarity_df_filtered.iterrows():
    if row[&#39;Cosine Similarity&#39;] &gt;= 0.9:
        word1 = row[&#39;Word 1&#39;]
        word2 = row[&#39;Word 2&#39;]
        set1 = row[&#39;Set 1&#39;]
        set2 = row[&#39;Set 2&#39;]
        
        # Add words to the grouped dictionary with their respective sets
        grouped_words[word1].add(set1)
        grouped_words[word2].add(set2)

# Prepare the data for the final table
table_rows = []
for word, sets in grouped_words.items():
    # Color the words based on the sets they belong to
    color_coded_words = []
    for set_name in sets:
        # Assign the color based on the set
        color = set_colors[set_name]
        colored_word = f&#39;&lt;span style=&quot;color:{color}&quot;&gt;{word}&lt;/span&gt;&#39;
        color_coded_words.append(colored_word)
    
    # Prepare the row for this word and its sets
    sets_str = &quot;, &quot;.join([f&#39;&lt;span style=&quot;color:{set_colors[set_name]}&quot;&gt;{set_name}&lt;/span&gt;&#39; for set_name in sets])
    table_rows.append({
        &quot;Words&quot;: &quot;, &quot;.join(color_coded_words),  # Join words with their color applied
        &quot;Sets&quot;: sets_str  # Color-coded sets
    })

# Convert to a DataFrame
final_table = pd.DataFrame(table_rows)

# Collapse the table by the &quot;Sets&quot; column, merging words that have the same &quot;Sets&quot;
collapsed_table = final_table.groupby(&#39;Sets&#39;, as_index=False).agg({
    &#39;Words&#39;: &#39;, &#39;.join  # Join all words with the same set
})

# Display the collapsed table with HTML rendering
collapsed_html_output = collapsed_table.to_html(escape=False)  # escape=False allows HTML in the table

# Save the collapsed HTML output to a file
collapsed_file_path = &#39;collapsed_grouped_words_table_colored_new.html&#39;
with open(collapsed_file_path, &#39;w&#39;) as file:
    file.write(collapsed_html_output)

print(f&quot;Collapsed styled table saved to {collapsed_file_path}&quot;)



#########

import pandas as pd
from collections import defaultdict

# Define the color map for sets
set_colors = {
    &quot;Bajta&quot;: &quot;yellow&quot;,
    &quot;Britto_2016&quot;: &quot;blue&quot;,
    &quot;Britto_2017&quot;: &quot;green&quot;,
    &quot;Dasthi&quot;: &quot;red&quot;,
    &quot;Mendes&quot;: &quot;purple&quot;,
    &quot;Usman&quot;: &quot;orange&quot;
}

# Initialize a dictionary to store words grouped by similarity
grouped_words = defaultdict(set)

# Iterate over the similarity dataframe
for index, row in similarity_df_filtered.iterrows():
    if row[&#39;Cosine Similarity&#39;] &gt;= 0.9:
        word1 = row[&#39;Word 1&#39;]
        word2 = row[&#39;Word 2&#39;]
        set1 = row[&#39;Set 1&#39;]
        set2 = row[&#39;Set 2&#39;]
        
        # Add words to the grouped dictionary with their respective sets
        grouped_words[word1].add(set1)
        grouped_words[word2].add(set2)

# Prepare the data for the final table
table_rows = []
for word, sets in grouped_words.items():
    # Color the words based on the sets they belong to
    color_coded_words = []
    for set_name in sets:
        # Assign the color based on the set
        color = set_colors[set_name]
        colored_word = f&#39;&lt;span style=&quot;color:{color}&quot;&gt;{word}&lt;/span&gt;&#39;
        color_coded_words.append(colored_word)
    
    # Prepare the row for this word and its sets
    sets_str = &quot;, &quot;.join([f&#39;&lt;span style=&quot;color:{set_colors[set_name]}&quot;&gt;{set_name}&lt;/span&gt;&#39; for set_name in sets])
    table_rows.append({
        &quot;Words&quot;: &quot;, &quot;.join(color_coded_words),  # Join words with their color applied
        &quot;Sets&quot;: sets_str  # Color-coded sets
    })

# Convert to a DataFrame
final_table = pd.DataFrame(table_rows)

# Collapse the table by the &quot;Sets&quot; column, merging words that have the same &quot;Sets&quot;
collapsed_table = final_table.groupby(&#39;Sets&#39;, as_index=False).agg({
    &#39;Words&#39;: &#39;, &#39;.join  # Join all words with the same set
})

# Display the collapsed table with HTML rendering
collapsed_html_output = collapsed_table.to_html(escape=False)  # escape=False allows HTML in the table

# Save the collapsed HTML output to a file
collapsed_file_path = &#39;collapsed_grouped_words_table_colored_new.html&#39;
with open(collapsed_file_path, &#39;w&#39;) as file:
    file.write(collapsed_html_output)

print(f&quot;Collapsed styled table saved to {collapsed_file_path}&quot;)</code></pre>
</div>
<div id="how-many-differences-not-counting-duplicates"
class="section level1">
<h1>How many differences (not counting duplicates)</h1>
<pre class="python"><code>import torch
from transformers import AutoModel, AutoTokenizer
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Define the taxonomy characteristics
Bajta = {&#39;Conceptualization&#39;, &#39;Feasibility study&#39;, &#39;Preliminary planning&#39;, &#39;Detail Planning&#39;, &#39;Execution&#39;, &#39;Commissioning&#39;, &#39;System investigation&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Other&#39;, &#39;SE&#39;, &#39;Research &amp; Dev&#39;, &#39;Telecommunication&#39;, &#39;Finance&#39;, &#39;Healthcare&#39;, &#39;Other&#39;, &#39;Close onshore&#39;, &#39;Distant onshore&#39;, &#39;Near offshore&#39;, &#39;Far offshore&#39;, &#39;Constructive Cost Model&#39;, &#39;Capability Maturity Model Integration&#39;, &#39;Agile&#39;, &#39;Delphi&#39;, &#39;GA&#39;, &#39;CBR&#39;, &#39;Fuzzy similar&#39;, &#39;Other&#39;, &#39;Value&#39;, &#39;No of team members&#39;, &#39;Expert judgment&#39;, &#39;Machine learning&#39;, &#39;Non-machine learning&#39;, &#39;Individual&#39;, &#39;Group-based estimation&#39;, &#39;Estimate value&#39;, &#39;Value&#39;, &#39;Effort hours&#39;, &#39;Staff/cost&#39;, &#39;Hardware&#39;, &#39;Risk&#39;, &#39;Portfolio&#39;, &#39;Baseline comparison&#39;, &#39;Variation reduction&#39;, &#39;Sensitivity analysis&#39;, &#39;Size report&#39;, &#39;Statistics analysis&#39;, &#39;Considered&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Other&#39;, &#39;Geographical distance&#39;, &#39;Temporal distance&#39;, &#39;Socio-cultural distance&#39;}
Britto_2017 = {&#39;Web page count&#39;, &#39;Media count&#39;, &#39;New media count&#39;, &#39;New Web page count&#39;, &#39;Link count&#39;, &#39;Program count&#39;, &#39;Reused component count&#39;, &#39;Lines of code&#39;, &#39;Reused program count&#39;, &#39;Reused media count&#39;, &#39;Web page allocation&#39;, &#39;Reused lines of code&#39;, &#39;Media allocation&#39;, &#39;Reused media allocation&#39;, &#39;Entity count&#39;, &#39;Attribute count&#39;, &#39;Component count&#39;, &#39;Statement count&#39;, &#39;Node count&#39;, &#39;Collection slot size&#39;, &#39;Component granularity level&#39;, &#39;Slot granularity level&#39;, &#39;Model node size&#39;, &#39;Cluster node size&#39;, &#39;Node slot size&#39;, &#39;Publishing model unit count&#39;, &#39;Model slot size&#39;, &#39;Association slot size&#39;, &#39;Client script count&#39;, &#39;Server script count&#39;, &#39;Information slot count&#39;, &#39;Association center slot count&#39;, &#39;Collection center slot count&#39;, &#39;Component slot count&#39;, &#39;Semantic association count&#39;, &#39;Segment count&#39;, &#39;Slot count&#39;, &#39;Cluster slot count&#39;, &#39;Cluster count&#39;, &#39;Publishing unit count&#39;, &#39;Section count&#39;, &#39;Inner/sub concern count&#39;, &#39;Indifferent concern count&#39;, &#39;Module point cut count&#39;, &#39;Module count&#39;, &#39;Module attribute count&#39;, &#39;Operation count&#39;, &#39;Comment count&#39;, &#39;Reused comment count&#39;, &#39;Media duration&#39;, &#39;Diffusion cut count&#39;, &#39;Concern module count&#39;, &#39;Concern operation count&#39;, &#39;Anchor count&#39;, &#39;High feature count&#39;, &#39;Low feature count&#39;, &#39;Reused high feature count&#39;, &#39;Reused low feature count&#39;, &#39;Web objects&#39;, &#39;Common Software Measurement International Consortium&#39;, &#39;International Function Point Users Group&#39;, &#39;Object-Oriented Heuristic Function Points&#39;, &#39;Object-Oriented Function Points&#39;, &#39;Use case count&#39;, &#39;Feature count&#39;, &#39;Data Web points&#39;, &#39;Cohesion&#39;, &#39;Class coupling&#39;, &#39;Concern coupling&#39;, &#39;Connectivity density&#39;, &#39;Cyclomatic complexity&#39;, &#39;Model collection complexity&#39;, &#39;Model association complexity&#39;, &#39;Model link complexity&#39;, &#39;Page complexity&#39;, &#39;Component complexity&#39;, &#39;Total complexity&#39;, &#39;Adaptation complexity&#39;, &#39;New complexity&#39;, &#39;Data usage complexity&#39;, &#39;Data flow complexity&#39;, &#39;Cohesion complexity&#39;, &#39;Interface complexity&#39;, &#39;Control flow complexity&#39;, &#39;Class complexity&#39;, &#39;Layout complexity&#39;, &#39;Input complexity&#39;, &#39;Output complexity&#39;, &#39;Product.Type&#39;, &#39;Stratum&#39;, &#39;Compactness&#39;, &#39;Product.Structure&#39;, &#39;Architecture&#39;, &#39;Integration with legacy systems&#39;, &#39;Concurrency level&#39;, &#39;Processing requirements&#39;, &#39;Database size&#39;, &#39;Requirements volatility level&#39;, &#39;Requirements novelty level&#39;, &#39;Reliability level&#39;, &#39;Maintainability level&#39;, &#39;Time efficiency level&#39;, &#39;Memory efficiency level&#39;, &#39;Portability level&#39;, &#39;Scalability level&#39;, &#39;Quality level&#39;, &#39;Usability level&#39;, &#39;Readability level&#39;, &#39;Security level&#39;, &#39;Installability level&#39;, &#39;Modularity level&#39;, &#39;Flexibility level&#39;, &#39;Testability level&#39;, &#39;Accessibility level&#39;, &#39;Trainability level&#39;, &#39;Innovation level&#39;, &#39;Technical factors&#39;, &#39;Storage constraint&#39;, &#39;Reusability level&#39;, &#39;Robustness level&#39;, &#39;Design volatility&#39;, &#39;Product.Experience level&#39;, &#39;Requirements clarity level&#39;, &#39;Availability level&#39;, &#39;IT literacy&#39;, &#39;Mapped workflows&#39;, &#39;Personality&#39;, &#39;SPI program&#39;, &#39;Metrics program&#39;, &#39;Number of projects in parallel&#39;, &#39;Software reuse&#39;, &#39;Documentation level&#39;, &#39;Number of programming languages&#39;, &#39;Project.Type&#39;, &#39;Process efficiency level&#39;, &#39;Project management level&#39;, &#39;Project.Infrastructure&#39;, &#39;Development restriction&#39;, &#39;Time restriction&#39;, &#39;Risk level&#39;, &#39;Rapid app development&#39;, &#39;Operational mode&#39;, &#39;Resource level&#39;, &#39;Lessons learned repository&#39;, &#39;Domain experience level&#39;, &#39;Team size&#39;, &#39;Deployment platform experience level&#39;, &#39;Team capability&#39;, &#39;Programming language experience level&#39;, &#39;Tool experience level&#39;, &#39;Communication level&#39;, &#39;Software development experience&#39;, &#39;Work Team level&#39;, &#39;Stability level&#39;, &#39;Motivation level&#39;, &#39;Focus factor&#39;, &#39;OO experience level&#39;, &#39;In-house experience&#39;, &#39;Authoring tool type&#39;, &#39;Productivity level&#39;, &#39;Novelty level&#39;, &#39;Platform volatility level&#39;, &#39;Difficulty level&#39;, &#39;Platform support level&#39;}
Britto_2016 = {&#39;Site.Location&#39;, &#39;Site.Legal Entity&#39;, &#39;Site.Geographic Distance&#39;, &#39;Site.Temporal Distance&#39;, &#39;Early&#39;, &#39;Early &amp; Late&#39;, &#39;Late&#39;, &#39;Estimator&#39;, &#39;Estimator &amp; Provider&#39;, &#39;Provider&#39;, &#39;Relationship.Location&#39;, &#39;Relationship.Legal Entity&#39;, &#39;Relationship.Geographic Distance&#39;, &#39;Relationship.Temporal Distance&#39;, &#39;Centralized&#39;, &#39;Distributed&#39;, &#39;Semi-distributed&#39;}
Dasthi = {&#39;Constructive Cost Model&#39;, &#39;Software Life Cycle Management&#39;, &#39;Software Evaluation and Estimation for Risk&#39;, &#39;Expert Judgment&#39;, &#39;Analogy-Based&#39;, &#39;Basic-Combination&#39;, &#39;Fuzzy Logic&#39;, &#39;Artificial Neural Networks&#39;, &#39;Computational Intelligence.swarm&#39;, &#39;Computational Intelligence.evolutionary&#39;, &#39;AI-Combined hybrid&#39;}
Mendes = {&#39;Motivation&#39;, &#39;Early size metric&#39;, &#39;Late size metric&#39;, &#39;Problem-oriented metric&#39;, &#39;Solution-oriented metric&#39;, &#39;Class.Length&#39;, &#39;Functionality&#39;, &#39;Complexity&#39;, &#39;Web hypermedia application&#39;, &#39;Web software application&#39;, &#39;Web application&#39;, &#39;Media&#39;, &#39;Program/Script&#39;, &#39;Nominal&#39;, &#39;Ordinal&#39;, &#39;Interval&#39;, &#39;Ratio&#39;, &#39;Absolute&#39;, &#39;Computation.Direct&#39;, &#39;Computation.Indirect&#39;, &#39;Validated Empirically&#39;, &#39;Validated Theoretically&#39;, &#39;Validation.Both&#39;, &#39;Validation.None&#39;, &#39;Model dependency.Specific&#39;, &#39;Model dependency.Nonspecific&#39;}
Usman = {&#39;Release&#39;, &#39;Sprint&#39;, &#39;Daily&#39;, &#39;Bidding&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Estimated activities.All&#39;, &#39;Extreme Programming&#39;, &#39;Scrum&#39;, &#39;Customized Extreme Programming&#39;, &#39;Customized Scrum&#39;, &#39;Dynamic Systems Development Method&#39;, &#39;Crystal&#39;, &#39;Feature-Driven Development&#39;, &#39;Kanban&#39;, &#39;Communications industry&#39;, &#39;Transportation&#39;, &#39;Financial&#39;, &#39;Education&#39;, &#39;Health&#39;, &#39;Retail/Wholesale&#39;, &#39;Manufacturing&#39;, &#39;Government/Military&#39;, &#39;Project domain.Other&#39;, &#39;Co-located&#39;, &#39;Distributed: Close Onshore&#39;, &#39;Distributed: Distant Onshore&#39;, &#39;Distributed: Near Offshore&#39;, &#39;Distributed: Far Offshore&#39;, &#39;User story&#39;, &#39;Task&#39;, &#39;Use case&#39;, &#39;Estimation entity.Other&#39;, &#39;Number of entities estimated.Value&#39;, &#39;No. of team members&#39;, &#39;Planning Poker&#39;, &#39;Expert Judgement&#39;, &#39;Analogy&#39;, &#39;Use case points method&#39;, &#39;Estimation Techniques.Other&#39;, &#39;Single&#39;, &#39;Group&#39;, &#39;Story points&#39;, &#39;User case points&#39;, &#39;Function points&#39;, &#39;Size.Other&#39;, &#39;Not used&#39;, &#39;Considered without any metric&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Non functional requirements.Other&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Not applicable&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Estimate value(s)&#39;, &#39;Actual effort.Value&#39;, &#39;Point&#39;, &#39;Three point&#39;, &#39;Distribution&#39;, &#39;Effort estimate.Type.Other&#39;, &#39;Hours/days&#39;, &#39;Pair days&#39;, &#39;Ideal hours&#39;, &#39;Unit.Other&#39;, &#39;Accuracy Level.Value&#39;, &#39;Mean Magnitude of Relative Error&#39;, &#39;Median Magnitude of Relative Error&#39;, &#39;Bias of Relative Error&#39;, &#39;Accuracy measure.Other&#39;, &#39;Not used&#39;}

taxonomies = {
    &#39;Bajta&#39;: Bajta,
    &#39;Britto_2017&#39;: Britto_2017,
    &#39;Britto_2016&#39;: Britto_2016,
    &#39;Dasthi&#39;: Dasthi,
    &#39;Mendes&#39;: Mendes,
    &#39;Usman&#39;: Usman
}


def load_embedding_model():
    &quot;&quot;&quot;Load the Jina AI embeddings model&quot;&quot;&quot;
    print(&quot;Loading Jina AI embeddings model...&quot;)
    model_name = &quot;jinaai/jina-embeddings-v3&quot;
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(&quot;Model loaded successfully!&quot;)
    return model, tokenizer

def get_embedding(word, model, tokenizer):
    &quot;&quot;&quot;Get embedding for a single word&quot;&quot;&quot;
    inputs = tokenizer(word, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def find_unique_characteristics(taxonomies, similarity_threshold=0.9):
    &quot;&quot;&quot;
    Find characteristics that are unique to each taxonomy (not similar to any other taxonomy)
    
    Args:
        taxonomies: Dictionary of taxonomy_name -&gt; set of characteristics
        similarity_threshold: Minimum similarity to consider characteristics as &quot;shared&quot;
    
    Returns:
        Dictionary with unique characteristics per taxonomy and summary statistics
    &quot;&quot;&quot;
    
    # Load model
    model, tokenizer = load_embedding_model()
    
    # Normalize all characteristics to lowercase for consistency
    normalized_taxonomies = {}
    for name, chars in taxonomies.items():
        normalized_taxonomies[name] = {char.lower() for char in chars}
    
    # Store embeddings for all characteristics
    all_embeddings = {}
    print(&quot;Generating embeddings for all characteristics...&quot;)
    
    for taxonomy_name, characteristics in normalized_taxonomies.items():
        print(f&quot;Processing {taxonomy_name}: {len(characteristics)} characteristics&quot;)
        for char in characteristics:
            if char not in all_embeddings:  # Avoid recomputing for duplicate characteristics
                all_embeddings[char] = get_embedding(char, model, tokenizer)
    
    print(f&quot;Generated embeddings for {len(all_embeddings)} unique characteristics&quot;)
    
    # Find unique characteristics for each taxonomy
    unique_results = {}
    
    for target_taxonomy, target_chars in normalized_taxonomies.items():
        print(f&quot;\nAnalyzing unique characteristics for {target_taxonomy}...&quot;)
        unique_chars = []
        
        for char in target_chars:
            is_unique = True
            char_embedding = all_embeddings[char].reshape(1, -1)
            
            # Check similarity with characteristics from all other taxonomies
            for other_taxonomy, other_chars in normalized_taxonomies.items():
                if other_taxonomy == target_taxonomy:
                    continue
                
                for other_char in other_chars:
                    if char == other_char:  # Exact match
                        is_unique = False
                        break
                    
                    other_embedding = all_embeddings[other_char].reshape(1, -1)
                    similarity = cosine_similarity(char_embedding, other_embedding)[0][0]
                    
                    if similarity &gt;= similarity_threshold:
                        is_unique = False
                        break
                
                if not is_unique:
                    break
            
            if is_unique:
                unique_chars.append(char)
        
        unique_results[target_taxonomy] = {
            &#39;unique_characteristics&#39;: unique_chars,
            &#39;count&#39;: len(unique_chars),
            &#39;total_characteristics&#39;: len(target_chars),
            &#39;percentage_unique&#39;: (len(unique_chars) / len(target_chars)) * 100
        }
        
        print(f&quot;{target_taxonomy}: {len(unique_chars)}/{len(target_chars)} unique characteristics ({unique_results[target_taxonomy][&#39;percentage_unique&#39;]:.1f}%)&quot;)
    
    return unique_results

def display_results(unique_results):
    &quot;&quot;&quot;Display results in a formatted way&quot;&quot;&quot;
    print(&quot;\n&quot; + &quot;=&quot;*80)
    print(&quot;UNIQUE CHARACTERISTICS ANALYSIS RESULTS&quot;)
    print(&quot;=&quot;*80)
    
    # Summary table
    print(&quot;\nSUMMARY TABLE:&quot;)
    print(&quot;-&quot; * 80)
    print(f&quot;{&#39;Taxonomy&#39;:&lt;15} {&#39;Total Chars&#39;:&lt;12} {&#39;Unique Chars&#39;:&lt;12} {&#39;% Unique&#39;:&lt;10} {&#39;Unique Characteristics&#39;}&quot;)
    print(&quot;-&quot; * 80)
    
    total_chars = 0
    total_unique = 0
    
    for taxonomy, results in unique_results.items():
        total_chars += results[&#39;total_characteristics&#39;]
        total_unique += results[&#39;count&#39;]
        
        unique_list = &#39;, &#39;.join(results[&#39;unique_characteristics&#39;][:3])  # Show first 3
        if len(results[&#39;unique_characteristics&#39;]) &gt; 3:
            unique_list += f&quot;, ... (+{len(results[&#39;unique_characteristics&#39;])-3} more)&quot;
        
        print(f&quot;{taxonomy:&lt;15} {results[&#39;total_characteristics&#39;]:&lt;12} {results[&#39;count&#39;]:&lt;12} {results[&#39;percentage_unique&#39;]:&lt;9.1f}% {unique_list}&quot;)
    
    print(&quot;-&quot; * 80)
    print(f&quot;{&#39;TOTAL&#39;:&lt;15} {total_chars:&lt;12} {total_unique:&lt;12} {(total_unique/total_chars)*100:&lt;9.1f}%&quot;)
    
    # Detailed breakdown
    print(&quot;\n\nDETAILED BREAKDOWN:&quot;)
    print(&quot;=&quot;*50)
    
    for taxonomy, results in unique_results.items():
        print(f&quot;\n{taxonomy.upper()} - Unique Characteristics ({results[&#39;count&#39;]}):&quot;)
        print(&quot;-&quot; * 50)
        if results[&#39;unique_characteristics&#39;]:
            for i, char in enumerate(sorted(results[&#39;unique_characteristics&#39;]), 1):
                print(f&quot;{i:2d}. {char}&quot;)
        else:
            print(&quot;   No unique characteristics found&quot;)

def save_results_to_csv(unique_results, filename=&quot;unique_characteristics_analysis.csv&quot;):
    &quot;&quot;&quot;Save results to CSV file&quot;&quot;&quot;
    data = []
    
    for taxonomy, results in unique_results.items():
        for char in results[&#39;unique_characteristics&#39;]:
            data.append({
                &#39;Taxonomy&#39;: taxonomy,
                &#39;Unique_Characteristic&#39;: char,
                &#39;Total_Characteristics_in_Taxonomy&#39;: results[&#39;total_characteristics&#39;],
                &#39;Total_Unique_in_Taxonomy&#39;: results[&#39;count&#39;],
                &#39;Percentage_Unique_in_Taxonomy&#39;: results[&#39;percentage_unique&#39;]
            })
    
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f&quot;\nResults saved to {filename}&quot;)
    
    return df

# Main execution
if __name__ == &quot;__main__&quot;:
    print(&quot;Starting Unique Characteristics Analysis...&quot;)
    print(f&quot;Analyzing {len(taxonomies)} taxonomies with similarity threshold of 90%&quot;)
    
    # Run the analysis
    results = find_unique_characteristics(taxonomies, similarity_threshold=0.9)
    
    # Display results
    display_results(results)
    
    # Save to CSV
    df = save_results_to_csv(results)
    
    print(&quot;\nAnalysis complete!&quot;)</code></pre>
<pre><code>Starting Unique Characteristics Analysis...
Analyzing 6 taxonomies with similarity threshold of 90%
Loading Jina AI embeddings model...
Model loaded successfully!
Generating embeddings for all characteristics...
Processing Bajta: 57 characteristics
Processing Britto_2017: 164 characteristics
Processing Britto_2016: 17 characteristics
Processing Dasthi: 11 characteristics
Processing Mendes: 26 characteristics
Processing Usman: 75 characteristics
Generated embeddings for 336 unique characteristics

Analyzing unique characteristics for Bajta...
Bajta: 39/57 unique characteristics (68.4%)

Analyzing unique characteristics for Britto_2017...
Britto_2017: 158/164 unique characteristics (96.3%)

Analyzing unique characteristics for Britto_2016...
Britto_2016: 17/17 unique characteristics (100.0%)

Analyzing unique characteristics for Dasthi...
Dasthi: 8/11 unique characteristics (72.7%)

Analyzing unique characteristics for Mendes...
Mendes: 24/26 unique characteristics (92.3%)

Analyzing unique characteristics for Usman...
Usman: 57/75 unique characteristics (76.0%)

================================================================================
UNIQUE CHARACTERISTICS ANALYSIS RESULTS
================================================================================

SUMMARY TABLE:
--------------------------------------------------------------------------------
Taxonomy        Total Chars  Unique Chars % Unique   Unique Characteristics
--------------------------------------------------------------------------------
Bajta           57           39           68.4     % variation reduction, conceptualization, risk, ... (+36 more)
Britto_2017     164          158          96.3     % connectivity density, in-house experience, entity count, ... (+155 more)
Britto_2016     17           17           100.0    % semi-distributed, estimator &amp; provider, provider, ... (+14 more)
Dasthi          11           8            72.7     % basic-combination, ai-combined hybrid, software life cycle management, ... (+5 more)
Mendes          26           24           92.3     % validated theoretically, absolute, web software application, ... (+21 more)
Usman           75           57           76.0     % bidding, pair days, mean magnitude of relative error, ... (+54 more)
--------------------------------------------------------------------------------
TOTAL           350          303          86.6     %


DETAILED BREAKDOWN:
==================================================

BAJTA - Unique Characteristics (39):
--------------------------------------------------
 1. agile
 2. baseline comparison
 3. capability maturity model integration
 4. cbr
 5. close onshore
 6. commissioning
 7. conceptualization
 8. delphi
 9. detail planning
10. effort hours
11. execution
12. far offshore
13. feasibility study
14. fuzzy similar
15. ga
16. geographical distance
17. group-based estimation
18. hardware
19. healthcare
20. individual
21. machine learning
22. near offshore
23. non-machine learning
24. other
25. portfolio
26. preliminary planning
27. research &amp; dev
28. risk
29. se
30. sensitivity analysis
31. size report
32. socio-cultural distance
33. staff/cost
34. statistics analysis
35. system investigation
36. telecommunication
37. temporal distance
38. value
39. variation reduction

BRITTO_2017 - Unique Characteristics (158):
--------------------------------------------------
 1. accessibility level
 2. adaptation complexity
 3. anchor count
 4. architecture
 5. association center slot count
 6. association slot size
 7. attribute count
 8. authoring tool type
 9. class complexity
10. class coupling
11. client script count
12. cluster count
13. cluster node size
14. cluster slot count
15. cohesion
16. cohesion complexity
17. collection center slot count
18. collection slot size
19. comment count
20. common software measurement international consortium
21. communication level
22. compactness
23. component complexity
24. component count
25. component granularity level
26. component slot count
27. concern coupling
28. concern module count
29. concern operation count
30. concurrency level
31. connectivity density
32. control flow complexity
33. cyclomatic complexity
34. data flow complexity
35. data usage complexity
36. data web points
37. database size
38. deployment platform experience level
39. design volatility
40. development restriction
41. difficulty level
42. diffusion cut count
43. documentation level
44. domain experience level
45. entity count
46. feature count
47. flexibility level
48. focus factor
49. high feature count
50. in-house experience
51. indifferent concern count
52. information slot count
53. inner/sub concern count
54. innovation level
55. input complexity
56. installability level
57. integration with legacy systems
58. interface complexity
59. international function point users group
60. it literacy
61. layout complexity
62. lessons learned repository
63. lines of code
64. link count
65. low feature count
66. mapped workflows
67. media allocation
68. media count
69. media duration
70. memory efficiency level
71. metrics program
72. model association complexity
73. model collection complexity
74. model link complexity
75. model node size
76. model slot size
77. modularity level
78. module attribute count
79. module count
80. module point cut count
81. new media count
82. new web page count
83. node count
84. node slot size
85. novelty level
86. number of programming languages
87. number of projects in parallel
88. object-oriented function points
89. object-oriented heuristic function points
90. oo experience level
91. operation count
92. operational mode
93. output complexity
94. page complexity
95. personality
96. platform support level
97. platform volatility level
98. portability level
99. process efficiency level
100. processing requirements
101. product.experience level
102. product.structure
103. product.type
104. productivity level
105. program count
106. programming language experience level
107. project management level
108. project.infrastructure
109. project.type
110. publishing model unit count
111. publishing unit count
112. quality level
113. rapid app development
114. readability level
115. reliability level
116. requirements clarity level
117. requirements novelty level
118. requirements volatility level
119. resource level
120. reusability level
121. reused comment count
122. reused component count
123. reused high feature count
124. reused lines of code
125. reused low feature count
126. reused media allocation
127. reused media count
128. reused program count
129. risk level
130. robustness level
131. scalability level
132. section count
133. security level
134. segment count
135. semantic association count
136. server script count
137. slot count
138. slot granularity level
139. software development experience
140. software reuse
141. spi program
142. stability level
143. statement count
144. storage constraint
145. stratum
146. team capability
147. technical factors
148. testability level
149. time efficiency level
150. time restriction
151. tool experience level
152. trainability level
153. usability level
154. use case count
155. web objects
156. web page allocation
157. web page count
158. work team level

BRITTO_2016 - Unique Characteristics (17):
--------------------------------------------------
 1. centralized
 2. distributed
 3. early
 4. early &amp; late
 5. estimator
 6. estimator &amp; provider
 7. late
 8. provider
 9. relationship.geographic distance
10. relationship.legal entity
11. relationship.location
12. relationship.temporal distance
13. semi-distributed
14. site.geographic distance
15. site.legal entity
16. site.location
17. site.temporal distance

DASTHI - Unique Characteristics (8):
--------------------------------------------------
 1. ai-combined hybrid
 2. artificial neural networks
 3. basic-combination
 4. computational intelligence.evolutionary
 5. computational intelligence.swarm
 6. fuzzy logic
 7. software evaluation and estimation for risk
 8. software life cycle management

MENDES - Unique Characteristics (24):
--------------------------------------------------
 1. absolute
 2. class.length
 3. computation.direct
 4. computation.indirect
 5. early size metric
 6. functionality
 7. interval
 8. late size metric
 9. media
10. model dependency.nonspecific
11. model dependency.specific
12. nominal
13. ordinal
14. problem-oriented metric
15. program/script
16. ratio
17. solution-oriented metric
18. validated empirically
19. validated theoretically
20. validation.both
21. validation.none
22. web application
23. web hypermedia application
24. web software application

USMAN - Unique Characteristics (57):
--------------------------------------------------
 1. accuracy level.value
 2. accuracy measure.other
 3. actual effort.value
 4. bias of relative error
 5. bidding
 6. co-located
 7. communications industry
 8. considered without any metric
 9. crystal
10. customized extreme programming
11. customized scrum
12. daily
13. distributed: close onshore
14. distributed: far offshore
15. distributed: near offshore
16. distribution
17. dynamic systems development method
18. education
19. effort estimate.type.other
20. estimated activities.all
21. estimation entity.other
22. estimation techniques.other
23. extreme programming
24. feature-driven development
25. function points
26. government/military
27. group
28. health
29. hours/days
30. ideal hours
31. kanban
32. manufacturing
33. mean magnitude of relative error
34. median magnitude of relative error
35. non functional requirements.other
36. not applicable
37. not used
38. number of entities estimated.value
39. pair days
40. planning poker
41. point
42. project domain.other
43. release
44. retail/wholesale
45. scrum
46. single
47. size.other
48. sprint
49. story points
50. task
51. three point
52. transportation
53. unit.other
54. use case
55. use case points method
56. user case points
57. user story

Results saved to unique_characteristics_analysis.csv

Analysis complete!</code></pre>
</div>
<div id="categoriescharacteristics-count" class="section level1">
<h1>Categories/characteristics count</h1>
<pre class="python"><code>def count_taxonomy_nodes(taxonomy_dict, taxonomy_name=None):
    &quot;&quot;&quot;
    Count inner nodes (categories) and outer nodes (leaves/characteristics) in a taxonomy.
    
    Args:
        taxonomy_dict: Dictionary representing the taxonomy structure
        taxonomy_name: Optional name for the taxonomy (for display purposes)
    
    Returns:
        Dictionary with counts and detailed breakdown
    &quot;&quot;&quot;
    
    inner_nodes = []  # Categories (excluding root)
    outer_nodes = []  # Characteristics/leaves
    
    def traverse_taxonomy(node, path=None, is_root=True):
        &quot;&quot;&quot;Recursively traverse the taxonomy structure&quot;&quot;&quot;
        if path is None:
            path = []
            
        if isinstance(node, dict):
            for key, value in node.items():
                current_path = path + [key]
                
                if isinstance(value, dict):
                    # This key represents a category (inner node)
                    if not is_root:  # Don&#39;t count the root level
                        inner_nodes.append({
                            &#39;name&#39;: key,
                            &#39;path&#39;: &#39; -&gt; &#39;.join(current_path),
                            &#39;depth&#39;: len(current_path)
                        })
                    
                    # Recurse into the category
                    traverse_taxonomy(value, current_path, is_root=False)
                    
                elif isinstance(value, str):
                    # This key represents a characteristic (leaf node)
                    outer_nodes.append({
                        &#39;name&#39;: key,
                        &#39;value&#39;: value,
                        &#39;path&#39;: &#39; -&gt; &#39;.join(current_path),
                        &#39;depth&#39;: len(current_path)
                    })
    
    # Start traversal
    traverse_taxonomy(taxonomy_dict)
    
    # Calculate statistics
    results = {
        &#39;taxonomy_name&#39;: taxonomy_name or &#39;Unnamed Taxonomy&#39;,
        &#39;inner_nodes&#39;: {
            &#39;count&#39;: len(inner_nodes),
            &#39;details&#39;: inner_nodes
        },
        &#39;outer_nodes&#39;: {
            &#39;count&#39;: len(outer_nodes),
            &#39;details&#39;: outer_nodes
        },
        &#39;total_nodes&#39;: len(inner_nodes) + len(outer_nodes),
        &#39;depth_stats&#39;: {
            &#39;max_depth&#39;: max([node[&#39;depth&#39;] for node in inner_nodes + outer_nodes]) if inner_nodes + outer_nodes else 0,
            &#39;min_depth&#39;: min([node[&#39;depth&#39;] for node in inner_nodes + outer_nodes]) if inner_nodes + outer_nodes else 0
        }
    }
    
    return results

def display_results(results):
    &quot;&quot;&quot;Display the counting results in a formatted way&quot;&quot;&quot;
    print(&quot;=&quot; * 80)
    print(f&quot;TAXONOMY NODE ANALYSIS: {results[&#39;taxonomy_name&#39;]}&quot;)
    print(&quot;=&quot; * 80)
    
    print(f&quot;\nSUMMARY:&quot;)
    print(f&quot;   Inner Nodes (Categories): {results[&#39;inner_nodes&#39;][&#39;count&#39;]}&quot;)
    print(f&quot;   Outer Nodes (Leaves):     {results[&#39;outer_nodes&#39;][&#39;count&#39;]}&quot;)
    print(f&quot;   Total Nodes:              {results[&#39;total_nodes&#39;]}&quot;)
    print(f&quot;   Max Depth:                {results[&#39;depth_stats&#39;][&#39;max_depth&#39;]}&quot;)
    print(f&quot;   Min Depth:                {results[&#39;depth_stats&#39;][&#39;min_depth&#39;]}&quot;)
    
    # Simple lists
    inner_node_names = [node[&#39;name&#39;] for node in results[&#39;inner_nodes&#39;][&#39;details&#39;]]
    outer_node_names = [node[&#39;name&#39;] for node in results[&#39;outer_nodes&#39;][&#39;details&#39;]]
    
    print(f&quot;\nINNER NODES LIST ({len(inner_node_names)} total):&quot;)
    print(inner_node_names)
    
    print(f&quot;\nOUTER NODES LIST ({len(outer_node_names)} total):&quot;)
    print(outer_node_names)
    

def analyze_multiple_taxonomies(taxonomies_dict):
    &quot;&quot;&quot;Analyze multiple taxonomies and provide comparative results&quot;&quot;&quot;
    all_results = {}
    
    print(&quot;ANALYZING MULTIPLE TAXONOMIES&quot;)
    print(&quot;=&quot; * 80)
    
    for name, taxonomy in taxonomies_dict.items():
        print(f&quot;\nProcessing: {name}&quot;)
        results = count_taxonomy_nodes(taxonomy, name)
        all_results[name] = results
        
        # Brief summary for each
        print(f&quot;  Inner nodes: {results[&#39;inner_nodes&#39;][&#39;count&#39;]}, &quot;
              f&quot;Outer nodes: {results[&#39;outer_nodes&#39;][&#39;count&#39;]}, &quot;
              f&quot;Total: {results[&#39;total_nodes&#39;]}&quot;)
    
    print(&quot;\n&quot; + &quot;=&quot; * 80)
    print(&quot;COMPARATIVE SUMMARY&quot;)
    print(&quot;=&quot; * 80)
    print(f&quot;{&#39;Taxonomy&#39;:&lt;25} {&#39;Inner&#39;:&lt;8} {&#39;Outer&#39;:&lt;8} {&#39;Total&#39;:&lt;8} {&#39;Max Depth&#39;:&lt;10}&quot;)
    print(&quot;-&quot; * 80)
    
    for name, results in all_results.items():
        print(f&quot;{name:&lt;25} {results[&#39;inner_nodes&#39;][&#39;count&#39;]:&lt;8} &quot;
              f&quot;{results[&#39;outer_nodes&#39;][&#39;count&#39;]:&lt;8} {results[&#39;total_nodes&#39;]:&lt;8} &quot;
              f&quot;{results[&#39;depth_stats&#39;][&#39;max_depth&#39;]:&lt;10}&quot;)
    
    return all_results


new_taxonomy = {
    &#39;Effort Estimation in ASD&#39;: {
        &#39;Estimation context&#39;: {
            &quot;Planning level&quot;: {
                &quot;Release&quot;: &quot;Release&quot;,
                &quot;Sprint&quot;: &quot;Sprint&quot;,
                &quot;Daily&quot;: &quot;Daily&quot;,
                &quot;Bidding&quot;: &quot;Bidding&quot;
            },
            &quot;Estimated activities&quot;: {
                &quot;Analysis&quot;: &quot;Analysis&quot;,
                &quot;Design&quot;: &quot;Design&quot;,
                &quot;Implementation&quot;: &quot;Implementation&quot;,
                &quot;Testing&quot;: &quot;Testing&quot;,
                &quot;Maintenance&quot;: &quot;Maintenance&quot;,
                &quot;Estimated activities.All&quot;: &quot;Estimated activities.All&quot;
            },
            &quot;Agile methods&quot;: {
                &quot;Extreme Programming&quot;: &quot;Extreme Programming&quot;,
                &quot;Scrum&quot;: &quot;Scrum&quot;,
                &quot;Customized Extreme Programming&quot;: &quot;Customized Extreme Programming&quot;,
                &quot;Customized Scrum&quot;: &quot;Customized Scrum&quot;,
                &quot;Dynamic Systems Development Method&quot;: &quot;Dynamic Systems Development Method&quot;,
                &quot;Crystal&quot;: &quot;Crystal&quot;,
                &quot;Feature-Driven Development&quot;: &quot;Feature-Driven Development&quot;,
                &quot;Kanban&quot;: &quot;Kanban&quot;
            },
            &quot;Project domain&quot;: {
                &quot;Communications industry&quot;: &quot;Communications industry&quot;,
                &quot;Transportation&quot;: &quot;Transportation&quot;,
                &quot;Financial&quot;: &quot;Financial&quot;,
                &quot;Education&quot;: &quot;Education&quot;,
                &quot;Health&quot;: &quot;Health&quot;,
                &quot;Retail/Wholesale&quot;: &quot;Retail/Wholesale&quot;,
                &quot;Manufacturing&quot;: &quot;Manufacturing&quot;,
                &quot;Government/Military&quot;: &quot;Government/Military&quot;,
                &quot;Project domain.Other&quot;: &quot;Project domain.Other&quot;
            },
            &quot;Project setting&quot;: {
                &quot;Co-located&quot;: &quot;Co-located&quot;,
                &quot;Distributed: Close Onshore&quot;: &quot;Distributed: Close Onshore&quot;,
                &quot;Distributed: Distant Onshore&quot;: &quot;Distributed: Distant Onshore&quot;,
                &quot;Distributed: Near Offshore&quot;: &quot;Distributed: Near Offshore&quot;,
                &quot;Distributed: Far Offshore&quot;: &quot;Distributed: Far Offshore&quot;
            },
            &quot;Estimation entity&quot;: {
                &quot;User story&quot;: &quot;User story&quot;,
                &quot;Task&quot;: &quot;Task&quot;,
                &quot;Use case&quot;: &quot;Use case&quot;,
                &quot;Estimation entity.Other&quot;: &quot;Estimation entity.Other&quot;
            },
            &quot;Number of entities estimated&quot;: {
                &quot;Number of entities estimated.Value&quot;: &quot;Number of entities estimated.Value&quot;
            },
            &quot;Team size&quot;: {
                &quot;No. of team members&quot;: &quot;No. of team members&quot;
            }
        },
        &#39;Estimation technique&#39;: {
            &quot;Estimation Techniques&quot;: {
                &quot;Planning Poker&quot;: &quot;Planning Poker&quot;,
                &quot;Expert Judgement&quot;: &quot;Expert Judgement&quot;,
                &quot;Analogy&quot;: &quot;Analogy&quot;,
                &quot;Use case points method&quot;: &quot;Use case points method&quot;,
                &quot;Estimation Techniques.Other&quot;: &quot;Estimation Techniques.Other&quot;
            },
            &quot;Type&quot;: {
                &quot;Single&quot;: &quot;Single&quot;,
                &quot;Group&quot;: &quot;Group&quot;
            }
        },
        &#39;Effort predictors&#39;: {
            &quot;Size&quot;: {
                &quot;Story points&quot;: &quot;Story points&quot;,
                &quot;User case points&quot;: &quot;User case points&quot;,
                &quot;Function points&quot;: &quot;Function points&quot;,
                &quot;Size.Other&quot;: &quot;Size.Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;,
                &quot;Considered without any metric&quot;: &quot;Considered without any metric&quot;
            },
            &quot;Team&#39;s prior experience&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Team&#39;s skill level&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            },
            &quot;Non functional requirements&quot;: {
                &quot;Performance&quot;: &quot;Performance&quot;,
                &quot;Security&quot;: &quot;Security&quot;,
                &quot;Availability&quot;: &quot;Availability&quot;,
                &quot;Reliability&quot;: &quot;Reliability&quot;,
                &quot;Maintainability&quot;: &quot;Maintainability&quot;,
                &quot;Non functional requirements.Other&quot;: &quot;Non functional requirements.Other&quot;,
                &quot;Not considered&quot;: &quot;Not considered&quot;
            },
            &quot;Distributed teams&#39; issues&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;,
                &quot;Not applicable&quot;: &quot;Not applicable&quot;
            },
            &quot;Customer Communication&quot;: {
                &quot;Considered&quot;: &quot;Considered&quot;,
                &quot;Not Considered&quot;: &quot;Not Considered&quot;
            }
        },
        &#39;Effort estimate&#39;: {
            &quot;Estimated effort&quot;: {
                &quot;Estimate value(s)&quot;: &quot;Estimate value(s)&quot;
            },
            &quot;Actual effort&quot;: {
                &quot;Actual effort.Value&quot;: &quot;Actual effort.Value&quot;
            },
            &quot;Effort estimate.Type&quot;: {
                &quot;Point&quot;: &quot;Point&quot;,
                &quot;Three point&quot;: &quot;Three point&quot;,
                &quot;Distribution&quot;: &quot;Distribution&quot;,
                &quot;Effort estimate.Type.Other&quot;: &quot;Effort estimate.Type.Other&quot;
            },
            &quot;Unit&quot;: {
                &quot;Hours/days&quot;: &quot;Hours/days&quot;,
                &quot;Pair days&quot;: &quot;Pair/days&quot;,
                &quot;Ideal hours&quot;: &quot;Ideal hours&quot;,
                &quot;Unit.Other&quot;: &quot;Unit.Other&quot;
            },
            &quot;Accuracy Level&quot;: {
                &quot;Accuracy Level.Value&quot;: &quot;Accuracy Level.Value&quot;
            },
            &quot;Accuracy measure&quot;: {
                &quot;Mean Magnitude of Relative Error&quot;: &quot;Mean Magnitude of Relative Error&quot;,
                &quot;Median Magnitude of Relative Error&quot;: &quot;Median Magnitude of Relative Error&quot;,
                &quot;Bias of Relative Error&quot;: &quot;Bias of Relative Error&quot;,
                &quot;Accuracy measure.Other&quot;: &quot;Accuracy measure.Other&quot;,
                &quot;Not used&quot;: &quot;Not used&quot;
            }
        }
    }
}

# Main execution
if __name__ == &quot;__main__&quot;:
    print(&quot;TAXONOMY NODE COUNTER&quot;)
    print(&quot;=&quot; * 80)
    
    # Analyze single taxonomy
    print(&quot;\n1. SINGLE TAXONOMY ANALYSIS:&quot;)
    results = count_taxonomy_nodes(new_taxonomy, &quot;Cost Estimation for GSD&quot;)
    display_results(results)</code></pre>
<pre><code>TAXONOMY NODE COUNTER
================================================================================

1. SINGLE TAXONOMY ANALYSIS:
================================================================================
TAXONOMY NODE ANALYSIS: Cost Estimation for GSD
================================================================================

SUMMARY:
   Inner Nodes (Categories): 26
   Outer Nodes (Leaves):     83
   Total Nodes:              109
   Max Depth:                4
   Min Depth:                2

INNER NODES LIST (26 total):
[&#39;Estimation context&#39;, &#39;Planning level&#39;, &#39;Estimated activities&#39;, &#39;Agile methods&#39;, &#39;Project domain&#39;, &#39;Project setting&#39;, &#39;Estimation entity&#39;, &#39;Number of entities estimated&#39;, &#39;Team size&#39;, &#39;Estimation technique&#39;, &#39;Estimation Techniques&#39;, &#39;Type&#39;, &#39;Effort predictors&#39;, &#39;Size&#39;, &quot;Team&#39;s prior experience&quot;, &quot;Team&#39;s skill level&quot;, &#39;Non functional requirements&#39;, &quot;Distributed teams&#39; issues&quot;, &#39;Customer Communication&#39;, &#39;Effort estimate&#39;, &#39;Estimated effort&#39;, &#39;Actual effort&#39;, &#39;Effort estimate.Type&#39;, &#39;Unit&#39;, &#39;Accuracy Level&#39;, &#39;Accuracy measure&#39;]

OUTER NODES LIST (83 total):
[&#39;Release&#39;, &#39;Sprint&#39;, &#39;Daily&#39;, &#39;Bidding&#39;, &#39;Analysis&#39;, &#39;Design&#39;, &#39;Implementation&#39;, &#39;Testing&#39;, &#39;Maintenance&#39;, &#39;Estimated activities.All&#39;, &#39;Extreme Programming&#39;, &#39;Scrum&#39;, &#39;Customized Extreme Programming&#39;, &#39;Customized Scrum&#39;, &#39;Dynamic Systems Development Method&#39;, &#39;Crystal&#39;, &#39;Feature-Driven Development&#39;, &#39;Kanban&#39;, &#39;Communications industry&#39;, &#39;Transportation&#39;, &#39;Financial&#39;, &#39;Education&#39;, &#39;Health&#39;, &#39;Retail/Wholesale&#39;, &#39;Manufacturing&#39;, &#39;Government/Military&#39;, &#39;Project domain.Other&#39;, &#39;Co-located&#39;, &#39;Distributed: Close Onshore&#39;, &#39;Distributed: Distant Onshore&#39;, &#39;Distributed: Near Offshore&#39;, &#39;Distributed: Far Offshore&#39;, &#39;User story&#39;, &#39;Task&#39;, &#39;Use case&#39;, &#39;Estimation entity.Other&#39;, &#39;Number of entities estimated.Value&#39;, &#39;No. of team members&#39;, &#39;Planning Poker&#39;, &#39;Expert Judgement&#39;, &#39;Analogy&#39;, &#39;Use case points method&#39;, &#39;Estimation Techniques.Other&#39;, &#39;Single&#39;, &#39;Group&#39;, &#39;Story points&#39;, &#39;User case points&#39;, &#39;Function points&#39;, &#39;Size.Other&#39;, &#39;Not used&#39;, &#39;Considered without any metric&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Performance&#39;, &#39;Security&#39;, &#39;Availability&#39;, &#39;Reliability&#39;, &#39;Maintainability&#39;, &#39;Non functional requirements.Other&#39;, &#39;Not considered&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Not applicable&#39;, &#39;Considered&#39;, &#39;Not Considered&#39;, &#39;Estimate value(s)&#39;, &#39;Actual effort.Value&#39;, &#39;Point&#39;, &#39;Three point&#39;, &#39;Distribution&#39;, &#39;Effort estimate.Type.Other&#39;, &#39;Hours/days&#39;, &#39;Pair days&#39;, &#39;Ideal hours&#39;, &#39;Unit.Other&#39;, &#39;Accuracy Level.Value&#39;, &#39;Mean Magnitude of Relative Error&#39;, &#39;Median Magnitude of Relative Error&#39;, &#39;Bias of Relative Error&#39;, &#39;Accuracy measure.Other&#39;, &#39;Not used&#39;]</code></pre>
<pre class="r"><code>library(workflowr)</code></pre>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span>
Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 4.3.1 (2023-06-16 ucrt)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 11 x64 (build 26100)

Matrix products: default


locale:
[1] LC_COLLATE=Catalan_Spain.utf8  LC_CTYPE=Catalan_Spain.utf8   
[3] LC_MONETARY=Catalan_Spain.utf8 LC_NUMERIC=C                  
[5] LC_TIME=Catalan_Spain.utf8    

time zone: Europe/Madrid
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] workflowr_1.7.1

loaded via a namespace (and not attached):
 [1] Matrix_1.6-1      jsonlite_1.8.7    highr_0.10        compiler_4.3.1   
 [5] promises_1.3.2    Rcpp_1.0.11       stringr_1.5.0     git2r_0.35.0     
 [9] callr_3.7.3       later_1.4.1       jquerylib_0.1.4   png_0.1-8        
[13] yaml_2.3.7        fastmap_1.1.1     here_1.0.1        lattice_0.21-8   
[17] reticulate_1.40.0 R6_2.5.1          knitr_1.43        tibble_3.2.1     
[21] rprojroot_2.0.3   bslib_0.5.1       pillar_1.9.0      rlang_1.1.1      
[25] utf8_1.2.3        cachem_1.0.8      stringi_1.7.12    httpuv_1.6.15    
[29] xfun_0.40         getPass_0.2-4     fs_1.6.3          sass_0.4.7       
[33] cli_3.6.1         magrittr_2.0.3    ps_1.7.5          grid_4.3.1       
[37] digest_0.6.33     processx_3.8.2    rstudioapi_0.15.0 lifecycle_1.0.3  
[41] vctrs_0.6.3       evaluate_0.21     glue_1.6.2        whisker_0.4.1    
[45] fansi_1.0.4       rmarkdown_2.24    httr_1.4.7        tools_4.3.1      
[49] pkgconfig_2.0.3   htmltools_0.5.6  </code></pre>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
https://docs.mathjax.org/en/latest/web/configuration.html. This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
